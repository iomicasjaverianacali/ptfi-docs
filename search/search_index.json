{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PTFI","text":""},{"location":"#project-overview","title":"\ud83d\udd2c Project Overview","text":"<p>This repository contains a machine learning pipeline for structural annotation of unknown molecules from mass spectrometry data using transformer-based neural networks. The project focuses on predicting SMILES (Simplified Molecular Input Line Entry System) representations from tandem mass spectra (MS\u00b2) data.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>End-to-end pipeline from raw MS2 and MS1 data to validated molecular structure predictions</li> <li>Multiple transformer architectures with different enhancement strategies</li> <li>Web API for easy integration and deployment</li> <li>Comprehensive validation against known metabolite databases</li> </ul>"},{"location":"#system-architecture","title":"\ud83c\udfd7\ufe0f System Architecture","text":"<p>The project consists of three major components:</p> <ol> <li>Client Interface - API consumers (cURL, frontend applications)</li> <li>Flask API Server - RESTful endpoints for pipeline operations</li> <li>Computation Cluster - High-performance computing for ML inference</li> </ol>"},{"location":"#documentation-structure","title":"\ud83d\udcda Documentation Structure","text":""},{"location":"#core-concepts-background","title":"\ud83d\udd0d Core Concepts &amp; Background","text":"<ul> <li>Structural Annotation Overview - Scientific background and methodology</li> <li>Bibliography Notes - Research context and related work</li> </ul>"},{"location":"#workflow-pipeline","title":"\u2699\ufe0f Workflow &amp; Pipeline","text":"<ul> <li>General Workflow - Overview of the complete pipeline</li> <li>Training Process - Model training methodology and data sources</li> <li>Pre-prediction Workflow - MS1 to MS2 spectrum extraction</li> <li>Integrated Pipeline - End-to-end processing pipeline</li> </ul>"},{"location":"#machine-learning-models","title":"\ud83e\udd16 Machine Learning Models","text":"<p>The project implements three transformer architectures:</p> <ol> <li>Architecture 1 (<code>MolecularStructureTeFT</code>) - Vanilla transformer encoder-decoder</li> <li>Architecture 2 (<code>MolecularStructureTeFTWithBias</code>) - Transformer with bias mechanisms</li> <li>Architecture 3 (<code>MolecularStructureTeFTWithIntensity</code>) - Transformer with bias mechanisms and gating of the intensities</li> </ol>"},{"location":"#api-application-usage","title":"\ud83c\udf10 API &amp; Application Usage","text":"<ul> <li>Flask API Documentation - Complete API reference</li> <li>Backend API Usage - cURL examples and client commands</li> <li>API Endpoints Guide - Detailed endpoint documentation</li> </ul>"},{"location":"#api-reference","title":"\ud83d\udcd6 API Reference","text":"<ul> <li>Input Management - Data input handling and preprocessing</li> <li>Signal Math - Mathematical operations on spectral data</li> <li>Pre-prediction API - MS2 spectrum extraction methods</li> <li>Prediction API - SMILES prediction from spectra</li> </ul>"},{"location":"#core-components","title":"\ud83d\udee0\ufe0f Core Components","text":""},{"location":"#python-package-ptfifrijolpujc","title":"Python Package (<code>ptfifrijolpujc/</code>)","text":"<ul> <li><code>Workflow.py</code> - Main workflow orchestration (2,174 lines)</li> <li><code>PredictWorkflow.py</code> - SMILES prediction pipeline</li> <li><code>PrePredictWorkflow.py</code> - MS2 spectrum extraction</li> <li><code>ValidatePredictWorkFlow.py</code> - Prediction validation</li> <li><code>TrainWorkFlowV2.py</code> - Training the vanilla transformer</li> <li><code>InputManagement.py</code> - Data input/output handling</li> <li><code>SignalMath.py</code> - Spectral data processing</li> <li><code>MolecularStructureTeFT.py</code> - Vanilla Transformer model implementation</li> <li><code>MolecularStructureTeFTWithBias.py</code> - Transformer with Attention Bias component</li> <li><code>MolecularStructureTeFTWithIntensity.py</code> - Transformer with Gating of intensities</li> </ul>"},{"location":"#scripts-scripts","title":"Scripts (<code>scripts/</code>)","text":"<ul> <li><code>end_2_end_pipeline.py</code> - Complete pipeline execution</li> <li><code>predict_CASMI_2022.py</code> - CASMI benchmark predictions</li> <li><code>validate_prediction.py</code> - Prediction validation utilities</li> <li><code>train_teft_*.py</code> - Model training scripts</li> <li><code>get_stats_*.py</code> - Statistical analysis tools</li> </ul>"},{"location":"#batch-processing","title":"Batch Processing","text":"<ul> <li><code>batch_end_2_end_pipeline.sh</code> - Batch pipeline execution</li> <li><code>batch_end_2_end_pipeline_ptfi.sh</code> - PTFI-specific batch processing</li> <li><code>predict_full_CASMI.sh</code> - CASMI dataset processing</li> </ul>"},{"location":"#data-resources","title":"\ud83d\uddc2\ufe0f Data &amp; Resources","text":""},{"location":"#training-data-sources","title":"Training Data Sources","text":"<ul> <li>MassSpecGym Dataset - Large-scale MS/MS dataset</li> <li>TeFT Dataset - Transformer-based fragmentation trees data</li> </ul>"},{"location":"#available-data-files","title":"Available Data Files","text":"<ul> <li>Training datasets - they are located at <code>/shared/users/ptfi/data/training</code></li> <li>Trained models - they are located at <code>/shared/users/ptfi/models</code></li> <li>Mass spectrometry experiments - they are located at  <code>/shared/users/ptfi/ptfi-app/data/</code>, one folder per potential user (Carolina, Camila, Gabriel, Gustavo, Julian)</li> <li>Pipeline results - they are located at <code>/shared/users/ptfi/ptfi-app/results</code>, one folder per potential user (Carolina, Camila, Gabriel, Gustavo, Julian)</li> <li>Tokenizer dictionary - located at <code>ptfi-frijol-pujc/data/SPE_ChEMBL.txt</code></li> <li>Group of metabolites for benchmarking - located at <code>ptfi-frijol-pujc/data/Espectros_conocidos 1.xlsx</code></li> <li>Setup file to install the repository as package - located at <code>ptfi-frijol-pujc/setup.py</code></li> <li>Documentation building directives - located at <code>ptfi-frijol-pujc/mkdocs.yml</code></li> <li>FLASK app - located at <code>ptfi-frijol-pujc/flask_api.py</code> </li> <li>API endpoints callbacks - located at <code>ptfi-frijol-pujc/scripts/endpoints/*</code></li> </ul>"},{"location":"#cluster-files","title":"Cluster files","text":"<p>Route for the data:</p> <p><code>/shared/users/ptfi/ptfi-app/data</code></p> <p>Route for the results:</p> <p><code>/shared/users/ptfi/ptfi-app/results</code></p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#1-installation","title":"1. Installation","text":"<pre><code>pip install -r requirements.txt\npip install mkdocs-material 'mkdocstrings[python]' mkdocs-glightbox\n</code></pre>"},{"location":"#2-launch-documentation-server","title":"2. Launch Documentation Server","text":"<pre><code>mkdocs serve\n</code></pre>"},{"location":"#3-run-flask-api","title":"3. Run Flask API","text":"<pre><code>python flask_api.py\n</code></pre>"},{"location":"#4-execute-end-to-end-pipeline","title":"4. Execute End-to-End Pipeline","text":"<pre><code>python scripts/end_2_end_pipeline.py --input_dir data/ --output_dir results/\n</code></pre>"},{"location":"#api-endpoints","title":"\ud83d\udcca API Endpoints","text":"<p>The Flask API provides four main endpoints corresponding to pipeline steps:</p> <ol> <li><code>/compute_ms2</code> - Extract MS2 spectra from raw MS1 data</li> <li><code>/predict_smiles</code> - Predict molecular structures from MS2 spectra</li> <li><code>/generate_predicted_ms2</code> - Generate predicted MS2 from SMILES</li> <li><code>/validate_predictions</code> - Validate predictions against reference spectra</li> <li><code>/orchestrate_pipeline</code> - Execute complete pipeline</li> </ol>"},{"location":"#development-maintenance","title":"\ud83d\udd27 Development &amp; Maintenance","text":""},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>ptfi-frijol-pujc/\n\u251c\u2500\u2500 ptfifrijolpujc/          # Main Python package\n\u251c\u2500\u2500 scripts/                 # Utility and processing scripts\n\u251c\u2500\u2500 docs/                    # Documentation (MkDocs)\n\u251c\u2500\u2500 notebooks/               # Jupyter notebooks and analysis\n\u251c\u2500\u2500 data/                    # Training and reference data\n\u251c\u2500\u2500 results/                 # Output and results storage\n\u251c\u2500\u2500 flask_api.py            # Web API server\n\u2514\u2500\u2500 batch_*.sh              # Batch processing scripts\n</code></pre>"},{"location":"#key-configuration-files","title":"Key Configuration Files","text":"<ul> <li><code>mkdocs.yml</code> - Documentation configuration</li> <li><code>setup.py</code> - Python package configuration</li> <li><code>requirements.txt</code> - Python dependencies</li> <li><code>prediction_params.json</code> - Model prediction parameters</li> </ul>"},{"location":"#performance-validation","title":"\ud83d\udcc8 Performance &amp; Validation","text":"<ul> <li>CASMI 2022 Benchmark - Standardized evaluation dataset</li> <li>Known Metabolite Validation - Comparison against reference libraries</li> <li>Statistical Analysis - Comprehensive performance metrics</li> <li>Cross-validation - Robust model evaluation</li> </ul>"},{"location":"#contributors","title":"\ud83d\udc65 Contributors","text":"<p>iOmicas Research Group - Authors: JDVG, GLARA - Contact: jd.villegas@javerianacali.edu.co - Institution: Pontificia Universidad Javeriana Cali</p>"},{"location":"#external-resources","title":"\ud83d\udd17 External Resources","text":"<ul> <li>GitHub Repository</li> <li>MassSpecGym Paper</li> <li>TeFT Repository</li> </ul>"},{"location":"#additional-documentation","title":"\ud83d\udccb Additional Documentation","text":""},{"location":"#specialized-topics","title":"Specialized Topics","text":"<ul> <li>Fragmentation Trees - Tree-based fragmentation analysis</li> <li>MSNovelist Integration - External tool integration</li> <li>MS2-SMILES Methodology - Core prediction methodology</li> <li>CFM-ID Compilation - Competitive fragmentation modeling</li> <li>Future Improvements - Planned enhancements</li> <li>Library Requirements - Detailed dependency information</li> <li>Quick Start Guide - Fast setup instructions</li> </ul>"},{"location":"CURL_CLIENT_CMDS/","title":"PTFI app","text":"<p>The following diagram gives a general idea of the backend of the PTFI application:</p> Flask diagram for the app <p>As can be seen there, there are three major components:</p> <ul> <li> <p>The client consuming the available API endpoints (either by performing cURL requests from a command line, or by using a library for doing so from the frontend).</p> </li> <li> <p>The Flask API server</p> </li> <li> <p>The cluster where the computations are executed.</p> </li> </ul>"},{"location":"CURL_CLIENT_CMDS/#files","title":"Files","text":"<p>The <code>flask_api_submission_server.py</code> file should be located wherever the frontend app will be located. It implements the API endpoints.</p> <p>It is in charge of creating a SSH connection with the cluster, and create a custom batch script for the specific endpoint the client wants to consume and the parameters provided also by the client.</p> <p>On the other hand, the files in charge of the actual computation of the specific endpoint are the following ones: <code>compute_ms2.py</code>, <code>generate_predicted_ms2.py</code>, <code>predict_smiles.py</code>, <code>validate_predictions.py</code>.</p> <p>These files are located in the cluster. Specifically under the PTFI project repository. The structure of the PTFI project is explained better in Index.</p> <p>Under that repository, they are located in the <code>scripts/endpoints</code> folder.</p>"},{"location":"CURL_CLIENT_CMDS/#input-files","title":"Input files","text":"<p>The data to be used by the backend should be placed at the specific route <code>/shared/users/ptfi/ptfi-app/data</code>.</p> <p>Under the <code>data</code> folder there are multiple folders each one per one of the potential users of the application (<code>Carolina</code>, <code>Camila</code>, <code>Gabriel</code>, <code>Gustavo</code>, <code>Julian</code>).</p> <p>The user should place its files inside its username folder, and the placed files should be placed in two folders each one with specific name: <code>dda</code> and <code>ms1</code>.</p> <p>Under the <code>dda</code> folder there would be the DDA type of files associated with the experiment.</p> <p>Under the <code>ms1</code> folder there would be the MS1 type of files associated with the same experiment.</p>"},{"location":"CURL_CLIENT_CMDS/#output-files","title":"Output files","text":"<p>The results that each endpoint give are located in <code>/shared/users/ptfi/ptfi-app/results</code>.</p> <p>For each of the potential users of the ptfi pipeline (<code>Carolina</code>, <code>Camila</code>, <code>Gustavo</code>, <code>Gabriel</code>, <code>Julian</code>) there is an assigned folder.</p> <p>Under each of this folders there will be a new folder created for each project. A project being a given set of measurements. </p>"},{"location":"CURL_CLIENT_CMDS/#installation","title":"Installation","text":"<ol> <li>Install the required dependencies:</li> </ol> <pre><code>pip install -r flask flask-cors paramiko\n</code></pre> <ol> <li>Make sure you have the <code>ptfifrijolpujc</code> package installed in your environment. For doing so, set the current directory to be the <code>ptfi-frijol-pujc</code> repo:</li> </ol> <pre><code>pip install -e .\n</code></pre>"},{"location":"CURL_CLIENT_CMDS/#running-the-api","title":"Running the API","text":"<p>Start the Flask app by running the following on the computer that is running the frontend:</p> <pre><code>python flask_api_submission.py\n</code></pre> <p>The API will be available at <code>http://localhost:5000</code></p> <p>To change the desired port of the application, modify in the following line:</p> <p><pre><code>if __name__ == '__main__':\n  app.run(host='0.0.0.0', port=5000, debug=False)\n</code></pre> of the <code>flask_api_submission_server.py</code> file</p>"},{"location":"CURL_CLIENT_CMDS/#consuming-the-api","title":"Consuming the API","text":"<p>In the following sections the cURL commands to consume the available endpoints are presented.</p>"},{"location":"CURL_CLIENT_CMDS/#endpoint-1-compute-ms2","title":"Endpoint 1: compute MS2","text":""},{"location":"CURL_CLIENT_CMDS/#windows-client","title":"Windows client","text":"<pre><code>curl -X POST http://localhost:5000/compute_ms2 ^\n-H \"Content-Type: application/json\" ^\n-d \"{\\\"parameters\\\": {\\\"user_name\\\": \\\"Julian\\\", \\\"project_name\\\": \\\"TestProject\\\", \\\"ground_truth_ms2_filename\\\": \\\"ground_truth\\\", \\\"mass_trace_mass_error_ppm\\\": 10.0, \\\"mass_trace_noise_threshold_int\\\": 1000.0, \\\"mass_trace_chrom_peak_snr\\\": 3.0, \\\"mass_trace_min_sample_rate\\\": 0.5, \\\"mass_trace_min_length\\\": 5.0, \\\"mass_trace_max_length\\\": -1.0, \\\"mass_trace_quant_method\\\": \\\"area\\\", \\\"elution_peak_width_filtering\\\": \\\"auto\\\", \\\"elution_peak_chrom_fwhm\\\": 5.0, \\\"elution_peak_chrom_peak_snr\\\": 3.0, \\\"elution_peak_min_fwhm\\\": 1.0, \\\"elution_peak_max_fwhm\\\": 60.0, \\\"elution_peak_masstrace_snr_filtering\\\": \\\"false\\\", \\\"feature_detection_remove_single_traces\\\": \\\"false\\\", \\\"feature_detection_local_rt_range\\\": 15.0, \\\"feature_detection_local_mz_range\\\": 5.0, \\\"feature_detection_charge_lower_bound\\\": 1, \\\"feature_detection_charge_upper_bound\\\": 3, \\\"feature_detection_chrom_fwhm\\\": 5.0, \\\"feature_detection_report_summed_ints\\\": \\\"false\\\", \\\"feature_detection_enable_RT_filtering\\\": \\\"true\\\", \\\"feature_detection_isotope_filtering_model\\\": \\\"metabolites (5% RMS)\\\", \\\"feature_detection_mz_scoring_13C\\\": \\\"false\\\", \\\"feature_detection_use_smoothed_intensities\\\": \\\"true\\\", \\\"feature_detection_report_convex_hulls\\\": \\\"true\\\", \\\"feature_detection_report_chromatograms\\\": \\\"false\\\", \\\"feature_detection_mz_scoring_by_elements\\\": \\\"false\\\", \\\"make_ms2_mz_tolerance\\\": 0.01, \\\"make_ms2_rt_tolerance\\\": 5.0, \\\"max_peak_filter_pptg\\\": 0.2, \\\"merger_spectra_mz_binning_width\\\": 5.0, \\\"merger_spectra_mz_binning_width_unit\\\": \\\"ppm\\\", \\\"merger_spectra_sort_blocks\\\": \\\"RT_ascending\\\", \\\"merger_spectra_mz_tolerance\\\": 0.0001, \\\"merger_spectra_mass_tolerance\\\": 0.0, \\\"merger_spectra_rt_tolerance\\\": 15.0, \\\"filter_type\\\": \\\"window_mower\\\", \\\"window_mower_windowsize\\\": 50.0, \\\"window_mower_peakcount\\\": 2, \\\"window_mower_movetype\\\": \\\"slide\\\", \\\"threshold_mower_threshold\\\": 0.05, \\\"nlargest_n\\\": 200, \\\"filename_feature_map\\\": \\\"feature_map\\\", \\\"filename_ms2s_mzml\\\": \\\"ms2s\\\", \\\"filename_consensus_map\\\": \\\"consensus_map\\\"}}\"\n</code></pre>"},{"location":"CURL_CLIENT_CMDS/#linux-client","title":"Linux client","text":"<pre><code>curl -X POST http://localhost:5000/compute_ms2 \\\n-H \"Content-Type: application/json\" \\\n-d '{\"parameters\": {\"user_name\": \"Julian\", \"project_name\": \"TestProject\", \"ground_truth_ms2_filename\": \"ground_truth\", \"mass_trace_mass_error_ppm\": 10.0, \"mass_trace_noise_threshold_int\": 1000.0, \"mass_trace_chrom_peak_snr\": 3.0, \"mass_trace_min_sample_rate\": 0.5, \"mass_trace_min_length\": 5.0, \"mass_trace_max_length\": -1.0, \"mass_trace_quant_method\": \"area\", \"elution_peak_width_filtering\": \"auto\", \"elution_peak_chrom_fwhm\": 5.0, \"elution_peak_chrom_peak_snr\": 3.0, \"elution_peak_min_fwhm\": 1.0, \"elution_peak_max_fwhm\": 60.0, \"elution_peak_masstrace_snr_filtering\": \"false\", \"feature_detection_remove_single_traces\": \"false\", \"feature_detection_local_rt_range\": 15.0, \"feature_detection_local_mz_range\": 5.0, \"feature_detection_charge_lower_bound\": 1, \"feature_detection_charge_upper_bound\": 3, \"feature_detection_chrom_fwhm\": 5.0, \"feature_detection_report_summed_ints\": \"false\", \"feature_detection_enable\n</code></pre>"},{"location":"CURL_CLIENT_CMDS/#endpoint-2-predict-smiles","title":"Endpoint 2: predict smiles","text":""},{"location":"CURL_CLIENT_CMDS/#windows-client_1","title":"Windows client","text":"<pre><code>curl -X POST http://localhost:5000/predict_smiles ^\n-H \"Content-Type: application/json\" ^\n-d \"{\\\"parameters\\\": {\\\"user_name\\\": \\\"Julian\\\", \\\"file_name\\\": \\\"red_ground_truth\\\", \\\"trained_model_name\\\": \\\"dd_arch1_lf1_data_1\\\", \\\"project_name\\\": \\\"QC_ppt_forages_clipped_DELETE\\\", \\\"num_pred\\\": 5, \\\"predict_approach\\\": \\\"beam_search\\\", \\\"device\\\": \\\"cpu\\\", \\\"beam_size\\\": 3}}\"\n</code></pre>"},{"location":"CURL_CLIENT_CMDS/#linux-client_1","title":"Linux client","text":"<pre><code>curl -X POST http://localhost:5000/predict_smiles \\\n-H \"Content-Type: application/json\" \\\n-d '{\"parameters\": {\"user_name\": \"Julian\", \"file_name\": \"red_ground_truth\", \"trained_model_name\": \"dd_arch1_lf1_data_1\", \"project_name\": \"QC_ppt_forages_clipped_DELETE\", \"num_pred\": 5, \"predict_approach\": \"beam_search\", \"device\": \"cpu\", \"beam_size\": 3}}'\n</code></pre>"},{"location":"CURL_CLIENT_CMDS/#endpoint-3-generate-predicted-smiles","title":"Endpoint 3: generate predicted SMILES","text":""},{"location":"CURL_CLIENT_CMDS/#windows-client_2","title":"Windows client","text":"<pre><code>curl -X POST http://localhost:5000/generate_predicted_ms2 ^\n-H \"Content-Type: application/json\" ^\n-d \"{\\\"parameters\\\": {\\\"user_name\\\": \\\"Julian\\\", \\\"project_name\\\": \\\"QC_ppt_forages\\\", \\\"predicted_ms2_filename\\\": \\\"predicted_ms2\\\", \\\"ionization\\\": \\\"[M+H]+\\\", \\\"device\\\": \\\"cpu\\\"}}\"\n</code></pre>"},{"location":"CURL_CLIENT_CMDS/#linux-client_2","title":"Linux client","text":"<pre><code>curl -X POST http://localhost:5000/generate_predicted_ms2 \\\n-H \"Content-Type: application/json\" \\\n-d '{\"parameters\": {\"user_name\": \"Julian\", \"project_name\": \"QC_ppt_forages\", \"predicted_ms2_filename\": \"predicted_ms2\", \"ionization\": \"[M+H]+\", \"device\": \"cpu\"}}'\n</code></pre>"},{"location":"CURL_CLIENT_CMDS/#endpoint-4-validate-predictions","title":"Endpoint 4: validate predictions","text":""},{"location":"CURL_CLIENT_CMDS/#windows-client_3","title":"Windows client","text":"<pre><code>curl -X POST http://localhost:5000/validate_predictions ^\n-H \"Content-Type: application/json\" ^\n-d \"{\\\"parameters\\\": {\\\"user_name\\\": \\\"Julian\\\", \\\"project_name\\\": \\\"QC_ppt_forages\\\", \\\"predicted_ms2_filename\\\": \\\"predicted_ms2\\\", \\\"ground_truth_ms2_filename\\\": \\\"ground_truth\\\", \\\"predicted_smiles_filename\\\": \\\"final_predictions\\\", \\\"weight_mass_missmatch\\\": 0.5, \\\"weight_dreams\\\": 0.5}}\"\n</code></pre>"},{"location":"CURL_CLIENT_CMDS/#linux-client_3","title":"Linux client","text":"<pre><code>curl -X POST http://localhost:5000/validate_predictions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"parameters\": {\"user_name\": \"Julian\", \"project_name\": \"QC_ppt_forages\", \"predicted_ms2_filename\": \"predicted_ms2\", \"ground_truth_ms2_filename\": \"ground_truth\", \"predicted_smiles_filename\": \"final_predictions\", \"weight_mass_missmatch\": 0.5, \"weight_dreams\": 0.5}}'\n</code></pre>"},{"location":"PrePredictionScript/","title":"How to run the <code>predict_CASMI_2022_modular.py</code> script?","text":""},{"location":"PrePredictionScript/#casmi-files","title":"CASMI files","text":"<p>The CASMI files should have the following folder structure:</p> <p>pos     -A_M1_posPFP         -A_M1_posPFP_01.mzml         -A_M1_posPFP_02.mzml     -A_M2_posPFP         -A_M2_posPFP_01.mzml         -A_M2_posPFP_02.mzml     .     .     . neg     -A_M7_negPFP         -A_M7_negPFP_03.mzml         -A_M7_negPFP_04.mzml     .     .     .</p>"},{"location":"PrePredictionScript/#option-1-using-default-parameters","title":"[OPTION 1] Using default parameters","text":"<p>python predict_CASMI_2022_modular.py \\     --chromatograms_dir path/to/dir/with/mzml/files \\     --model_dir /path/to/models/dir \\     --model_name originaldata_datadriven_tokenizer \\     --output_dir /path/to/output/dir     --protcol CASMI</p>"},{"location":"PrePredictionScript/#option-2-overriding-specific-parameters-via-command-line","title":"[OPTION 2] Overriding specific parameters via command line","text":"<p>python predict_CASMI_2022_modular.py \\     --chromatograms_dir path/to/dir/with/mzml/files \\     --model_dir /path/to/models \\     --model_name originaldata_datadriven_tokenizer \\     --output_dir /path/to/output/dir \\     --mass_trace_mass_error_ppm 15.0 \\     --mass_trace_chrom_peak_snr 4.0 \\     --feature_detection_local_rt_range 3.0     --protcol CASMI</p>"},{"location":"PrePredictionScript/#option-3-providing-a-json-file-with-the-desired-parameters","title":"[OPTION 3] Providing a JSON file with the desired parameters","text":"<p>python predict_CASMI_2022_modular.py \\     --chromatograms_dir path/to/dir/with/mzml/files \\     --model_dir /path/to/models/ \\     --model_name originaldata_datadriven_tokenizer \\     --output_dir /path/to/output/dir \\     --params_file params.json \\     --protcol CASMI</p>"},{"location":"PrePredictionScript/#option-4-combining-json-file-and-command-line-overrides","title":"[OPTION 4] Combining JSON file and command-line overrides","text":"<p>python predict_CASMI_2022_modular.py \\     --chromatograms_dir path/to/dir/with/mzml/files \\     --model_dir /path/to/models/dir \\     --model_name originaldata_datadriven_tokenizer \\     --output_dir /path/to/output/dir \\     --params_file params.json \\     --mass_trace_mass_error_ppm 15.0     --protcol CASMI</p> <p>If both a file and command line parameters are used, and the user specifies the same parameter in the command line and in the file, the value given by the command line will override the value in the file.</p>"},{"location":"PrePredictionScript/#example-of-paramsjson-file","title":"Example of <code>params.json</code> file","text":"<p>{     \"mass_trace_mass_error_ppm\": 10.0,     \"mass_trace_noise_threshold_int\": 0.12e04,     \"mass_trace_chrom_peak_snr\": 3.0,     \"mass_trace_min_sample_rate\": 0.5,     \"mass_trace_min_length\": 5.0,     \"mass_trace_max_length\": -1.0,     \"mass_trace_quant_method\": \"area\",     \"elution_peak_width_filtering\": \"auto\",     \"elution_peak_chrom_fwhm\": 2.0,     \"elution_peak_chrom_peak_snr\": 3.0,     \"elution_peak_min_fwhm\": 1.0,     \"elution_peak_max_fwhm\": 60.0,     \"elution_peak_masstrace_snr_filtering\": \"false\",     \"feature_detection_remove_single_traces\": \"false\",     \"feature_detection_local_rt_range\": 2.0,     \"feature_detection_local_mz_range\": 10.0,     \"feature_detection_charge_lower_bound\": 1,     \"feature_detection_charge_upper_bound\": 3,     \"feature_detection_chrom_fwhm\": 2.0,     \"feature_detection_report_summed_ints\": \"false\",     \"feature_detection_enable_RT_filtering\": \"true\",     \"feature_detection_isotope_filtering_model\": \"metabolites (5% RMS)\",     \"feature_detection_mz_scoring_13C\": \"false\",     \"feature_detection_use_smoothed_intensities\": \"true\",     \"feature_detection_report_convex_hulls\": \"true\",     \"feature_detection_report_chromatograms\": \"false\",     \"feature_detection_mz_scoring_by_elements\": \"false\",     'make_ms2_mz_tolerance': 0.01,      'make_ms2_rt_tolerance': 5.0,      'max_peak_filter_pptg':0.2,     'merger_spectra_mz_binning_width':5.0,     'merger_spectra_mz_binning_width_unit':\"ppm\",     'merger_spectra_sort_blocks':\"RT_ascending\",     'merger_spectra_mz_tolerance':1.0e-04,     'merger_spectra_mass_tolerance':0.0,     'merger_spectra_rt_tolerance':15.0,     'filter_type':\"window_mower\",     'window_mower_windowsize':50.0,     'window_mower_peakcount':2,      'window_mower_movetype':'slide',     'threshold_mower_threshold':0.05,     'nlargest_n':200,      'filename_feature_map':\"feature_map\",     'filename_ms2s_mzml':\"ms2s\",     'filename_consensus_map':\"consensus_map\", }</p>"},{"location":"PrePredictionScript/#command-line-arguments-description","title":"Command Line Arguments Description","text":"<p>Below is a description of each argument that can be used with the <code>predict_CASMI_2022_modular.py</code> script:</p>"},{"location":"PrePredictionScript/#required-arguments","title":"Required Arguments","text":"<ul> <li><code>--chromatograms</code>: Path to the directory containing the mzML files structured as described above.</li> <li><code>--model_dir</code>: Path to the directory containing the prediction models.</li> <li><code>--model_name</code>: Name of the model to use for predictions (e.g., \"originaldata_datadriven_tokenizer.pth\") WITH THE EXTENSION.</li> <li><code>--output_dir</code>: Path to the directory where output files will be saved.</li> <li><code>--protcol</code>: Protocol to use for prediction, e.g., \"CASMI\".</li> </ul>"},{"location":"PrePredictionScript/#mass-trace-parameters","title":"Mass Trace Parameters","text":"<ul> <li><code>--mass_trace_mass_error_ppm</code>: Mass error tolerance in parts per million (default: 10.0).</li> <li><code>--mass_trace_noise_threshold_int</code>: Intensity threshold for noise filtering (default: 1200).</li> <li><code>--mass_trace_chrom_peak_snr</code>: Signal-to-noise ratio for chromatographic peak detection (default: 3.0).</li> <li><code>--mass_trace_min_sample_rate</code>: Minimum required sampling rate (default: 0.5).</li> <li><code>--mass_trace_min_length</code>: Minimum length of mass traces (default: 5.0).</li> <li><code>--mass_trace_max_length</code>: Maximum length of mass traces. -1.0 for no maximum (default: -1.0).</li> <li><code>--mass_trace_quant_method</code>: Method for quantification (\"area\" or alternative methods).</li> </ul>"},{"location":"PrePredictionScript/#elution-peak-parameters","title":"Elution Peak Parameters","text":"<ul> <li><code>--elution_peak_width_filtering</code>: Setting for peak width filtering (default: \"auto\").</li> <li><code>--elution_peak_chrom_fwhm</code>: Full width at half maximum for chromatographic peaks (default: 2.0).</li> <li><code>--elution_peak_chrom_peak_snr</code>: Signal-to-noise ratio for elution peak detection (default: 3.0).</li> <li><code>--elution_peak_min_fwhm</code>: Minimum full width at half maximum (default: 1.0).</li> <li><code>--elution_peak_max_fwhm</code>: Maximum full width at half maximum (default: 60.0).</li> <li><code>--elution_peak_masstrace_snr_filtering</code>: Enable/disable SNR filtering for mass traces (default: \"false\").</li> </ul>"},{"location":"PrePredictionScript/#feature-detection-parameters","title":"Feature Detection Parameters","text":"<ul> <li><code>--feature_detection_remove_single_traces</code>: Whether to remove features with single traces (default: \"false\").</li> <li><code>--feature_detection_local_rt_range</code>: Local retention time range for feature detection (default: 2.0).</li> <li><code>--feature_detection_local_mz_range</code>: Local m/z range for feature detection (default: 10.0).</li> <li><code>--feature_detection_charge_lower_bound</code>: Lower bound for charge state detection (default: 1).</li> <li><code>--feature_detection_charge_upper_bound</code>: Upper bound for charge state detection (default: 3).</li> <li><code>--feature_detection_chrom_fwhm</code>: Chromatographic FWHM for feature detection (default: 2.0).</li> <li><code>--feature_detection_report_summed_ints</code>: Whether to report summed intensities (default: \"false\").</li> <li><code>--feature_detection_enable_RT_filtering</code>: Enable/disable RT filtering (default: \"true\").</li> <li><code>--feature_detection_isotope_filtering_model</code>: Model for isotope filtering (default: \"metabolites (5% RMS)\").</li> <li><code>--feature_detection_mz_scoring_13C</code>: Enable/disable 13C m/z scoring (default: \"false\").</li> <li><code>--feature_detection_use_smoothed_intensities</code>: Use smoothed intensities (default: \"true\").</li> <li><code>--feature_detection_report_convex_hulls</code>: Report convex hulls (default: \"true\").</li> <li><code>--feature_detection_report_chromatograms</code>: Report chromatograms (default: \"false\").</li> <li><code>--feature_detection_mz_scoring_by_elements</code>: Enable/disable m/z scoring by elements (default: \"false\"). </li> <li><code>--filename_feature_map</code>: Name of the feautureXML files with the Feature map info. Do not add the extension. (default: feature_map).</li> <li><code>--filename_consensus_map</code>: Name of the consensusXML files with the Consensus map info. Do not add the extension. (default: consensus_map).</li> </ul>"},{"location":"PrePredictionScript/#ms2-features-mapping-parameters","title":"MS2-Features Mapping Parameters","text":"<ul> <li><code>--make_ms2_mz_tolerance</code>: m/z tolerance for MS2 features mapping (default: 0.01).</li> <li><code>--make_ms2_rt_tolerance</code>: RT tolerance for MS2 features mapping (default: 5.0).</li> <li><code>--filename_ms2s_mzml</code>: Name of the mzML files with the MS2 spectra. Do not add the extension. (default: ms2s).</li> <li><code>--max_peak_filter_pptg</code>: Maximum peak filter for pptg (default: 0.2).</li> <li><code>--merger_spectra_mz_binning_width</code>: m/z binning width for merging spectra (default: 5.0).</li> <li><code>--merger_spectra_mz_binning_width_unit</code>: Unit for m/z binning width (default: \"ppm\").</li> <li><code>--merger_spectra_sort_blocks</code>: Sorting method for blocks (default: \"RT_ascending\").</li> <li><code>--merger_spectra_mz_tolerance</code>: m/z tolerance for merging spectra (default: 1.0e-04).</li> <li><code>--merger_spectra_mass_tolerance</code>: Mass tolerance for merging spectra (default: 0.0).</li> <li><code>--merger_spectra_rt_tolerance</code>: RT tolerance for merging spectra (default: 15.0).</li> <li><code>--filter_type</code>: Type of filter to apply (default: \"window_mower\").</li> <li><code>--window_mower_windowsize</code>: Window size for the window mower filter if <code>--filter_type:\"window_mower\"</code> (default: 50.0).</li> <li><code>--window_mower_peakcount</code>: Number of peaks to keep in the window mower filter if <code>--filter_type:\"window_mower\"</code> (default: 2).</li> <li><code>--window_mower_movetype</code>: Movement type for the window mower filter if <code>--filter_type:\"window_mower\"</code> (default: \"slide\").</li> <li><code>--threshold_mower_threshold</code>: Threshold for the threshold mower filter if <code>--filter_type:\"threshold_mower\"</code> (default: 0.05).</li> <li><code>--nlargest_n</code>: Number of largest peaks to keep if <code>--filter_type:\"nlargest\"</code> (default: 200).</li> </ul>"},{"location":"PrePredictionScript/#other-arguments","title":"Other Arguments","text":"<ul> <li><code>--params_file</code>: Path to a JSON file containing parameter settings (as shown in the example above).</li> </ul>"},{"location":"PrePredictionWorkflow/","title":"Mass trace detection","text":""},{"location":"PrePredictionWorkflow/#purpose","title":"Purpose","text":"<ul> <li>Extract mass traces by gathering peaks with similar m/z values that move along retention time.</li> </ul>"},{"location":"PrePredictionWorkflow/#methodology","title":"Methodology","text":""},{"location":"PrePredictionWorkflow/#initial-peak-selection","title":"Initial Peak Selection","text":"<p>Sorting:</p> <ul> <li>Peaks from an MSExperiment are sorted by intensity.</li> <li>Stored as a list of potential chromatographic apex positions.</li> </ul> <p>Noise Filtering:</p> <ul> <li>Only peaks above a user-defined noise threshold are analyzed.</li> <li>Peaks considered as apices must be n times above the minimal threshold.</li> <li>Reduces noise and saves computational resources.</li> </ul>"},{"location":"PrePredictionWorkflow/#trace-extension","title":"Trace Extension","text":"<p>Process:</p> <ul> <li>Mass traces are extended in both increasing and decreasing retention time directions.</li> <li>Centroid m/z is calculated on-the-fly as an intensity-weighted mean of peaks.</li> </ul> <p>Termination:</p> <ul> <li>Frequency of gathered peaks drops below <code>min_sample_rate</code>.</li> <li>Number of missed scans exceeds the threshold <code>trace_termination_outliers</code>.</li> </ul>"},{"location":"PrePredictionWorkflow/#final-filtering","title":"Final Filtering","text":"<p>Only mass traces that meet the following criteria are retained:</p> <ul> <li>Satisfy minimal and maximal length requirements.</li> <li>Fulfill the minimal sample rate criterion.</li> </ul>"},{"location":"PrePredictionWorkflow/#outputs","title":"Outputs","text":"<ul> <li>A refined set of mass traces that meet the defined quality filters.</li> </ul>"},{"location":"PrePredictionWorkflow/#elution-peak-detection","title":"Elution peak detection","text":"<p>\"Mass traces may consist of several consecutively (partly overlapping) eluting peaks, e.g., stemming from (almost) isobaric compounds that are separated by retention time. Especially in metabolomics, isomeric compounds with exactly the same mass but different retentional behaviour may still be contained in the same mass trace\" <sup>2</sup>.</p>"},{"location":"PrePredictionWorkflow/#purpose_1","title":"Purpose","text":"<ul> <li>Extract chromatographic peaks from a mass trace.</li> </ul>"},{"location":"PrePredictionWorkflow/#methodology_1","title":"Methodology","text":""},{"location":"PrePredictionWorkflow/#preprocessing","title":"Preprocessing","text":"<ul> <li>Applies smoothing to the mass trace's intensities.</li> <li>Uses Savitzky-Golay smoothing:</li> <li>Second-order polynomial.</li> <li>Frame length defined by <code>chrom_fwhm</code> (full width at half maximum).</li> </ul>"},{"location":"PrePredictionWorkflow/#peak-detection","title":"Peak Detection","text":"<ul> <li>Detects local minima and maxima on smoothed intensities.</li> <li>Assumes one peak maximum within a fixed peak width (<code>chrom_fwhm</code>).</li> </ul>"},{"location":"PrePredictionWorkflow/#filtering","title":"Filtering","text":"<ul> <li>Optional filtering of mass traces by:</li> <li>Length in seconds (\"fixed\" filter).</li> <li>Quantile-based criteria.</li> </ul>"},{"location":"PrePredictionWorkflow/#outputs_1","title":"Outputs","text":"<ul> <li>A vector of chromatographic peaks for each mass trace (split mass traces).</li> </ul>"},{"location":"PrePredictionWorkflow/#usage","title":"Usage","text":"<ul> <li>Call the <code>detectPeaks</code> function to extract peaks.</li> <li>Optionally, use the <code>filterByPeakWidth</code> function for additional filtering.</li> </ul>"},{"location":"PrePredictionWorkflow/#feature-detection","title":"Feature Detection","text":""},{"location":"PrePredictionWorkflow/#purpose_2","title":"Purpose","text":"<ul> <li>Assemble mass traces into composite features belonging to the same isotope pattern.</li> <li>Ensure compatibility in:</li> <li>Retention times (RTs).</li> <li>Mass-to-charge ratios (m/z).</li> <li>Isotope abundances.</li> </ul>"},{"location":"PrePredictionWorkflow/#methodology_2","title":"Methodology","text":""},{"location":"PrePredictionWorkflow/#input","title":"Input","text":"<ul> <li>Uses mass traces detected by MassTraceDetection.</li> <li>Chromatographic peaks are split by ElutionPeakDetection.</li> </ul>"},{"location":"PrePredictionWorkflow/#feature-hypothesis-formulation","title":"Feature Hypothesis Formulation","text":"<p>Hypothesis Generation:</p> <ul> <li>Feature hypotheses are exhaustively formulated based on mass traces within a local RT and m/z region.</li> </ul> <p>Scoring:</p> <ul> <li>Hypotheses are scored by their similarity to real metabolite isotope patterns.</li> <li>Scores are derived from:<ul> <li>Independent models for RT shifts and m/z differences between isotopic mass traces.</li> </ul> </li> <li>Isotopic abundances are evaluated using an SVM model to distinguish between correct and false patterns.</li> </ul>"},{"location":"PrePredictionWorkflow/#output","title":"Output","text":"<p>Composite Features:</p> <p>Mass traces that match feature hypotheses are assembled into composite features.</p> <p>Singletons:</p> <p>Mass traces that could not be assembled and represent low-intensity metabolites with only a monoisotopic mass trace, remain in the FeatureMap as singletons with an undefined charge state (0).</p> <ol> <li> <p>C++ API for pyOpenMS: Mass Trace Detection \u21a9</p> </li> <li> <p>C++ API for pyOpenMS: Elution Peak Detection \u21a9</p> </li> <li> <p>C++ API for pyOpenMS: Feature Detection \u21a9</p> </li> </ol>"},{"location":"README_flask_api/","title":"MS2 Prediction Flask API","text":"<p>This Flask API provides 4 endpoints that correspond to the main steps in the end-to-end MS2 spectra prediction and validation pipeline.</p>"},{"location":"README_flask_api/#installation","title":"Installation","text":"<ol> <li> <p>Install the required dependencies: <pre><code>pip install -r requirements_flask.txt\n</code></pre></p> </li> <li> <p>Make sure you have the <code>ptfifrijolpujc</code> package installed in your environment.</p> </li> </ol>"},{"location":"README_flask_api/#running-the-api","title":"Running the API","text":"<p>Start the Flask server: <pre><code>python flask_api.py\n</code></pre></p> <p>The API will be available at <code>http://localhost:5000</code></p>"},{"location":"README_flask_api/#api-endpoints","title":"API Endpoints","text":""},{"location":"README_flask_api/#usage-examples-using-curl","title":"Usage examples using CURL","text":"<ol> <li>Compute MS2 spectra and download pickle file:</li> </ol> <p>CURL WORKING: CHECKED</p> <pre><code>curl -X POST http://localhost:5000/compute_ms2 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"chromatograms_dir\": \"/path/to/chromatograms\",\n    \"ddas_dir\": \"/path/to/ddas\",\n    \"project_name\": \"my_project\",\n    \"ground_truth_ms2_filename\": \"ground_truth\"\n  }' \\\n  --output ground_truth.pkl\n</code></pre> <p>more detailed example:</p> <pre><code>curl -X POST http://localhost:5000/compute_ms2 -H \"Content-Type: application/json\" -d \"{\\\"chromatograms_dir\\\": \\\"/mnt/c/Users/daniel.sinisterra/Documents/repos/data/Datos .mzML/MS1 test/ABC-22393-DM_pos\\\", \\\"ddas_dir\\\": \\\"/mnt/c/Users/daniel.sinisterra/Documents/repos/data/Datos .mzML/DDA test/ABC-22393-DM_HE_pos\\\", \\\"project_name\\\": \\\"my_project\\\", \\\"ground_truth_ms2_filename\\\": \\\"ground_truth\\\"}\"\n</code></pre> <ol> <li>Predict SMILES:</li> </ol> <p>CURL WORKING: CHECKED</p> <pre><code>curl -X POST http://localhost:5000/predict_smiles \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"data_file_path\": \"path/to/ground/truth/ms2.pkl\",\n    \"path_to_mdl\": \"/path/to/model\",\n    \"trained_model_name\": \"model.pth\",\n    \"project_name\": \"my_project\",\n    \"num_pred\": 5,\n    \"predict_approach\": \"greedy\",\n    \"device\": \"cpu\"\n  }'\n</code></pre> <p>more detailed example in windows:</p> <pre><code>curl -X POST http://localhost:5000/predict_smiles ^\n  -H \"Content-Type: application/json\" ^\n  -d \"{\\\"data_file_path\\\": \\\"C:\\\\\\\\Users\\\\\\\\daniel.sinisterra\\\\\\\\Documents\\\\\\\\repos\\\\\\\\results\\\\\\\\test_postman\\\\\\\\ground_truth.pkl\\\",\\\"path_to_mdl\\\": \\\"C:\\\\\\\\Users\\\\\\\\daniel.sinisterra\\\\\\\\Documents\\\\\\\\repos\\\\\\\\models\\\",\\\"trained_model_name\\\": \\\"dd_arch1_lf1_data_1.pth\\\",\\\"project_name\\\": \\\"test_postman\\\",\\\"num_pred\\\": 5,\\\"predict_approach\\\": \\\"greedy\\\",\\\"device\\\": \\\"cpu\\\"}\"\n</code></pre> <p>more detailed example in WSL:</p> <pre><code>curl -X POST http://localhost:5000/predict_smiles ^\n  -H \"Content-Type: application/json\" ^\n  -d \"{\\\"data_file_path\\\": \\\"/mnt/c/Users/daniel.sinisterra/Documents/repos/results/my_project/ground_truth.pkl\\\",\\\"path_to_mdl\\\": \\\"/mnt/c/Users/daniel.sinisterra/Documents/repos/models\\\",\\\"trained_model_name\\\": \\\"dd_arch1_lf1_data_1.pth\\\",\\\"project_name\\\": \\\"my_project\\\",\\\"num_pred\\\": 5,\\\"predict_approach\\\": \\\"greedy\\\",\\\"device\\\": \\\"cuda\\\"}\"\n</code></pre> <ol> <li>Generate predicted MS2:</li> </ol> <p>CURL WORKING: CHECKED</p> <pre><code>curl -X POST http://localhost:5000/generate_predicted_ms2 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_name\": \"my_project\",\n    \"predicted_ms2_filename\": \"predicted_ms2\",\n    \"ionization\": \"[M+H]+\"\n  }'\n</code></pre> <p>a more detailed example in windows or WSL:</p> <pre><code>curl -X POST http://localhost:5000/generate_predicted_ms2 ^\n  -H \"Content-Type: application/json\" ^\n  -d \"{\\\"project_name\\\": \\\"my_project\\\", \\\"predicted_ms2_filename\\\": \\\"predicted_ms2\\\", \\\"ionization\\\": \\\"[M+H]+\\\", \\\"device\\\": \\\"cpu\\\"}\"\n</code></pre> <ol> <li>Validate predictions: <pre><code>curl -X POST http://localhost:5000/validate_predictions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_name\": \"my_project\",\n    \"predicted_ms2_filename\": \"predicted_ms2\",\n    \"ground_truth_ms2_filename\": \"ground_truth\",\n    \"predicted_smiles_filename\": \"final_predictions\"\n  }'\n</code></pre></li> </ol> <p>a more detailed example:</p> <pre><code>curl -X POST http://localhost:5000/validate_predictions ^\n  -H \"Content-Type: application/json\" ^\n  -d \"{\\\"project_name\\\": \\\"my_project\\\", \\\"predicted_ms2_filename\\\": \\\"predicted_ms2\\\", \\\"ground_truth_ms2_filename\\\": \\\"ground_truth\\\", \\\"predicted_smiles_filename\\\": \\\"final_predictions\\\", \\\"weight_mass_missmatch\\\": \\\"0.5\\\", \\\"weight_dreams\\\": \\\"0.5\\\"}\"\n</code></pre>"},{"location":"README_flask_api/#using-python-requests","title":"Using Python requests","text":"<pre><code>import requests\nimport json\n\n# Base URL\nbase_url = \"http://localhost:5000\"\n\n# 1. Compute MS2 spectra and download pickle file\nresponse = requests.post(f\"{base_url}/compute_ms2\", json={\n    \"chromatograms_dir\": \"/path/to/chromatograms\",\n    \"ddas_dir\": \"/path/to/ddas\",  # optional\n    \"project_name\": \"my_project\",\n    \"ground_truth_ms2_filename\": \"ground_truth\"\n})\n\n# Save the pickle file\nwith open(\"ground_truth.pkl\", \"wb\") as f:\n    f.write(response.content)\n\nprint(\"MS2 spectra computed and saved to ground_truth.pkl\")\n\n# 2. Predict SMILES\nresponse = requests.post(f\"{base_url}/predict_smiles\", json={\n    \"intensities\": [[1.0, 2.0, 3.0]],\n    \"masses\": [[100.0, 200.0, 300.0]],\n    \"path_to_mdl\": \"/path/to/model\",\n    \"trained_model_name\": \"model.pth\",\n    \"project_name\": \"my_project\",\n    \"num_pred\": 5,\n    \"predict_approach\": \"greedy\",\n    \"device\": \"cpu\"\n})\nprint(response.json())\n\n# 3. Generate predicted MS2\nresponse = requests.post(f\"{base_url}/generate_predicted_ms2\", json={\n    \"project_name\": \"my_project\",\n    \"predicted_ms2_filename\": \"predicted_ms2\",\n    \"ionization\": \"[M+H]+\"\n})\nprint(response.json())\n\n# 4. Validate predictions\nresponse = requests.post(f\"{base_url}/validate_predictions\", json={\n    \"project_name\": \"my_project\",\n    \"predicted_ms2_filename\": \"predicted_ms2\",\n    \"ground_truth_ms2_filename\": \"ground_truth\",\n    \"predicted_smiles_filename\": \"final_predictions\"\n})\nprint(response.json())\n</code></pre>"},{"location":"README_flask_api/#error-handling","title":"Error Handling","text":"<p>All endpoints return appropriate HTTP status codes: - <code>200</code>: Success - <code>400</code>: Bad Request (missing or invalid parameters) - <code>500</code>: Internal Server Error</p> <p>Error responses include an <code>error</code> field with a description of what went wrong.</p>"},{"location":"README_flask_api/#configuration","title":"Configuration","text":"<p>The API uses hardcoded paths: - Base output directory: <code>C:\\Users\\daniel.sinisterra\\Documents\\repos\\results\\</code> - Protocol: Always \"PTFI\" - Project folders: Created automatically under the base output directory</p>"},{"location":"README_flask_api/#notes","title":"Notes","text":"<ul> <li>All project files are stored in <code>C:\\Users\\daniel.sinisterra\\Documents\\repos\\results\\PROJECT_NAME\\</code></li> <li>The project folder will be created automatically if it doesn't exist</li> <li>The <code>/compute_ms2</code> endpoint always returns the pickle file as a download</li> <li>Protocol is hardcoded to \"PTFI\" for all operations</li> <li>Make sure you have sufficient disk space for processing large datasets</li> <li>The pickle files contain numpy arrays and can be loaded using <code>pickle.load()</code> </li> </ul>"},{"location":"Workflow/","title":"Workflow","text":""},{"location":"Workflow/#introduction","title":"Introduction","text":"<p>The preprediction pipeline is mainly implemented in the classes <code>PrePredictionWorkflow</code> and <code>PredictWorkflow</code>  from the file Workflow.py.</p> <p>The <code>PrePredictionWorkflow</code> class implements a methodology to obtain the tandem mass spectrums from a list of experiments (MS1 scans) given in .mzML format. All the functionality of such class can be consulted in the API page for it (Preprediction API).</p> <p>The <code>PredictWorkflow</code> class implements a methodology to predict the molecular structure representation (SMILES) of a given tandem mass spectrum. Likewise, all the functionality of this class can be consulted in the API page for it (Prediction API).</p> <p>On the other hand there is also functionality developed to train a Transformer-based neural network architecture to get the SMILES from the tandem mass spectrums. This training process is developed in the classes of the file <code>TrainWorkFlowV2.py</code>. All the functionality of the classes and methods used there can be consulted in Training. </p> <p>All the functionality is available through the methods that that class implements. The specific functionality details are specified in the API documentation.</p>"},{"location":"cfm_id_compilation/","title":"Install CMake and GCC","text":"<pre><code>sudo apt update\nsudo apt install cmake gcc g++ -y\n</code></pre>"},{"location":"cfm_id_compilation/#install-boost-library","title":"Install Boost Library","text":"<p>Install old boost library version (1.60.0)</p> <p>1 - Download an old boost library from BOOST</p> <p>2 - </p>"},{"location":"cfm_id_compilation/#install-specific-rdkit-insource-location","title":"Install specific RDKIT (insource location)","text":"<pre><code>wget https://github.com/rdkit/rdkit/archive/Release_2017_09_3.tar.gz;\ntar -zxvf Release_2017_09_3.tar.gz\ncd ./rdkit-Release_2017_09_3\nmkdir build\ncd build\ncmake .. -DRDK_PGSQL_STATIC=OFF -DRDK_BUILD_PYTHON_WRAPPERS=OFF -DRDK_BUILD_CPP_TESTS=OFF -DRDK_BUILD_DESCRIPTORS3D=OFF -DRDK_INSTALL_STATIC_LIBS=OFF-DRDK_INSTALL_INTREE=ON -DRDK_BUILD_INCHI_SUPPORT=ON -DRDK_OPTIMIZE_NATIVE=ON -DCMAKE_CXX_STANDARD=14 -DCMAKE_BUILD_TYPE=Release\nsudo make install -j8\n</code></pre>"},{"location":"fragmentationTrees/","title":"Using fragmentation trees and mass spectral trees for identifying unknown compounds in metabolomics","text":""},{"location":"fragmentationTrees/#introduction","title":"Introduction","text":"<p>The Metabolomics Standards Initiative (MSI) categorizes structure elucidation into four different levels: </p> <ul> <li>Identification</li> <li>Annotation</li> <li>Characterization</li> <li>Classification</li> </ul> <p>However, MSI does not provide a scoring schema to rank identified compounds within the identified and annotated categories, a caveat that was recently highlighted by metabolomics investigators.</p> <p>Identification of metabolites refers to complete identification of the structure, including molecular connections and stereochemical assignments.</p> <p>Some major differences between metabolomics and proteomics are the presence of multiply-charged ions from peptides and the much larger chemical diversity in metabolomics and exposome analyses.</p> <p>Structure annotations are often ambiguous due to the large number of possible isomers, data inaccuracies, limited amounts of corroborating information, and human errors, including misclassification of sub-structures.</p> <p>However, annotation can also be viewed as a strategy to reduce the need for isolation of compounds and de-novo elucidation. </p> <p>The idea is to annotate mass spectra using the most probable elemental compositions found in public databases and to add additional orthogonal filters to decrease the number of structure hits.</p> <p>Computer-assisted structural elucidation (CASE) encompasses structural dereplication using various analytical techniques like tandem MS (MS2).</p> <p>Essentially, dereplication is a process to identify \u201cknown unknowns\u201d, which are compounds that are unknown at the time of detection and with further investigation are then found to be known compounds</p> <p>CASE first reduces chemical and spectral properties of an unknown compound, second generates candidate structures compatible with spectral features, and then ranks these candidates.</p> <p>CASE can be used when manual interpretation of data is impractical and outcomes are unreliable using certain techniques, such as artificial intelligence, pattern recognition, library search, and spectral simulation.</p> <p>Both structural dereplication and CASE are not considered de-novo identification because they rely on database searches with pre-existing known metabolites or reference standards.</p> <p>Full de-novo identification by MS alone can hardly be achieved because isomers are difficult to distinguish by MS.</p> <p>Mass spectral data inform about elemental compositions by combining accurate mass and isotopic information.</p> <p>Collision-induced fragmentation data on the MS2 or MSn levels are used to find structural information from unique fragmentation patterns to test for the presence and the absence of functional groups.</p>"},{"location":"fragmentationTrees/#trees","title":"Trees","text":"<p>Typically, the graphs are called fragmentation trees, family trees or identification trees, if these trees show the fragmentation pathway of a molecule.</p> <p>Fragmentation trees are generated computationally to predict the fragmentation pathway of a molecule.</p>"},{"location":"future_improvements/","title":"Mass trace detection","text":"<p>Ordered by priority:</p> <ol> <li>Identify possible algorithms</li> <li>Incorporate an ensemble method over multiple algorithms</li> <li>Document available metric</li> <li>Identify new metrics</li> </ol>"},{"location":"future_improvements/#elution-peak-detection","title":"Elution peak detection","text":"<p>Ordered by priority:</p> <ol> <li>Identify possible algorithms</li> <li>Incorporate an ensemble method over multiple algorithms</li> <li>Document available metric</li> <li>Identify new metrics</li> </ol>"},{"location":"future_improvements/#feature-detection","title":"Feature detection","text":"<p>Ordered by priority:</p> <ol> <li>Identify possible algorithms</li> <li>Incorporate an ensemble method over multiple algorithms</li> <li>Document available metric</li> <li>Identify new metrics</li> </ol>"},{"location":"input/","title":"Input Management","text":""},{"location":"input/#ptfifrijolpujc.InputManagement.InputManagement","title":"<code>InputManagement</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>ptfifrijolpujc/InputManagement.py</code> <pre><code>class InputManagement(object):\n    def __init__(self):\n        self.vocab_smi = vocab_smi\n        self.vocab_smi_rev = vocab_smi_rev\n        self.vocab_mz = vocab_mz\n\n    def read_ms_from_excel(self, dir_path=\"./data\", filename=\"Pool moleculas de prueba.xlsx\", index_excel_sheet=0, index_ms2_col=-1, index_smi=0):\n        \"\"\"\n        Read an Excel file with (potentially) multiple SMILES and Mass spectrums.\n\n        The excel file has ONLY ONE sheet with the following format:\n\n        SMILES,     Precursor m/z,      Name,   MSMS spectrum\n        struct1,        1221,    namemolecule1, 12.09 91;13 145;....\n        struct2,          12,    namemolecule2, 11.07 76;17.1 232;....\n\n        Args:\n            dir_path (str, optional): path to directory containing the excel file. Defaults to \"./data\".\n            filename (str, optional): name of the excel file. Defaults to \"Pool moleculas de prueba.xlsx\".\n\n        Returns:\n            MZ (list of lists): Has the m/z values. Its length corresponds to the number of molecules in the file. The length of each entry of the list is the number of samples in the Mass spectrum.\n            ITZ (list of lists): Has the Intentsity values. Its length corresponds to the number of molecules in the file. The length of each entry of the list is the number of samples in the Mass spectrum.\n        \"\"\"\n\n        # Get the Excel sheet\n        file_path = os.path.join(dir_path,filename)\n        workbook = load_workbook(filename=file_path)\n        sheet = workbook[workbook.sheetnames[index_excel_sheet]]\n\n        # Get the columns\n        columns = []\n        for col in sheet.iter_cols(values_only=True):\n            column_data = list(col)\n            columns.append(column_data)\n\n        # Get the Mass spectrum\n        MZ = []\n        ITZ = []\n\n        for ms2 in columns[index_ms2_col][1:]:\n            aux = ms2.split(';')\n            mz = []\n            itz = []\n            for entry in aux:\n                aux2 = entry.split(' ')\n                mz.append(float(aux2[0]))\n                itz.append(float(aux2[1]))\n            MZ.append(mz)\n            ITZ.append(itz)\n\n        smiles = columns[index_smi][1:]\n\n        return MZ, ITZ, smiles\n\n    def make_clean_smiles(self, mz, smiles, prid_num=100):\n        judge = 0\n        SS = []\n        jihe = []\n        enc_input2 = []\n        dec_input2 = []\n        real_smiles = []\n        for i in range(1):\n            mz = [float(x) for x in mz]\n            mz = [int(100 * x) for x in mz]\n            if max(mz) &gt; 50000:\n                continue\n            if len(smiles) &lt; 100:\n                for j in range(len(smiles)):\n                    if judge == 1:\n                        judge = 0\n                    else:\n                        if smiles[j] == 'C' and j &lt; len(smiles) - 1:\n                            if smiles[j + 1] == 'l':\n                                SS.append('Cl')\n                                judge = 1\n                                continue\n                        if smiles[j] == 'B' and j &lt; len(smiles) - 1:\n                            if smiles[j + 1] == 'r':\n                                SS.append('Br')\n                                judge = 1\n                                continue\n                        if smiles[j] == '\\xa0': # detects a space at the end of the line in the string\n                            continue\n                        SS.append(smiles[j])\n                SS = ['&lt;SOS&gt;'] + SS + ['&lt;EOS&gt;']\n                for j in range(0, 100):\n                    mz.append(0)\n                    SS.append('&lt;PAD&gt;')\n                enc_input = [[self.vocab_mz[n] for n in mz]]\n                dec_input = [[self.vocab_smi[n] for n in SS]]\n                enc_input2.append(enc_input[0][0:100])\n                dec_input2.append(dec_input[0][0:101])\n                real_smiles.append(smiles)\n                SS = []\n\n        return enc_input2, dec_input2, real_smiles\n\n    def load_data_in_three_energy_levels_from_databases(self, filepath=\"/home/julian/Documents/Librerias publicas/\", he=36, me=16, save_info=False):\n        \"\"\"\n        This method loads all the compounds available in the files allocated under `filepath`, and only returns the compounds for which there is a mass spectra in each of the three levels of collision energy. \n\n        These files correspond to database `.msp` files containing information of the compound such as its name, precursormz, mass spectra, smiles, inkikey, among others. \n\n        There are repeated compounds. The criterion to filter repeated compounds (choose one of them) is to pick the one having the more data in the mass spectra.\n\n        Args:\n            filepath (str, optional): path to the directory under which are allocated all the files containing compounds information. Defaults to \"/home/julian/Documents/Librerias publicas/\".\n            he (int, optional): cutoff value above which are the high level collision energy compounds. Defaults to 36.\n            me (int, optional): cutoff value above which are the medium level collision energy compounds. Defaults to 16.\n            save_info (bool, optional): if true saves some useful information about all the scraped database files. Defaults to False.\n\n        Returns:\n            le_compounds_in_three_energy (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a low-level collision energy.\n            me_compounds_in_three_energy (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a medium-level collision energy.\n            he_compounds_in_three_energy (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a high-level collision energy. \n        \"\"\"\n\n        he_compounds = []\n        me_compounds = []\n        le_compounds = []\n\n        total_compounds_count = 0\n        total_compounds_have_energy_info = 0\n\n        for file_i in os.listdir(filepath):\n            aux = file_i.split(\".\")\n            if aux[1] == 'npy':\n                this_database = np.load(os.path.join(filepath, file_i), allow_pickle=True)\n                for entry in this_database:\n                    for key, val in entry.items():\n                        if key == \"COLLISIONENERGY\" and val != '' and val != \"--\":\n                            aux2 = val.split(\"HCD\")\n                            if len(aux2) == 1:\n                                val = val.split(\" \")\n                                val = float(val[0])\n                            else:\n                                val = float(aux2[0])\n                            if val &gt; 0 and val &lt; me:\n                                le_compounds.append(entry)\n                            if val &gt;= me and val &lt; he:\n                                me_compounds.append(entry)\n                            if val &gt;= he:\n                                he_compounds.append(entry)\n\n                            total_compounds_have_energy_info = total_compounds_have_energy_info + 1\n\n                    total_compounds_count = total_compounds_count + 1\n\n        names_le = {d[\"NAME\"] for d in le_compounds}\n        names_me = {d[\"NAME\"] for d in me_compounds}\n        names_he = {d[\"NAME\"] for d in he_compounds}\n\n        names_in_three_energy_levels = names_le.intersection(names_me, names_he)\n        le_compounds_in_three_energy = []\n        me_compounds_in_three_energy = []\n        he_compounds_in_three_energy = []\n\n        last_num_peaks_le = 0\n        last_num_peaks_me = 0\n        last_num_peaks_he = 0\n\n        for name in names_in_three_energy_levels:\n            for d in le_compounds:\n                if d[\"NAME\"] == name and float(d[\"Num Peaks\"])&gt; last_num_peaks_le:\n                    last_le_compound = d\n                    last_num_peaks_le = float(d[\"Num Peaks\"])\n            if last_num_peaks_le &gt; 0:\n                le_compounds_in_three_energy.append(last_le_compound)\n                last_num_peaks_le = 0\n            for d in me_compounds:\n                if d[\"NAME\"] == name and float(d[\"Num Peaks\"])&gt; last_num_peaks_me:\n                    last_me_compound = d\n                    last_num_peaks_me = float(d[\"Num Peaks\"])\n            if last_num_peaks_me &gt; 0:\n                me_compounds_in_three_energy.append(last_me_compound)\n                last_num_peaks_me = 0\n            for d in he_compounds:\n                if d[\"NAME\"] == name and float(d[\"Num Peaks\"])&gt; last_num_peaks_he:\n                    last_he_compounds = d\n                    last_num_peaks_he = float(d[\"Num Peaks\"])\n            if last_num_peaks_he &gt; 0:\n                he_compounds_in_three_energy.append(last_he_compounds)\n                last_num_peaks_he = 0\n        if save_info:\n            DB_INFO = {\"NumberOfCompoundsIn3EnergyLevels\": len(names_in_three_energy_levels), \n                    \"NumberOfCompoundsInAllDB\": total_compounds_count,\n                    \"NumberOfCompoundsHaveEnergyInfo\": total_compounds_have_energy_info}\n\n            np.save(os.path.join(filepath, \"info_databases\"), DB_INFO)\n\n        return le_compounds_in_three_energy, me_compounds_in_three_energy, he_compounds_in_three_energy\n\n    def load_data_in_ionmodes_from_databases(self, filepath=\"/home/julian/Documents/Librerias publicas/\", save_info=False):\n        \"\"\"\n        This method loads all the compounds available in the files allocated under `filepath`, and only returns the compounds for which there is a mass spectra in each of the two ion modes. \n\n        These files correspond to database `.msp` files containing information of the compound such as its name, precursormz, mass spectra, smiles, inkikey, among others. \n\n        There are repeated compounds. The criterion to filter repeated compounds (choose one of them) is to pick the one having the more data in the mass spectra.\n\n        Args:\n            filepath (str, optional): path to the directory under which are allocated all the files containing compounds information. Defaults to \"/home/julian/Documents/Librerias publicas/\".\n            save_info (bool, optional): if true saves some useful information about all the scraped database files. Defaults to False.\n\n        Returns:\n            ionmode_positive_compounds_in_both_iomodes (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a positive ion mode.\n            ionmode_negative_compounds_in_both_iomodes (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a negative ion mode.\n        \"\"\"\n\n        ionmode_positive_compounds = []\n        ionmode_negative_compounds = []\n\n        total_compounds_count = 0\n        total_compounds_have_ionmode_info = 0\n\n        for file_i in os.listdir(filepath):\n            aux = file_i.split(\".\")\n            if aux[1] == 'npy':\n                this_database = np.load(os.path.join(filepath, file_i), allow_pickle=True)\n                for entry in this_database:\n                    for key, val in entry.items():\n                        if key == \"IONMODE\" and val != '' and val != \"--\":\n                            if val == \"Positive\":\n                                ionmode_positive_compounds.append(entry)\n                            if val == \"Negative\":\n                                ionmode_negative_compounds.append(entry)\n                            else:\n                                continue\n\n                            total_compounds_have_ionmode_info = total_compounds_have_ionmode_info + 1\n\n                    total_compounds_count = total_compounds_count + 1\n\n        names_ionmode_positive = {d[\"NAME\"] for d in ionmode_positive_compounds}\n        names_ionmode_negative = {d[\"NAME\"] for d in ionmode_negative_compounds}\n\n        names_in_both_ionmode = names_ionmode_positive.intersection(names_ionmode_negative)\n\n        ionmode_positive_compounds_in_both_iomodes = []\n        ionmode_negative_compounds_in_both_iomodes = []\n\n        last_num_peaks_ionmode_positive = 0\n        last_num_peaks_ionmode_negative = 0\n\n        for name in names_in_both_ionmode:\n            for d in ionmode_positive_compounds:\n                if d[\"NAME\"] == name and float(d[\"Num Peaks\"])&gt; last_num_peaks_ionmode_positive:\n                    last_ionmode_positive_compound = d\n                    last_num_peaks_ionmode_positive = float(d[\"Num Peaks\"])\n            if last_num_peaks_ionmode_positive &gt; 0:\n                ionmode_positive_compounds_in_both_iomodes.append(last_ionmode_positive_compound)\n                last_num_peaks_ionmode_positive = 0\n            for d in ionmode_negative_compounds:\n                if d[\"NAME\"] == name and float(d[\"Num Peaks\"])&gt; last_num_peaks_ionmode_negative:\n                    last_ionmode_negative_compound = d\n                    last_num_peaks_ionmode_negative = float(d[\"Num Peaks\"])\n            if last_num_peaks_ionmode_negative &gt; 0:\n                ionmode_negative_compounds_in_both_iomodes.append(last_ionmode_negative_compound)\n                last_num_peaks_ionmode_negative = 0\n        if save_info:\n            DB_INFO = {\"NumberOfCompoundsInBothIonModes\": len(names_in_both_ionmode), \n                    \"NumberOfCompoundsInAllDB\": total_compounds_count,\n                    \"NumberOfCompoundsHaveIonModeInfo\": total_compounds_have_ionmode_info}\n\n            np.save(os.path.join(filepath, \"info_databases_ionmode\"), DB_INFO)\n\n        return ionmode_positive_compounds_in_both_iomodes, ionmode_negative_compounds_in_both_iomodes    \n\n    def load_all_compounds_database(self, filepath=\"/home/julian/Documents/Librerias publicas/\", min_num_peaks=3):\n        \"\"\"\n        Concatenates in a list the compounds of (possibly) multiple pickle files, each of them having molecular information for multiple compounds.\n\n        The pickle files are all under the directory `filepath`.\n\n        Args:\n            filepath (str, optional): path to directory under which are located all the pickle files with compound info. Defaults to \"/home/julian/Documents/Librerias publicas/\".\n            min_num_peaks (int, optional): only save compounds having mass spectra with a minimum number of peaks given by this parameter. Defaults to 3.\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        compounds = []\n\n        for file_i in os.listdir(filepath):\n            aux = file_i.split(\".\")\n            if aux[1] == 'npy':\n                this_database = np.load(os.path.join(filepath, file_i), allow_pickle=True)\n                for entry in this_database:\n                    for key, val in entry.items():\n                        if key == \"Num Peaks\" and float(val) &gt;= min_num_peaks:\n                            compounds.append(entry)\n\n        return compounds\n\n    def load_compounds_from_file(self, filepath, idx=None):\n        \"\"\"\n        Returns the m/z, intensities and smiles contained in the pickle file `filepath`.\n\n        The file might contain m/z, intensities, smiles for more than 1 compound.\n\n        Args:\n            filepath (str): path to the pickle file\n            idx (int, optional): the index of a particular molecule from the pickle file. Defaults to None.\n\n        Returns:\n            INT (list of lists): each sublist has the intensities (relative abundance) for each compound in the file.\n            MZ (list of lists): each sublist has the m/z for each compound in the file.\n            SMILES (list): each entry has the SMILES for each compound in the file.\n        \"\"\"\n        molecules = np.load(filepath, allow_pickle=True)\n\n        if idx is not None:\n            mol_of_interest = []\n\n            for i in idx:\n                mol_of_interest.append(molecules[i])\n        else:\n            mol_of_interest = molecules\n\n        INT = []\n        MZ = []\n        SMILES = []\n\n        for mol in mol_of_interest:\n            intensities = []\n            masscharges = []\n            for i in mol['peaks']:\n                intensities.append(i[1])\n                masscharges.append(i[0])\n\n            SMILES.append(mol['SMILES'])\n            INT.append(intensities)\n            MZ.append(masscharges)\n\n        return INT, MZ, SMILES\n\n    def get_mz_and_int_from_list(self, list_compounds):\n        \"\"\"\n        Gets the mass-charge and intensities values from a list of compounds of the type produced by `load_data_in_three_energy_levels_from_databases` or `load_data_in_ionmodes_from_databases`.\n\n        Args:\n            list_compounds (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra \n\n        Returns:\n            MZ (list of lists): each sub-list is for a compound, and has the mass-charge values of the compound\n            INT (list of lists): each sub-list is for a compound, and has the intensity values of the compound\n        \"\"\"\n        MZ = []\n        INT = []\n        for compound_dict in list_compounds:\n            peaks = compound_dict[\"peaks\"]\n            this_mz = [x[0] for x in peaks]\n            this_int = [x[1] for x in peaks]\n            MZ.append(this_mz)\n            INT.append(this_int)\n\n        return MZ, INT\n\n    def get_mz_and_smiles_from_list(self, list_compounds):\n        \"\"\"\n        Gets the m/z and SMILES from a list of compounds of the type produced by `load_data_in_three_energy_levels_from_databases` or `load_data_in_ionmodes_from_databases`.\n\n        Args:\n            list_compounds (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra\n\n        Returns:\n            dataset (list of dicts): each dictionary of the list represents a compound, and has two keys: \"smiles\" and \"mz\", for the SMILES of the compound and its mass-charge values, respectively\n            precursormz (list of dicts): each dictionary of the list represents a compound and has two keys: \"smiles\" and \"precursormz\", for the SMILES of the compound and its precursor mass\n        \"\"\"\n\n        dataset = []\n        precursormz = []\n        for compound_dict in list_compounds:\n            peaks = compound_dict[\"peaks\"]\n            this_mz = [x[0] for x in peaks]\n            dataset.append({\"smiles\": compound_dict[\"SMILES\"], \"mz\": this_mz})\n            precursormz.append({\"smiles\": compound_dict[\"SMILES\"], \"precursormz\": compound_dict[\"PRECURSORMZ\"]})\n        return dataset, precursormz\n</code></pre>"},{"location":"input/#ptfifrijolpujc.InputManagement.InputManagement.get_mz_and_int_from_list","title":"<code>get_mz_and_int_from_list(list_compounds)</code>","text":"<p>Gets the mass-charge and intensities values from a list of compounds of the type produced by <code>load_data_in_three_energy_levels_from_databases</code> or <code>load_data_in_ionmodes_from_databases</code>.</p> <p>Parameters:</p> Name Type Description Default <code>list_compounds</code> <code>list of dicts</code> <p>each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra </p> required <p>Returns:</p> Name Type Description <code>MZ</code> <code>list of lists</code> <p>each sub-list is for a compound, and has the mass-charge values of the compound</p> <code>INT</code> <code>list of lists</code> <p>each sub-list is for a compound, and has the intensity values of the compound</p> Source code in <code>ptfifrijolpujc/InputManagement.py</code> <pre><code>def get_mz_and_int_from_list(self, list_compounds):\n    \"\"\"\n    Gets the mass-charge and intensities values from a list of compounds of the type produced by `load_data_in_three_energy_levels_from_databases` or `load_data_in_ionmodes_from_databases`.\n\n    Args:\n        list_compounds (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra \n\n    Returns:\n        MZ (list of lists): each sub-list is for a compound, and has the mass-charge values of the compound\n        INT (list of lists): each sub-list is for a compound, and has the intensity values of the compound\n    \"\"\"\n    MZ = []\n    INT = []\n    for compound_dict in list_compounds:\n        peaks = compound_dict[\"peaks\"]\n        this_mz = [x[0] for x in peaks]\n        this_int = [x[1] for x in peaks]\n        MZ.append(this_mz)\n        INT.append(this_int)\n\n    return MZ, INT\n</code></pre>"},{"location":"input/#ptfifrijolpujc.InputManagement.InputManagement.get_mz_and_smiles_from_list","title":"<code>get_mz_and_smiles_from_list(list_compounds)</code>","text":"<p>Gets the m/z and SMILES from a list of compounds of the type produced by <code>load_data_in_three_energy_levels_from_databases</code> or <code>load_data_in_ionmodes_from_databases</code>.</p> <p>Parameters:</p> Name Type Description Default <code>list_compounds</code> <code>list of dicts</code> <p>each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra</p> required <p>Returns:</p> Name Type Description <code>dataset</code> <code>list of dicts</code> <p>each dictionary of the list represents a compound, and has two keys: \"smiles\" and \"mz\", for the SMILES of the compound and its mass-charge values, respectively</p> <code>precursormz</code> <code>list of dicts</code> <p>each dictionary of the list represents a compound and has two keys: \"smiles\" and \"precursormz\", for the SMILES of the compound and its precursor mass</p> Source code in <code>ptfifrijolpujc/InputManagement.py</code> <pre><code>def get_mz_and_smiles_from_list(self, list_compounds):\n    \"\"\"\n    Gets the m/z and SMILES from a list of compounds of the type produced by `load_data_in_three_energy_levels_from_databases` or `load_data_in_ionmodes_from_databases`.\n\n    Args:\n        list_compounds (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra\n\n    Returns:\n        dataset (list of dicts): each dictionary of the list represents a compound, and has two keys: \"smiles\" and \"mz\", for the SMILES of the compound and its mass-charge values, respectively\n        precursormz (list of dicts): each dictionary of the list represents a compound and has two keys: \"smiles\" and \"precursormz\", for the SMILES of the compound and its precursor mass\n    \"\"\"\n\n    dataset = []\n    precursormz = []\n    for compound_dict in list_compounds:\n        peaks = compound_dict[\"peaks\"]\n        this_mz = [x[0] for x in peaks]\n        dataset.append({\"smiles\": compound_dict[\"SMILES\"], \"mz\": this_mz})\n        precursormz.append({\"smiles\": compound_dict[\"SMILES\"], \"precursormz\": compound_dict[\"PRECURSORMZ\"]})\n    return dataset, precursormz\n</code></pre>"},{"location":"input/#ptfifrijolpujc.InputManagement.InputManagement.load_all_compounds_database","title":"<code>load_all_compounds_database(filepath='/home/julian/Documents/Librerias publicas/', min_num_peaks=3)</code>","text":"<p>Concatenates in a list the compounds of (possibly) multiple pickle files, each of them having molecular information for multiple compounds.</p> <p>The pickle files are all under the directory <code>filepath</code>.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>path to directory under which are located all the pickle files with compound info. Defaults to \"/home/julian/Documents/Librerias publicas/\".</p> <code>'/home/julian/Documents/Librerias publicas/'</code> <code>min_num_peaks</code> <code>int</code> <p>only save compounds having mass spectra with a minimum number of peaks given by this parameter. Defaults to 3.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>ptfifrijolpujc/InputManagement.py</code> <pre><code>def load_all_compounds_database(self, filepath=\"/home/julian/Documents/Librerias publicas/\", min_num_peaks=3):\n    \"\"\"\n    Concatenates in a list the compounds of (possibly) multiple pickle files, each of them having molecular information for multiple compounds.\n\n    The pickle files are all under the directory `filepath`.\n\n    Args:\n        filepath (str, optional): path to directory under which are located all the pickle files with compound info. Defaults to \"/home/julian/Documents/Librerias publicas/\".\n        min_num_peaks (int, optional): only save compounds having mass spectra with a minimum number of peaks given by this parameter. Defaults to 3.\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    compounds = []\n\n    for file_i in os.listdir(filepath):\n        aux = file_i.split(\".\")\n        if aux[1] == 'npy':\n            this_database = np.load(os.path.join(filepath, file_i), allow_pickle=True)\n            for entry in this_database:\n                for key, val in entry.items():\n                    if key == \"Num Peaks\" and float(val) &gt;= min_num_peaks:\n                        compounds.append(entry)\n\n    return compounds\n</code></pre>"},{"location":"input/#ptfifrijolpujc.InputManagement.InputManagement.load_compounds_from_file","title":"<code>load_compounds_from_file(filepath, idx=None)</code>","text":"<p>Returns the m/z, intensities and smiles contained in the pickle file <code>filepath</code>.</p> <p>The file might contain m/z, intensities, smiles for more than 1 compound.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>path to the pickle file</p> required <code>idx</code> <code>int</code> <p>the index of a particular molecule from the pickle file. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>INT</code> <code>list of lists</code> <p>each sublist has the intensities (relative abundance) for each compound in the file.</p> <code>MZ</code> <code>list of lists</code> <p>each sublist has the m/z for each compound in the file.</p> <code>SMILES</code> <code>list</code> <p>each entry has the SMILES for each compound in the file.</p> Source code in <code>ptfifrijolpujc/InputManagement.py</code> <pre><code>def load_compounds_from_file(self, filepath, idx=None):\n    \"\"\"\n    Returns the m/z, intensities and smiles contained in the pickle file `filepath`.\n\n    The file might contain m/z, intensities, smiles for more than 1 compound.\n\n    Args:\n        filepath (str): path to the pickle file\n        idx (int, optional): the index of a particular molecule from the pickle file. Defaults to None.\n\n    Returns:\n        INT (list of lists): each sublist has the intensities (relative abundance) for each compound in the file.\n        MZ (list of lists): each sublist has the m/z for each compound in the file.\n        SMILES (list): each entry has the SMILES for each compound in the file.\n    \"\"\"\n    molecules = np.load(filepath, allow_pickle=True)\n\n    if idx is not None:\n        mol_of_interest = []\n\n        for i in idx:\n            mol_of_interest.append(molecules[i])\n    else:\n        mol_of_interest = molecules\n\n    INT = []\n    MZ = []\n    SMILES = []\n\n    for mol in mol_of_interest:\n        intensities = []\n        masscharges = []\n        for i in mol['peaks']:\n            intensities.append(i[1])\n            masscharges.append(i[0])\n\n        SMILES.append(mol['SMILES'])\n        INT.append(intensities)\n        MZ.append(masscharges)\n\n    return INT, MZ, SMILES\n</code></pre>"},{"location":"input/#ptfifrijolpujc.InputManagement.InputManagement.load_data_in_ionmodes_from_databases","title":"<code>load_data_in_ionmodes_from_databases(filepath='/home/julian/Documents/Librerias publicas/', save_info=False)</code>","text":"<p>This method loads all the compounds available in the files allocated under <code>filepath</code>, and only returns the compounds for which there is a mass spectra in each of the two ion modes. </p> <p>These files correspond to database <code>.msp</code> files containing information of the compound such as its name, precursormz, mass spectra, smiles, inkikey, among others. </p> <p>There are repeated compounds. The criterion to filter repeated compounds (choose one of them) is to pick the one having the more data in the mass spectra.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>path to the directory under which are allocated all the files containing compounds information. Defaults to \"/home/julian/Documents/Librerias publicas/\".</p> <code>'/home/julian/Documents/Librerias publicas/'</code> <code>save_info</code> <code>bool</code> <p>if true saves some useful information about all the scraped database files. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ionmode_positive_compounds_in_both_iomodes</code> <code>list of dicts</code> <p>each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a positive ion mode.</p> <code>ionmode_negative_compounds_in_both_iomodes</code> <code>list of dicts</code> <p>each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a negative ion mode.</p> Source code in <code>ptfifrijolpujc/InputManagement.py</code> <pre><code>def load_data_in_ionmodes_from_databases(self, filepath=\"/home/julian/Documents/Librerias publicas/\", save_info=False):\n    \"\"\"\n    This method loads all the compounds available in the files allocated under `filepath`, and only returns the compounds for which there is a mass spectra in each of the two ion modes. \n\n    These files correspond to database `.msp` files containing information of the compound such as its name, precursormz, mass spectra, smiles, inkikey, among others. \n\n    There are repeated compounds. The criterion to filter repeated compounds (choose one of them) is to pick the one having the more data in the mass spectra.\n\n    Args:\n        filepath (str, optional): path to the directory under which are allocated all the files containing compounds information. Defaults to \"/home/julian/Documents/Librerias publicas/\".\n        save_info (bool, optional): if true saves some useful information about all the scraped database files. Defaults to False.\n\n    Returns:\n        ionmode_positive_compounds_in_both_iomodes (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a positive ion mode.\n        ionmode_negative_compounds_in_both_iomodes (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a negative ion mode.\n    \"\"\"\n\n    ionmode_positive_compounds = []\n    ionmode_negative_compounds = []\n\n    total_compounds_count = 0\n    total_compounds_have_ionmode_info = 0\n\n    for file_i in os.listdir(filepath):\n        aux = file_i.split(\".\")\n        if aux[1] == 'npy':\n            this_database = np.load(os.path.join(filepath, file_i), allow_pickle=True)\n            for entry in this_database:\n                for key, val in entry.items():\n                    if key == \"IONMODE\" and val != '' and val != \"--\":\n                        if val == \"Positive\":\n                            ionmode_positive_compounds.append(entry)\n                        if val == \"Negative\":\n                            ionmode_negative_compounds.append(entry)\n                        else:\n                            continue\n\n                        total_compounds_have_ionmode_info = total_compounds_have_ionmode_info + 1\n\n                total_compounds_count = total_compounds_count + 1\n\n    names_ionmode_positive = {d[\"NAME\"] for d in ionmode_positive_compounds}\n    names_ionmode_negative = {d[\"NAME\"] for d in ionmode_negative_compounds}\n\n    names_in_both_ionmode = names_ionmode_positive.intersection(names_ionmode_negative)\n\n    ionmode_positive_compounds_in_both_iomodes = []\n    ionmode_negative_compounds_in_both_iomodes = []\n\n    last_num_peaks_ionmode_positive = 0\n    last_num_peaks_ionmode_negative = 0\n\n    for name in names_in_both_ionmode:\n        for d in ionmode_positive_compounds:\n            if d[\"NAME\"] == name and float(d[\"Num Peaks\"])&gt; last_num_peaks_ionmode_positive:\n                last_ionmode_positive_compound = d\n                last_num_peaks_ionmode_positive = float(d[\"Num Peaks\"])\n        if last_num_peaks_ionmode_positive &gt; 0:\n            ionmode_positive_compounds_in_both_iomodes.append(last_ionmode_positive_compound)\n            last_num_peaks_ionmode_positive = 0\n        for d in ionmode_negative_compounds:\n            if d[\"NAME\"] == name and float(d[\"Num Peaks\"])&gt; last_num_peaks_ionmode_negative:\n                last_ionmode_negative_compound = d\n                last_num_peaks_ionmode_negative = float(d[\"Num Peaks\"])\n        if last_num_peaks_ionmode_negative &gt; 0:\n            ionmode_negative_compounds_in_both_iomodes.append(last_ionmode_negative_compound)\n            last_num_peaks_ionmode_negative = 0\n    if save_info:\n        DB_INFO = {\"NumberOfCompoundsInBothIonModes\": len(names_in_both_ionmode), \n                \"NumberOfCompoundsInAllDB\": total_compounds_count,\n                \"NumberOfCompoundsHaveIonModeInfo\": total_compounds_have_ionmode_info}\n\n        np.save(os.path.join(filepath, \"info_databases_ionmode\"), DB_INFO)\n\n    return ionmode_positive_compounds_in_both_iomodes, ionmode_negative_compounds_in_both_iomodes    \n</code></pre>"},{"location":"input/#ptfifrijolpujc.InputManagement.InputManagement.load_data_in_three_energy_levels_from_databases","title":"<code>load_data_in_three_energy_levels_from_databases(filepath='/home/julian/Documents/Librerias publicas/', he=36, me=16, save_info=False)</code>","text":"<p>This method loads all the compounds available in the files allocated under <code>filepath</code>, and only returns the compounds for which there is a mass spectra in each of the three levels of collision energy. </p> <p>These files correspond to database <code>.msp</code> files containing information of the compound such as its name, precursormz, mass spectra, smiles, inkikey, among others. </p> <p>There are repeated compounds. The criterion to filter repeated compounds (choose one of them) is to pick the one having the more data in the mass spectra.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>path to the directory under which are allocated all the files containing compounds information. Defaults to \"/home/julian/Documents/Librerias publicas/\".</p> <code>'/home/julian/Documents/Librerias publicas/'</code> <code>he</code> <code>int</code> <p>cutoff value above which are the high level collision energy compounds. Defaults to 36.</p> <code>36</code> <code>me</code> <code>int</code> <p>cutoff value above which are the medium level collision energy compounds. Defaults to 16.</p> <code>16</code> <code>save_info</code> <code>bool</code> <p>if true saves some useful information about all the scraped database files. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>le_compounds_in_three_energy</code> <code>list of dicts</code> <p>each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a low-level collision energy.</p> <code>me_compounds_in_three_energy</code> <code>list of dicts</code> <p>each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a medium-level collision energy.</p> <code>he_compounds_in_three_energy</code> <code>list of dicts</code> <p>each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a high-level collision energy.</p> Source code in <code>ptfifrijolpujc/InputManagement.py</code> <pre><code>def load_data_in_three_energy_levels_from_databases(self, filepath=\"/home/julian/Documents/Librerias publicas/\", he=36, me=16, save_info=False):\n    \"\"\"\n    This method loads all the compounds available in the files allocated under `filepath`, and only returns the compounds for which there is a mass spectra in each of the three levels of collision energy. \n\n    These files correspond to database `.msp` files containing information of the compound such as its name, precursormz, mass spectra, smiles, inkikey, among others. \n\n    There are repeated compounds. The criterion to filter repeated compounds (choose one of them) is to pick the one having the more data in the mass spectra.\n\n    Args:\n        filepath (str, optional): path to the directory under which are allocated all the files containing compounds information. Defaults to \"/home/julian/Documents/Librerias publicas/\".\n        he (int, optional): cutoff value above which are the high level collision energy compounds. Defaults to 36.\n        me (int, optional): cutoff value above which are the medium level collision energy compounds. Defaults to 16.\n        save_info (bool, optional): if true saves some useful information about all the scraped database files. Defaults to False.\n\n    Returns:\n        le_compounds_in_three_energy (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a low-level collision energy.\n        me_compounds_in_three_energy (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a medium-level collision energy.\n        he_compounds_in_three_energy (list of dicts): each entry of the list is a dictionary having information of the compound such as its name, smiles, inkikey and mass spectra for a high-level collision energy. \n    \"\"\"\n\n    he_compounds = []\n    me_compounds = []\n    le_compounds = []\n\n    total_compounds_count = 0\n    total_compounds_have_energy_info = 0\n\n    for file_i in os.listdir(filepath):\n        aux = file_i.split(\".\")\n        if aux[1] == 'npy':\n            this_database = np.load(os.path.join(filepath, file_i), allow_pickle=True)\n            for entry in this_database:\n                for key, val in entry.items():\n                    if key == \"COLLISIONENERGY\" and val != '' and val != \"--\":\n                        aux2 = val.split(\"HCD\")\n                        if len(aux2) == 1:\n                            val = val.split(\" \")\n                            val = float(val[0])\n                        else:\n                            val = float(aux2[0])\n                        if val &gt; 0 and val &lt; me:\n                            le_compounds.append(entry)\n                        if val &gt;= me and val &lt; he:\n                            me_compounds.append(entry)\n                        if val &gt;= he:\n                            he_compounds.append(entry)\n\n                        total_compounds_have_energy_info = total_compounds_have_energy_info + 1\n\n                total_compounds_count = total_compounds_count + 1\n\n    names_le = {d[\"NAME\"] for d in le_compounds}\n    names_me = {d[\"NAME\"] for d in me_compounds}\n    names_he = {d[\"NAME\"] for d in he_compounds}\n\n    names_in_three_energy_levels = names_le.intersection(names_me, names_he)\n    le_compounds_in_three_energy = []\n    me_compounds_in_three_energy = []\n    he_compounds_in_three_energy = []\n\n    last_num_peaks_le = 0\n    last_num_peaks_me = 0\n    last_num_peaks_he = 0\n\n    for name in names_in_three_energy_levels:\n        for d in le_compounds:\n            if d[\"NAME\"] == name and float(d[\"Num Peaks\"])&gt; last_num_peaks_le:\n                last_le_compound = d\n                last_num_peaks_le = float(d[\"Num Peaks\"])\n        if last_num_peaks_le &gt; 0:\n            le_compounds_in_three_energy.append(last_le_compound)\n            last_num_peaks_le = 0\n        for d in me_compounds:\n            if d[\"NAME\"] == name and float(d[\"Num Peaks\"])&gt; last_num_peaks_me:\n                last_me_compound = d\n                last_num_peaks_me = float(d[\"Num Peaks\"])\n        if last_num_peaks_me &gt; 0:\n            me_compounds_in_three_energy.append(last_me_compound)\n            last_num_peaks_me = 0\n        for d in he_compounds:\n            if d[\"NAME\"] == name and float(d[\"Num Peaks\"])&gt; last_num_peaks_he:\n                last_he_compounds = d\n                last_num_peaks_he = float(d[\"Num Peaks\"])\n        if last_num_peaks_he &gt; 0:\n            he_compounds_in_three_energy.append(last_he_compounds)\n            last_num_peaks_he = 0\n    if save_info:\n        DB_INFO = {\"NumberOfCompoundsIn3EnergyLevels\": len(names_in_three_energy_levels), \n                \"NumberOfCompoundsInAllDB\": total_compounds_count,\n                \"NumberOfCompoundsHaveEnergyInfo\": total_compounds_have_energy_info}\n\n        np.save(os.path.join(filepath, \"info_databases\"), DB_INFO)\n\n    return le_compounds_in_three_energy, me_compounds_in_three_energy, he_compounds_in_three_energy\n</code></pre>"},{"location":"input/#ptfifrijolpujc.InputManagement.InputManagement.read_ms_from_excel","title":"<code>read_ms_from_excel(dir_path='./data', filename='Pool moleculas de prueba.xlsx', index_excel_sheet=0, index_ms2_col=-1, index_smi=0)</code>","text":"<p>Read an Excel file with (potentially) multiple SMILES and Mass spectrums.</p> <p>The excel file has ONLY ONE sheet with the following format:</p> <p>SMILES,     Precursor m/z,      Name,   MSMS spectrum struct1,        1221,    namemolecule1, 12.09 91;13 145;.... struct2,          12,    namemolecule2, 11.07 76;17.1 232;....</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>str</code> <p>path to directory containing the excel file. Defaults to \"./data\".</p> <code>'./data'</code> <code>filename</code> <code>str</code> <p>name of the excel file. Defaults to \"Pool moleculas de prueba.xlsx\".</p> <code>'Pool moleculas de prueba.xlsx'</code> <p>Returns:</p> Name Type Description <code>MZ</code> <code>list of lists</code> <p>Has the m/z values. Its length corresponds to the number of molecules in the file. The length of each entry of the list is the number of samples in the Mass spectrum.</p> <code>ITZ</code> <code>list of lists</code> <p>Has the Intentsity values. Its length corresponds to the number of molecules in the file. The length of each entry of the list is the number of samples in the Mass spectrum.</p> Source code in <code>ptfifrijolpujc/InputManagement.py</code> <pre><code>def read_ms_from_excel(self, dir_path=\"./data\", filename=\"Pool moleculas de prueba.xlsx\", index_excel_sheet=0, index_ms2_col=-1, index_smi=0):\n    \"\"\"\n    Read an Excel file with (potentially) multiple SMILES and Mass spectrums.\n\n    The excel file has ONLY ONE sheet with the following format:\n\n    SMILES,     Precursor m/z,      Name,   MSMS spectrum\n    struct1,        1221,    namemolecule1, 12.09 91;13 145;....\n    struct2,          12,    namemolecule2, 11.07 76;17.1 232;....\n\n    Args:\n        dir_path (str, optional): path to directory containing the excel file. Defaults to \"./data\".\n        filename (str, optional): name of the excel file. Defaults to \"Pool moleculas de prueba.xlsx\".\n\n    Returns:\n        MZ (list of lists): Has the m/z values. Its length corresponds to the number of molecules in the file. The length of each entry of the list is the number of samples in the Mass spectrum.\n        ITZ (list of lists): Has the Intentsity values. Its length corresponds to the number of molecules in the file. The length of each entry of the list is the number of samples in the Mass spectrum.\n    \"\"\"\n\n    # Get the Excel sheet\n    file_path = os.path.join(dir_path,filename)\n    workbook = load_workbook(filename=file_path)\n    sheet = workbook[workbook.sheetnames[index_excel_sheet]]\n\n    # Get the columns\n    columns = []\n    for col in sheet.iter_cols(values_only=True):\n        column_data = list(col)\n        columns.append(column_data)\n\n    # Get the Mass spectrum\n    MZ = []\n    ITZ = []\n\n    for ms2 in columns[index_ms2_col][1:]:\n        aux = ms2.split(';')\n        mz = []\n        itz = []\n        for entry in aux:\n            aux2 = entry.split(' ')\n            mz.append(float(aux2[0]))\n            itz.append(float(aux2[1]))\n        MZ.append(mz)\n        ITZ.append(itz)\n\n    smiles = columns[index_smi][1:]\n\n    return MZ, ITZ, smiles\n</code></pre>"},{"location":"input/#ptfifrijolpujc.InputManagement.MoleculesAndSpectraDataset","title":"<code>MoleculesAndSpectraDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>ptfifrijolpujc/InputManagement.py</code> <pre><code>class MoleculesAndSpectraDataset(Dataset):\n    def __init__(self, sentences, \n                 tokenizer, \n                 max_mz=500, \n                 target_length=100, \n                 pad_data_aug=True, \n                 max_outputs=20,\n                 additional_features=True, \n                 precursor_mz=False):\n        super(MoleculesAndSpectraDataset, self).__init__()\n        self.additional_features = additional_features\n        self.precursor_mz = precursor_mz\n        self.sentences = sentences\n        self.tokenizer = tokenizer\n        self.max_mz = max_mz\n        self.target_length = target_length\n        self.max_outputs = max_outputs\n        self.pad_data_aug = pad_data_aug\n\n    def create_random_padded_versions(self, original_list):\n        # This is guaranteed by the caller\n        #if len(original_list) &gt; self.target_length:\n        #    return []\n\n        padded_versions = []\n        original_length = len(original_list)\n        max_indices = self.target_length - original_length + 1\n        possible_start_indices = random.sample(range(max_indices), \n                                               min(self.max_outputs, max_indices))\n\n        for start_idx in possible_start_indices:\n            padded_list = [self.tokenizer.vocab_smi['&lt;PAD&gt;']] * self.target_length\n            padded_list[start_idx:start_idx + original_length] = original_list\n            padded_versions.append(padded_list)\n\n        return padded_versions\n\n    def add_features(self, mz_input):\n        distances = pdist(np.array(mz_input).reshape(-1, 1), metric='euclidean')\n\n        # Crop the distances\n        distances = distances[:100]\n        return distances\n\n    def process_unknown_tokens(self, token):\n        \"\"\"\n        Process tokens that where not found in the tokenizer vocabulary. Those tokens are split into 'character-level' tokens.\n\n        THE BIGGEST SIZE OF A 'CHARACTER-LEVEL' TOKEN IS 2 CHARACTERS.\n\n        Args:\n            token (_type_): _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        alive = True\n        token_enc = []\n        while(alive):\n            if token[:2] not in self.tokenizer.vocab_smi:\n                indexes_to_remove = [0]\n                token_enc.append(self.tokenizer.vocab_smi[token[0]])\n            else:\n                indexes_to_remove = [0,1]\n                token_enc.append(self.tokenizer.vocab_smi[token[:2]])\n            token = ''.join(char for i, char in enumerate(token) if i not in indexes_to_remove)\n            if len(token) == 0:\n                alive = False\n\n        return token_enc\n\n    def process_sentence(self, sentence):\n        mz_input = sentence['mz']\n        smiles = sentence['smiles'].strip()\n\n        mz_input = [int(100 * float(x)) for x in mz_input]\n\n        # Tokenize SMILES\n        SS = self.tokenizer.tokenize(smiles)\n\n        smi_input = []\n        for token in SS:\n            if token in self.tokenizer.vocab_smi:\n                smi_input.append(self.tokenizer.vocab_smi[token])\n            else:\n                for j in self.process_unknown_tokens(token):\n                    smi_input.append(j)\n\n        smi_input.insert(0, self.tokenizer.vocab_smi.get('&lt;SOS&gt;'))\n        smi_input.append(self.tokenizer.vocab_smi.get('&lt;EOS&gt;'))\n\n        '''\n        if self.precursor_mz:\n            precursor_mz = Descriptors.ExactMolWt(Chem.MolFromSmiles(smiles))\n            mz_input.append(int(100*float(precursor_mz)))\n        '''\n\n        if self.additional_features:\n            dists = self.add_features(mz_input)\n            mz_input.extend(dists.tolist())\n\n        if self.pad_data_aug:\n            mz_input = self.create_random_padded_versions(mz_input)\n            smi_input_padded = self.create_random_padded_versions(smi_input)\n            smi_output = self.create_random_padded_versions(smi_input[1:])\n\n            # reuse the variable\n            smi_input = smi_input_padded\n        else:\n            for i in range(self.target_length):\n                smi_input.append(self.tokenizer.vocab_smi['&lt;PAD&gt;'])\n                mz_input.append(self.tokenizer.vocab_smi['&lt;PAD&gt;'])  \n\n            mz_input = mz_input[:self.target_length]\n            smi_output = smi_input[1:self.target_length+1]\n            smi_input = smi_input[:self.target_length]\n\n        return torch.LongTensor(mz_input), torch.LongTensor(smi_input), torch.LongTensor(smi_output)\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        mz_input, smi_input, smi_output = self.process_sentence(sentence)\n        return mz_input, smi_input, smi_output\n</code></pre>"},{"location":"input/#ptfifrijolpujc.InputManagement.MoleculesAndSpectraDataset.process_unknown_tokens","title":"<code>process_unknown_tokens(token)</code>","text":"<p>Process tokens that where not found in the tokenizer vocabulary. Those tokens are split into 'character-level' tokens.</p> <p>THE BIGGEST SIZE OF A 'CHARACTER-LEVEL' TOKEN IS 2 CHARACTERS.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>ptfifrijolpujc/InputManagement.py</code> <pre><code>def process_unknown_tokens(self, token):\n    \"\"\"\n    Process tokens that where not found in the tokenizer vocabulary. Those tokens are split into 'character-level' tokens.\n\n    THE BIGGEST SIZE OF A 'CHARACTER-LEVEL' TOKEN IS 2 CHARACTERS.\n\n    Args:\n        token (_type_): _description_\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    alive = True\n    token_enc = []\n    while(alive):\n        if token[:2] not in self.tokenizer.vocab_smi:\n            indexes_to_remove = [0]\n            token_enc.append(self.tokenizer.vocab_smi[token[0]])\n        else:\n            indexes_to_remove = [0,1]\n            token_enc.append(self.tokenizer.vocab_smi[token[:2]])\n        token = ''.join(char for i, char in enumerate(token) if i not in indexes_to_remove)\n        if len(token) == 0:\n            alive = False\n\n    return token_enc\n</code></pre>"},{"location":"library_requirements/","title":"Library requirements","text":"<p>For this repository to be used, the following listed packages need to be installed. Check their websites for the installation process.</p> <p>Python version</p> <p>It is recommended to install Python 3.11.</p> <ol> <li> <p>DreaMS</p> </li> <li> <p>Mist-CF</p> </li> </ol> <p>This library has a potential source of issues during the installation process: </p> <ul> <li>In <code>setup.cfg</code> change <code>version = &lt;version number: 0.0.1</code> with <code>version = 0.0.1</code>.</li> </ul> <p>Mist-Pred</p> <ol> <li>The file <code>algos2.pyx</code> has some issues with new data types in numpy. </li> </ol> <pre><code>...\n\nadj_mat_copy = adjacency_matrix.astype(numpy.int64, order='C', casting='safe', copy=True) # jdvg\n\n...\n\npath_copy = path.astype(numpy.int64, order='C', casting='safe', copy=True) # jdvg\nedge_feat_copy = edge_feat.astype(numpy.int64, order='C', casting='safe', copy=True) # jdvg\n\n...\n\ncdef numpy.ndarray[numpy.int64_t, ndim=4, mode='c'] edge_fea_all = -1 * numpy.ones([n, n, max_dist_copy, edge_feat.shape[-1]], dtype=numpy.int64) # jdvg\n</code></pre> <ol> <li>The file <code>misc_utils.py</code> has some issues with old <code>LightningLoggerBase</code> pytorch lightning class. </li> </ol> <pre><code>#from pytorch_lightning.loggers import LightningLoggerBase # jdvg\nfrom pytorch_lightning.loggers.logger import Logger  # jdvg\nfrom pytorch_lightning.utilities.rank_zero import rank_zero_only # jdvg\n#from pytorch_lightning.utilities import rank_zero_only # jdvg\n#from pytorch_lightning.loggers.base import rank_zero_experiment #jdvg\n\n...\n\nclass ConsoleLogger(Logger):\n    ...\n\n    @property\n    @rank_zero_only # jdvg\n    def name(self):\n        pass\n\n    @property\n    @rank_zero_only # jdvg\n    def experiment(self):\n        pass\n\n    @property\n    @rank_zero_only # jdvg\n    def version(self):\n        pass\n</code></pre> <ol> <li>The file <code>nn_utils.py</code> has an outdated definition for an adjancency matrix.</li> </ol> <pre><code>...\n\n#A = g.adj(scipy_fmt=\"csr\")  # adjacency matrix # jdvg\nA = g.adj_external(scipy_fmt=\"csr\") # jdvg\n</code></pre>"},{"location":"ms2smiles/","title":"De novo molecular structure generation from mass spectra","text":"<p>Metabolomics: small molecule identification from biological samples</p> <p>Approaches used in identification of unknown molecules:</p> <ul> <li>Search a mass spectral library to find molecular structures that may potentially match the unidentified mass spectra</li> <li>Use tools like MetFrag, CFM-ID, SIRIUS to predict molecular fingerprints based on the mass spectra. Then, the fingerprints are also compared to the ones already calculated and stored in a database to determine potential matches,</li> </ul>"},{"location":"ms2smiles/#other-frameworks","title":"Other frameworks","text":"<ul> <li>MSNovelist predicts small molecule structures using MS data, departing from traditional database searches and comparisons</li> </ul>"},{"location":"ms2smiles/#challenge","title":"Challenge","text":"<p>One key challenge is the implicit inclusion of hydrogen atoms, which, if overlooked, can lead to incorrect molecular formula corresponding to the generated SMILES.</p> <p>To overcome this, MS2SMILES predicts both heavy atoms and their associated hydrogen atoms as a single unit. </p> <p>Additionally, molecular fingerprint and formula convey distinct information, and to leverage both effectively, MS2SMILES incorporates a learnable parameter to enhance their combined impact.</p>"},{"location":"ms2smiles/#results","title":"Results","text":"<ul> <li> <p>Validated on GNPS.</p> </li> <li> <p>It correctly predicted 27.5%, 41.0%, and 45.1% of SMILES at top 1, top 5, and top 10, respectively.</p> </li> </ul>"},{"location":"msnovelist/","title":"MSNovelist: de novo structure generation from mass spectra","text":"<p>Usage on tandem mass spectroscopy (\\(MS^{2}\\))</p>"},{"location":"msnovelist/#architecture","title":"Architecture","text":"<p>Encoder-Decoder</p>"},{"location":"msnovelist/#results","title":"Results","text":"<ul> <li>3863 \\(MS^{2}\\) from Global Natural Product Social Molecular Networking (GNPSM)</li> <li>Accuracy 25% on first rank</li> <li>Accuracy 45% overall</li> <li>Reproduced 61% of correct database annotations without having ever seen the structure in the training phase.</li> </ul>"},{"location":"msnovelist/#use-case","title":"Use case","text":"<p>MSNovelist is ideally suited to complement library-based annotation in the case of poorly represented analyte classes and novel compounds.</p>"},{"location":"prediction/","title":"Prediction","text":""},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule","title":"<code>PredictWorkflowModule</code>","text":"<p>               Bases: <code>object</code></p> <p>Prediction workflow for molecular structure generation using different transformer architectures. Handles model loading, input preparation, decoding, and result ranking for various transformer variants.</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>class PredictWorkflowModule(object):\n    \"\"\"\n    Prediction workflow for molecular structure generation using different transformer architectures.\n    Handles model loading, input preparation, decoding, and result ranking for various transformer variants.\n    \"\"\"\n    def __init__(self, architecture='arch_1'):\n        \"\"\"\n        Initialize PredictWorkflow with specified architecture.\n\n        Args:\n            architecture (str): Architecture to use. Options: 'arch_1', 'arch_2', 'arch_3'\n                - arch_1: Basic transformer (MolecularStructureTeFT)\n                - arch_2: Transformer with bias (MolecularStructureTeFTWithBias)\n                - arch_3: Transformer with intensity (MolecularStructureTeFTWithIntensity)\n        \"\"\"\n        self.architecture = architecture\n\n        # Map architecture names to transformer classes\n        self.transformer_classes = {\n            'arch_1': TransformerArch1,\n            'arch_2': TransformerArch2,\n            'arch_3': TransformerArch3\n        }\n\n        if architecture not in self.transformer_classes:\n            raise ValueError(f\"Unknown architecture: {architecture}. Available options: {list(self.transformer_classes.keys())}\")\n\n    def get_transformer_class(self):\n        \"\"\"\n        Get the appropriate transformer class for the selected architecture.\n\n        Returns:\n            class: Transformer class corresponding to the selected architecture.\n        \"\"\"\n        return self.transformer_classes[self.architecture]\n\n    def get_encoder_outputs(self, model, enc_input, intensities=None):\n        \"\"\"\n        Get encoder outputs handling different architectures.\n\n        Args:\n            model: The transformer model\n            enc_input: The encoded input tensor\n            intensities: Optional intensities for arch_3\n\n        Returns:\n            tuple: (enc_outputs, enc_self_attns) for all architectures\n        \"\"\"\n        if self.architecture == 'arch_1':\n            enc_outputs, enc_self_attns = model.encoder(enc_input)\n        elif self.architecture == 'arch_2':\n            enc_outputs, enc_self_attns, raw_enc_inputs = model.encoder(enc_input)\n        elif self.architecture == 'arch_3':\n            enc_outputs, enc_self_attns, raw_enc_inputs, intensities = model.encoder(enc_input, intensities)\n        else:\n            raise ValueError(f\"Unknown architecture: {self.architecture}\")\n\n        return enc_outputs, enc_self_attns\n\n    def make_encoder_inputs(self, tokenizer, mz, prid_num=100, intensities=None):\n        \"\"\"\n        Create encoder input tensor from m/z values.\n\n        Args:\n            tokenizer: Tokenizer object with vocab_mz attribute.\n            mz (list or numpy.ndarray): List or array of m/z values.\n            prid_num (int, optional): Number of positions to pad/truncate to. Defaults to 100.\n            intensities (list or numpy.ndarray, optional): List or array of intensity values. Defaults to None.\n\n        Returns:\n            list: List containing the encoder input sequence as indices.\n        \"\"\"\n        mz = [float(x) for x in mz]\n        mz = [int(100 * x) for x in mz]\n\n        ity = [float(x) for x in intensities]\n        ity = [x/max(ity) for x in ity]\n\n        print(f\"mz shape: {len(mz)}, ity shape: {len(ity)}\")\n\n        #if max(mz) &gt; 50000:\n        #    return [] \n\n        enc_input2 = []\n\n        for j in range(0, prid_num):\n            mz.append(0)\n            ity.append(0)\n\n        print(f\"mz shape after padding: {len(mz)}, ity shape after padding: {len(ity)}\")\n\n        enc_input = [tokenizer.vocab_mz[n] for n in mz]\n        enc_input2.append(enc_input[0:prid_num])\n\n        ity = [ity[0:prid_num]]\n\n        if intensities is not None:\n            return enc_input2, ity\n        else:\n            return enc_input2   \n\n    def make_decoder_inputs(self, tokenizer, mz, smiles, prid_num=100):\n        \"\"\"\n        Create decoder input tensor from SMILES string and m/z values.\n\n        Args:\n            tokenizer: Tokenizer object with vocab_smi attribute.\n            mz (list): List of m/z values.\n            smiles (str): SMILES string.\n            prid_num (int, optional): Number of positions to pad/truncate to. Defaults to 100.\n\n        Returns:\n            tuple: (decoder input list, real_smiles list)\n        \"\"\"\n\n        if max(mz) &gt; 50000:\n            return []\n\n        judge = 0\n        SS = []\n        dec_input2 = []\n        real_smiles = []\n\n        for j in range(len(smiles)):\n            if judge == 1:\n                judge = 0\n            else:\n                if smiles[j] == 'C' and j &lt; len(smiles) - 1:\n                    if smiles[j + 1] == 'l':\n                        SS.append('Cl')\n                        judge = 1\n                        continue\n                if smiles[j] == 'B' and j &lt; len(smiles) - 1:\n                    if smiles[j + 1] == 'r':\n                        SS.append('Br')\n                        judge = 1\n                        continue\n                if smiles[j] == '\\xa0': # detects a space at the end of the line in the string\n                    continue\n                SS.append(smiles[j])\n        SS = ['&lt;SOS&gt;'] + SS + ['&lt;EOS&gt;']\n\n        for j in range(0, prid_num):\n            SS.append('&lt;PAD&gt;')\n\n        dec_input = [tokenizer.vocab_smi[n] for n in SS]\n        dec_input2.append(dec_input[0:prid_num+1])\n        real_smiles.append(smiles)\n\n        return dec_input2, real_smiles\n\n    def greedy_decoder(self, model, enc_input, molmol, start_symbol, device, tokenizer, intensities=None):\n        \"\"\"\n        Greedy decoding for sequence generation.\n\n        Args:\n            model: Transformer model with encoder and decoder.\n            enc_input (torch.Tensor): Encoder input tensor.\n            molmol (str): Target sequence for forced decoding.\n            start_symbol (int): Start token index.\n            device: Torch device.\n            tokenizer: Tokenizer object with vocab_smi attribute.\n            intensities (torch.Tensor, optional): Optional intensities for arch_3. Defaults to None.\n\n        Returns:\n            torch.Tensor: Decoded sequence tensor (without initial start symbol).\n        \"\"\"\n        enc_outputs, enc_self_attns = self.get_encoder_outputs(model, enc_input, intensities)\n        dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n        terminal = False\n        next_symbol = start_symbol\n        while not terminal:\n            dec_input = torch.cat([dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],\n                                -1)\n            dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n            projected = model.projection(dec_outputs)\n            prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n            next_word = prob.data[-1]\n            next_symbol = next_word\n            # print(next_word)\n            num = dec_input.shape\n            if num[1] &lt; len(molmol) + 1 and num[1] != 0:\n                ana = molmol[num[1] - 1]\n                next_symbol = torch.tensor(tokenizer.vocab_smi[ana]).to(device)\n            if num[1] == 101:\n                terminal = True\n            # print(next_word)\n\n        # greedy_dec_predict = torch.cat(\n        #     [dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],\n        #     -1)\n        greedy_dec_predict = dec_input[:, 1:]\n        return greedy_dec_predict\n\n    def load_model_and_tokenizer(self, full_path_to_mdl, device):\n        \"\"\"\n        Loads the model and tokenizer from the checkpoint.\n        Args:\n            full_path_to_mdl (str): Path to model checkpoint.\n            device: Torch device.\n        Returns:\n            model: Loaded transformer model.\n            tokenizer: Loaded or default tokenizer.\n        \"\"\"\n        checkpoint = load_model_with_compatibility(full_path_to_mdl)\n        chekpoint_modified = checkpoint.copy()\n        d_k = checkpoint.get('d_k', 64)\n        d_v = checkpoint.get('d_v', 64)\n        d_ff = checkpoint.get('d_ff', 2048)\n        n_heads = checkpoint.get('n_heads', 8)\n        n_layers = checkpoint.get('n_layers', 6)\n        d_model = checkpoint.get('d_model', 512)\n        if 'tokenizer' in checkpoint:\n            tokenizer = checkpoint['tokenizer']\n            chekpoint_modified.pop('tokenizer')\n        else:\n            tokenizer = DataDrivenTokenizer()\n        # Remove used keys\n        for k in ['d_k', 'd_v', 'd_ff', 'n_heads', 'n_layers', 'd_model']:\n            if k in chekpoint_modified:\n                chekpoint_modified.pop(k)\n        TransformerClass = self.get_transformer_class()\n        model = TransformerClass(d_k=d_k, \n                                d_ff=d_ff, \n                                d_model=d_model, \n                                d_v=d_v, \n                                n_heads=n_heads, \n                                n_layers=n_layers, \n                                device=device, \n                                vocab_mz_size=tokenizer.vocab_mz_size,\n                                vocab_smi_size=tokenizer.vocab_smi_size).to(device)\n        if 'model_state_dict' in chekpoint_modified:\n            state_dict = chekpoint_modified.get('model_state_dict', chekpoint_modified)\n        elif 'state_dict' in chekpoint_modified:\n            state_dict = chekpoint_modified.get('state_dict', chekpoint_modified)\n            state_dict = {k.replace(\"model.\", \"\"): v for k, v in state_dict.items()}\n        else:\n            state_dict = chekpoint_modified\n        model.load_state_dict(state_dict)\n        return model, tokenizer\n\n    def prepare_inputs(self, tokenizer, Spec_list_Mz, smiles=None, sample_i=None, intensities=None):\n        \"\"\"\n        Prepares encoder and decoder inputs for a sample.\n        Args:\n            tokenizer: Tokenizer object.\n            Spec_list_Mz: List of m/z values for the sample.\n            smiles: List of ground truth SMILES strings (optional).\n            sample_i: Index of the sample (optional, required if smiles is provided).\n            intensities: List of intensity values for the sample (optional).\n        Returns:\n            enc_input2: Encoder input.\n            dec_input2: Decoder input (if smiles is provided, else None).\n            real_smiles: Real SMILES (if smiles is provided, else None).\n        \"\"\"\n        if smiles is None:\n            if intensities is not None: # for arch_3\n                enc_input2, ity = self.make_encoder_inputs(tokenizer, Spec_list_Mz, intensities=intensities)\n                if len(enc_input2) == 0:\n                    return [], None, None, None\n                else:\n                    return enc_input2, ity, None, None\n            else:\n                enc_input2 = self.make_encoder_inputs(tokenizer, Spec_list_Mz)\n                if len(enc_input2) == 0:\n                    return [], None, None\n                else:\n                    return enc_input2, None, None\n        else:\n            dec_input2, real_smiles = self.make_decoder_inputs(tokenizer, Spec_list_Mz, smiles[sample_i])\n            if intensities is not None:\n                enc_input2, ity = self.make_encoder_inputs(tokenizer, Spec_list_Mz, intensities=intensities)\n                if len(enc_input2) == 0:\n                    return [], None, None, None\n                else:\n                    return enc_input2, ity, dec_input2, real_smiles\n            else:\n                enc_input2 = self.make_encoder_inputs(tokenizer, Spec_list_Mz)\n                if len(enc_input2) == 0:\n                    return [], None, None\n                else:\n                    return enc_input2, dec_input2, real_smiles\n\n    def predict_single_sample(self, model, tokenizer, enc_inputs, intensities, num_pred, predict_approach, predict_approach_args, real_smiles=None):\n        \"\"\"\n        Runs prediction/decoding for a single sample.\n        Args:\n            model: Transformer model.\n            tokenizer: Tokenizer object.\n            enc_inputs: Encoder input tensor(s).\n            intensities: Intensities for the sample.\n            num_pred: Number of predictions per input.\n            predict_approach: Decoding approach (\"greedy\" or \"beam_search\").\n            predict_approach_args: Arguments for decoding approach.\n            real_smiles: Real SMILES string(s) for the sample (optional).\n        Returns:\n            ans: List of prediction results for this sample.\n            SIM: List of similarity scores (if real_smiles is provided).\n            cnt_correct_smiles: Count of correct predictions (if real_smiles is provided).\n        \"\"\"\n        jihe = []\n        SIM = []\n        ans = []\n        sm = ''\n        cnt_correct_smiles = 0\n        for i in range(len(enc_inputs)):\n            for j in range(num_pred):\n                if predict_approach == \"greedy\":\n                    enc_smiles_predicted = self.greedy_decoder(model, enc_inputs[i].view(1, -1).to(enc_inputs.device), sm, start_symbol=tokenizer.vocab_smi['&lt;SOS&gt;'], device=enc_inputs.device, tokenizer=tokenizer, intensities=intensities)\n                elif predict_approach == \"beam_search\":\n                    enc_smiles_predicted = self.beam_search_decoder(model, enc_inputs[i].view(1, -1), tokenizer.vocab_smi['&lt;SOS&gt;'], beam_size=predict_approach_args['beam_size'], intensities=intensities)\n                predictsmiles = [tokenizer.vocab_smi_rev[n.item()] for n in enc_smiles_predicted.squeeze()]\n                psm = ''.join(predictsmiles)\n                psm = psm.replace('&lt;PAD&gt;', '')\n                c = psm.find('&lt;EOS&gt;')\n                if real_smiles is not None:\n                    realsmiles = real_smiles[i]\n                    try:\n                        max_sim = similarity_score(psm[0:c], str(realsmiles), which_similarity=\"all\")\n                        tomitosim = similarity_score(psm[0:c], str(realsmiles), which_similarity=\"tomitosim\")\n                    except Exception as e:\n                        max_sim = 0.0\n                        tomitosim = 0.0\n                    SIM.append(max_sim)\n                    DITT = dict(num=i, predict=psm[0:c], real=str(realsmiles), similar=max_sim, tomitosim=tomitosim)\n                    print('predictsmiles:', psm[0:c])\n                    print('realsmiles:', realsmiles)\n                    if psm[0:c] == realsmiles:\n                        cnt_correct_smiles += 1\n                else:\n                    DITT = dict(num=i, predict=psm[0:c])\n                    print('predictsmiles:', psm[0:c])\n                jihe.append(DITT)\n            if real_smiles is not None:\n                if SIM:\n                    max_sim = max(SIM)\n                    pos = SIM.index(max_sim)\n                    print(\"--------------------------------------\")\n                    print('From all predicted smiles, max sim:', max_sim)\n                    print('Smiles for max sim:', jihe[pos]['predict'])\n            ans.append(jihe)\n            SIM = []\n            jihe = []\n        return ans, cnt_correct_smiles\n\n    def write_predictions_to_file(self, ans, smiles, filenames, sample_i):\n        \"\"\"\n        Writes the predictions for a sample to a file, sorting as needed.\n        Args:\n            ans: List of prediction results for this sample.\n            smiles: List of ground truth SMILES strings (optional).\n            filenames: List of output filenames.\n            sample_i: Index of the sample.\n        \"\"\"\n        if smiles is not None:\n            sorted_data = sorted(ans[0], key=lambda x: x['similar'], reverse=True)\n        else:\n            sorted_data = self.ranking_smiles(ans[0])\n        final = json.dumps(sorted_data)\n        with open(filenames[sample_i], 'w') as fileOb:\n            fileOb.write(final)\n        print('predict!')\n\n    def predict(self, INT, \n                MZ, \n                smiles=None, \n                full_path_to_mdl=\"/home\", \n                device=None, \n                output_dir=\"\", \n                ids_ms2=None, \n                num_pred=10, \n                predict_approach=\"greedy\",\n                predict_approach_args=None):\n        \"\"\"\n        Run prediction for a batch of spectra and optionally compare to ground truth SMILES.\n        Args:\n            INT (list of lists): Intensities for one or more MS2 spectra.\n            MZ (list of lists): m/z values for one or more MS2 spectra.\n            smiles (list, optional): List of ground truth SMILES strings. Defaults to None.\n            full_path_to_mdl (str, optional): Path to model checkpoint. Defaults to \"/home\".\n            device: Torch device. Defaults to None.\n            output_dir (str, optional): Directory to save results. Defaults to \"\".\n            ids_ms2 (list, optional): List of MS2 identifiers. Defaults to None.\n            num_pred (int, optional): Number of predictions per input. Defaults to 10.\n            predict_approach (str, optional): Decoding approach (\"greedy\" or \"beam_search\"). Defaults to \"greedy\".\n            predict_approach_args (dict, optional): Arguments for decoding approach. Defaults to None.\n        Returns:\n            None\n        \"\"\"\n        my_signal_math = SignalMath()\n        if device is None:\n            device = self.device\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        if ids_ms2 is not None:\n            filenames = [f'{output_dir}/pred_{ids_ms2[i]}.json' for i in range(len(MZ))]\n        else:\n            filenames = [f'{output_dir}/pred_{i}.json' for i in range(len(MZ))]\n        model, tokenizer = self.load_model_and_tokenizer(full_path_to_mdl, device)\n        cnt_correct_smiles = 0\n        for sample_i in range(len(MZ)):\n            intensities, Spec_list_Mz = my_signal_math.find_peak(np.array(INT[sample_i]), np.array(MZ[sample_i]))\n\n            if intensities is not None:\n                enc_input2, intensities, dec_input2, real_smiles = self.prepare_inputs(tokenizer, Spec_list_Mz, smiles, sample_i, intensities=intensities)\n            else:\n                enc_input2, dec_input2, real_smiles = self.prepare_inputs(tokenizer, Spec_list_Mz, smiles, sample_i)\n\n            if len(enc_input2) == 0:\n                continue\n\n            enc_inputs = torch.LongTensor(enc_input2).to(device)\n\n            if intensities is not None:\n                intensities = torch.LongTensor(intensities).to(torch.float32).to(device)\n\n            ans, correct_count = self.predict_single_sample(\n                model, tokenizer, enc_inputs, intensities, num_pred, predict_approach, predict_approach_args, real_smiles=real_smiles if smiles is not None else None\n            )\n            cnt_correct_smiles += correct_count\n            self.write_predictions_to_file(ans, smiles, filenames, sample_i)\n        print(f\"Accuracy: {cnt_correct_smiles/len(MZ)}\")\n\n    def ranking_smiles(self, predictions):\n        \"\"\"\n        Rank SMILES predictions when no ground truth is available.\n        For now, returns predictions as-is since we don't have similarity metrics.\n\n        Args:\n            predictions (list): List of prediction dictionaries\n\n        Returns:\n            list: Sorted list of predictions\n        \"\"\"\n        # Simple ranking - could be enhanced with other metrics\n        return predictions\n\n    def beam_search_decoder(self, model, enc_input, start_symbol, beam_size=5, max_len=101, intensities=None):\n        \"\"\"\n        Beam search decoder for sequence generation.\n\n        Args:\n            model: The transformer model.\n            enc_input (torch.Tensor): The encoded input tensor.\n            start_symbol (int): The starting token for decoding.\n            beam_size (int, optional): Number of sequences to keep in the beam. Defaults to 5.\n            max_len (int, optional): Maximum sequence length. Defaults to 101.\n            intensities (torch.Tensor, optional): Optional intensities for arch_3. Defaults to None.\n\n        Returns:\n            torch.Tensor: The best decoded sequence (without initial start symbol).\n        \"\"\"\n        enc_outputs, _ = self.get_encoder_outputs(model, enc_input, intensities)\n        device = enc_input.device\n\n        # Initialize beam with start_symbol\n        beams = [(torch.tensor([[start_symbol]], dtype=enc_input.dtype, device=device), 0.0)]  # (sequence, score)\n\n        for _ in range(max_len):\n            candidates = []\n            for seq, score in beams:\n                dec_input = seq.to(device)\n                dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n                projected = model.projection(dec_outputs)  # Get logits\n\n                # Get the last token probabilities\n                log_probs = F.log_softmax(projected[:, -1, :], dim=-1)  # (1, vocab_size)\n\n                # Get top beam_size candidates\n                top_k_probs, top_k_words = torch.topk(log_probs, beam_size, dim=-1)\n\n                for i in range(beam_size):\n                    next_token = top_k_words[0, i].item()\n                    next_score = score + top_k_probs[0, i].item()\n\n                    # Extend the sequence\n                    new_seq = torch.cat([seq, torch.tensor([[next_token]], dtype=seq.dtype, device=device)], dim=-1)\n                    candidates.append((new_seq, next_score))\n\n            # Sort candidates by score and keep top `beam_size`\n            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n\n            # Check if all beams have reached the max length\n            if all(seq.shape[1] &gt;= max_len for seq, _ in beams):\n                break\n\n        # Return the best sequence (highest score)\n        best_seq, _ = max(beams, key=lambda x: x[1])\n        return best_seq[:, 1:]  # Remove initial start symbol\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.__init__","title":"<code>__init__(architecture='arch_1')</code>","text":"<p>Initialize PredictWorkflow with specified architecture.</p> <p>Parameters:</p> Name Type Description Default <code>architecture</code> <code>str</code> <p>Architecture to use. Options: 'arch_1', 'arch_2', 'arch_3' - arch_1: Basic transformer (MolecularStructureTeFT) - arch_2: Transformer with bias (MolecularStructureTeFTWithBias) - arch_3: Transformer with intensity (MolecularStructureTeFTWithIntensity)</p> <code>'arch_1'</code> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def __init__(self, architecture='arch_1'):\n    \"\"\"\n    Initialize PredictWorkflow with specified architecture.\n\n    Args:\n        architecture (str): Architecture to use. Options: 'arch_1', 'arch_2', 'arch_3'\n            - arch_1: Basic transformer (MolecularStructureTeFT)\n            - arch_2: Transformer with bias (MolecularStructureTeFTWithBias)\n            - arch_3: Transformer with intensity (MolecularStructureTeFTWithIntensity)\n    \"\"\"\n    self.architecture = architecture\n\n    # Map architecture names to transformer classes\n    self.transformer_classes = {\n        'arch_1': TransformerArch1,\n        'arch_2': TransformerArch2,\n        'arch_3': TransformerArch3\n    }\n\n    if architecture not in self.transformer_classes:\n        raise ValueError(f\"Unknown architecture: {architecture}. Available options: {list(self.transformer_classes.keys())}\")\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.beam_search_decoder","title":"<code>beam_search_decoder(model, enc_input, start_symbol, beam_size=5, max_len=101, intensities=None)</code>","text":"<p>Beam search decoder for sequence generation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The transformer model.</p> required <code>enc_input</code> <code>Tensor</code> <p>The encoded input tensor.</p> required <code>start_symbol</code> <code>int</code> <p>The starting token for decoding.</p> required <code>beam_size</code> <code>int</code> <p>Number of sequences to keep in the beam. Defaults to 5.</p> <code>5</code> <code>max_len</code> <code>int</code> <p>Maximum sequence length. Defaults to 101.</p> <code>101</code> <code>intensities</code> <code>Tensor</code> <p>Optional intensities for arch_3. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch.Tensor: The best decoded sequence (without initial start symbol).</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def beam_search_decoder(self, model, enc_input, start_symbol, beam_size=5, max_len=101, intensities=None):\n    \"\"\"\n    Beam search decoder for sequence generation.\n\n    Args:\n        model: The transformer model.\n        enc_input (torch.Tensor): The encoded input tensor.\n        start_symbol (int): The starting token for decoding.\n        beam_size (int, optional): Number of sequences to keep in the beam. Defaults to 5.\n        max_len (int, optional): Maximum sequence length. Defaults to 101.\n        intensities (torch.Tensor, optional): Optional intensities for arch_3. Defaults to None.\n\n    Returns:\n        torch.Tensor: The best decoded sequence (without initial start symbol).\n    \"\"\"\n    enc_outputs, _ = self.get_encoder_outputs(model, enc_input, intensities)\n    device = enc_input.device\n\n    # Initialize beam with start_symbol\n    beams = [(torch.tensor([[start_symbol]], dtype=enc_input.dtype, device=device), 0.0)]  # (sequence, score)\n\n    for _ in range(max_len):\n        candidates = []\n        for seq, score in beams:\n            dec_input = seq.to(device)\n            dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n            projected = model.projection(dec_outputs)  # Get logits\n\n            # Get the last token probabilities\n            log_probs = F.log_softmax(projected[:, -1, :], dim=-1)  # (1, vocab_size)\n\n            # Get top beam_size candidates\n            top_k_probs, top_k_words = torch.topk(log_probs, beam_size, dim=-1)\n\n            for i in range(beam_size):\n                next_token = top_k_words[0, i].item()\n                next_score = score + top_k_probs[0, i].item()\n\n                # Extend the sequence\n                new_seq = torch.cat([seq, torch.tensor([[next_token]], dtype=seq.dtype, device=device)], dim=-1)\n                candidates.append((new_seq, next_score))\n\n        # Sort candidates by score and keep top `beam_size`\n        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n\n        # Check if all beams have reached the max length\n        if all(seq.shape[1] &gt;= max_len for seq, _ in beams):\n            break\n\n    # Return the best sequence (highest score)\n    best_seq, _ = max(beams, key=lambda x: x[1])\n    return best_seq[:, 1:]  # Remove initial start symbol\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.get_encoder_outputs","title":"<code>get_encoder_outputs(model, enc_input, intensities=None)</code>","text":"<p>Get encoder outputs handling different architectures.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The transformer model</p> required <code>enc_input</code> <p>The encoded input tensor</p> required <code>intensities</code> <p>Optional intensities for arch_3</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(enc_outputs, enc_self_attns) for all architectures</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def get_encoder_outputs(self, model, enc_input, intensities=None):\n    \"\"\"\n    Get encoder outputs handling different architectures.\n\n    Args:\n        model: The transformer model\n        enc_input: The encoded input tensor\n        intensities: Optional intensities for arch_3\n\n    Returns:\n        tuple: (enc_outputs, enc_self_attns) for all architectures\n    \"\"\"\n    if self.architecture == 'arch_1':\n        enc_outputs, enc_self_attns = model.encoder(enc_input)\n    elif self.architecture == 'arch_2':\n        enc_outputs, enc_self_attns, raw_enc_inputs = model.encoder(enc_input)\n    elif self.architecture == 'arch_3':\n        enc_outputs, enc_self_attns, raw_enc_inputs, intensities = model.encoder(enc_input, intensities)\n    else:\n        raise ValueError(f\"Unknown architecture: {self.architecture}\")\n\n    return enc_outputs, enc_self_attns\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.get_transformer_class","title":"<code>get_transformer_class()</code>","text":"<p>Get the appropriate transformer class for the selected architecture.</p> <p>Returns:</p> Name Type Description <code>class</code> <p>Transformer class corresponding to the selected architecture.</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def get_transformer_class(self):\n    \"\"\"\n    Get the appropriate transformer class for the selected architecture.\n\n    Returns:\n        class: Transformer class corresponding to the selected architecture.\n    \"\"\"\n    return self.transformer_classes[self.architecture]\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.greedy_decoder","title":"<code>greedy_decoder(model, enc_input, molmol, start_symbol, device, tokenizer, intensities=None)</code>","text":"<p>Greedy decoding for sequence generation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Transformer model with encoder and decoder.</p> required <code>enc_input</code> <code>Tensor</code> <p>Encoder input tensor.</p> required <code>molmol</code> <code>str</code> <p>Target sequence for forced decoding.</p> required <code>start_symbol</code> <code>int</code> <p>Start token index.</p> required <code>device</code> <p>Torch device.</p> required <code>tokenizer</code> <p>Tokenizer object with vocab_smi attribute.</p> required <code>intensities</code> <code>Tensor</code> <p>Optional intensities for arch_3. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch.Tensor: Decoded sequence tensor (without initial start symbol).</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def greedy_decoder(self, model, enc_input, molmol, start_symbol, device, tokenizer, intensities=None):\n    \"\"\"\n    Greedy decoding for sequence generation.\n\n    Args:\n        model: Transformer model with encoder and decoder.\n        enc_input (torch.Tensor): Encoder input tensor.\n        molmol (str): Target sequence for forced decoding.\n        start_symbol (int): Start token index.\n        device: Torch device.\n        tokenizer: Tokenizer object with vocab_smi attribute.\n        intensities (torch.Tensor, optional): Optional intensities for arch_3. Defaults to None.\n\n    Returns:\n        torch.Tensor: Decoded sequence tensor (without initial start symbol).\n    \"\"\"\n    enc_outputs, enc_self_attns = self.get_encoder_outputs(model, enc_input, intensities)\n    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\n    terminal = False\n    next_symbol = start_symbol\n    while not terminal:\n        dec_input = torch.cat([dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],\n                            -1)\n        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n        projected = model.projection(dec_outputs)\n        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n        next_word = prob.data[-1]\n        next_symbol = next_word\n        # print(next_word)\n        num = dec_input.shape\n        if num[1] &lt; len(molmol) + 1 and num[1] != 0:\n            ana = molmol[num[1] - 1]\n            next_symbol = torch.tensor(tokenizer.vocab_smi[ana]).to(device)\n        if num[1] == 101:\n            terminal = True\n        # print(next_word)\n\n    # greedy_dec_predict = torch.cat(\n    #     [dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],\n    #     -1)\n    greedy_dec_predict = dec_input[:, 1:]\n    return greedy_dec_predict\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.load_model_and_tokenizer","title":"<code>load_model_and_tokenizer(full_path_to_mdl, device)</code>","text":"<p>Loads the model and tokenizer from the checkpoint. Args:     full_path_to_mdl (str): Path to model checkpoint.     device: Torch device. Returns:     model: Loaded transformer model.     tokenizer: Loaded or default tokenizer.</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def load_model_and_tokenizer(self, full_path_to_mdl, device):\n    \"\"\"\n    Loads the model and tokenizer from the checkpoint.\n    Args:\n        full_path_to_mdl (str): Path to model checkpoint.\n        device: Torch device.\n    Returns:\n        model: Loaded transformer model.\n        tokenizer: Loaded or default tokenizer.\n    \"\"\"\n    checkpoint = load_model_with_compatibility(full_path_to_mdl)\n    chekpoint_modified = checkpoint.copy()\n    d_k = checkpoint.get('d_k', 64)\n    d_v = checkpoint.get('d_v', 64)\n    d_ff = checkpoint.get('d_ff', 2048)\n    n_heads = checkpoint.get('n_heads', 8)\n    n_layers = checkpoint.get('n_layers', 6)\n    d_model = checkpoint.get('d_model', 512)\n    if 'tokenizer' in checkpoint:\n        tokenizer = checkpoint['tokenizer']\n        chekpoint_modified.pop('tokenizer')\n    else:\n        tokenizer = DataDrivenTokenizer()\n    # Remove used keys\n    for k in ['d_k', 'd_v', 'd_ff', 'n_heads', 'n_layers', 'd_model']:\n        if k in chekpoint_modified:\n            chekpoint_modified.pop(k)\n    TransformerClass = self.get_transformer_class()\n    model = TransformerClass(d_k=d_k, \n                            d_ff=d_ff, \n                            d_model=d_model, \n                            d_v=d_v, \n                            n_heads=n_heads, \n                            n_layers=n_layers, \n                            device=device, \n                            vocab_mz_size=tokenizer.vocab_mz_size,\n                            vocab_smi_size=tokenizer.vocab_smi_size).to(device)\n    if 'model_state_dict' in chekpoint_modified:\n        state_dict = chekpoint_modified.get('model_state_dict', chekpoint_modified)\n    elif 'state_dict' in chekpoint_modified:\n        state_dict = chekpoint_modified.get('state_dict', chekpoint_modified)\n        state_dict = {k.replace(\"model.\", \"\"): v for k, v in state_dict.items()}\n    else:\n        state_dict = chekpoint_modified\n    model.load_state_dict(state_dict)\n    return model, tokenizer\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.make_decoder_inputs","title":"<code>make_decoder_inputs(tokenizer, mz, smiles, prid_num=100)</code>","text":"<p>Create decoder input tensor from SMILES string and m/z values.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>Tokenizer object with vocab_smi attribute.</p> required <code>mz</code> <code>list</code> <p>List of m/z values.</p> required <code>smiles</code> <code>str</code> <p>SMILES string.</p> required <code>prid_num</code> <code>int</code> <p>Number of positions to pad/truncate to. Defaults to 100.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(decoder input list, real_smiles list)</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def make_decoder_inputs(self, tokenizer, mz, smiles, prid_num=100):\n    \"\"\"\n    Create decoder input tensor from SMILES string and m/z values.\n\n    Args:\n        tokenizer: Tokenizer object with vocab_smi attribute.\n        mz (list): List of m/z values.\n        smiles (str): SMILES string.\n        prid_num (int, optional): Number of positions to pad/truncate to. Defaults to 100.\n\n    Returns:\n        tuple: (decoder input list, real_smiles list)\n    \"\"\"\n\n    if max(mz) &gt; 50000:\n        return []\n\n    judge = 0\n    SS = []\n    dec_input2 = []\n    real_smiles = []\n\n    for j in range(len(smiles)):\n        if judge == 1:\n            judge = 0\n        else:\n            if smiles[j] == 'C' and j &lt; len(smiles) - 1:\n                if smiles[j + 1] == 'l':\n                    SS.append('Cl')\n                    judge = 1\n                    continue\n            if smiles[j] == 'B' and j &lt; len(smiles) - 1:\n                if smiles[j + 1] == 'r':\n                    SS.append('Br')\n                    judge = 1\n                    continue\n            if smiles[j] == '\\xa0': # detects a space at the end of the line in the string\n                continue\n            SS.append(smiles[j])\n    SS = ['&lt;SOS&gt;'] + SS + ['&lt;EOS&gt;']\n\n    for j in range(0, prid_num):\n        SS.append('&lt;PAD&gt;')\n\n    dec_input = [tokenizer.vocab_smi[n] for n in SS]\n    dec_input2.append(dec_input[0:prid_num+1])\n    real_smiles.append(smiles)\n\n    return dec_input2, real_smiles\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.make_encoder_inputs","title":"<code>make_encoder_inputs(tokenizer, mz, prid_num=100, intensities=None)</code>","text":"<p>Create encoder input tensor from m/z values.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>Tokenizer object with vocab_mz attribute.</p> required <code>mz</code> <code>list or ndarray</code> <p>List or array of m/z values.</p> required <code>prid_num</code> <code>int</code> <p>Number of positions to pad/truncate to. Defaults to 100.</p> <code>100</code> <code>intensities</code> <code>list or ndarray</code> <p>List or array of intensity values. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <p>List containing the encoder input sequence as indices.</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def make_encoder_inputs(self, tokenizer, mz, prid_num=100, intensities=None):\n    \"\"\"\n    Create encoder input tensor from m/z values.\n\n    Args:\n        tokenizer: Tokenizer object with vocab_mz attribute.\n        mz (list or numpy.ndarray): List or array of m/z values.\n        prid_num (int, optional): Number of positions to pad/truncate to. Defaults to 100.\n        intensities (list or numpy.ndarray, optional): List or array of intensity values. Defaults to None.\n\n    Returns:\n        list: List containing the encoder input sequence as indices.\n    \"\"\"\n    mz = [float(x) for x in mz]\n    mz = [int(100 * x) for x in mz]\n\n    ity = [float(x) for x in intensities]\n    ity = [x/max(ity) for x in ity]\n\n    print(f\"mz shape: {len(mz)}, ity shape: {len(ity)}\")\n\n    #if max(mz) &gt; 50000:\n    #    return [] \n\n    enc_input2 = []\n\n    for j in range(0, prid_num):\n        mz.append(0)\n        ity.append(0)\n\n    print(f\"mz shape after padding: {len(mz)}, ity shape after padding: {len(ity)}\")\n\n    enc_input = [tokenizer.vocab_mz[n] for n in mz]\n    enc_input2.append(enc_input[0:prid_num])\n\n    ity = [ity[0:prid_num]]\n\n    if intensities is not None:\n        return enc_input2, ity\n    else:\n        return enc_input2   \n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.predict","title":"<code>predict(INT, MZ, smiles=None, full_path_to_mdl='/home', device=None, output_dir='', ids_ms2=None, num_pred=10, predict_approach='greedy', predict_approach_args=None)</code>","text":"<p>Run prediction for a batch of spectra and optionally compare to ground truth SMILES. Args:     INT (list of lists): Intensities for one or more MS2 spectra.     MZ (list of lists): m/z values for one or more MS2 spectra.     smiles (list, optional): List of ground truth SMILES strings. Defaults to None.     full_path_to_mdl (str, optional): Path to model checkpoint. Defaults to \"/home\".     device: Torch device. Defaults to None.     output_dir (str, optional): Directory to save results. Defaults to \"\".     ids_ms2 (list, optional): List of MS2 identifiers. Defaults to None.     num_pred (int, optional): Number of predictions per input. Defaults to 10.     predict_approach (str, optional): Decoding approach (\"greedy\" or \"beam_search\"). Defaults to \"greedy\".     predict_approach_args (dict, optional): Arguments for decoding approach. Defaults to None. Returns:     None</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def predict(self, INT, \n            MZ, \n            smiles=None, \n            full_path_to_mdl=\"/home\", \n            device=None, \n            output_dir=\"\", \n            ids_ms2=None, \n            num_pred=10, \n            predict_approach=\"greedy\",\n            predict_approach_args=None):\n    \"\"\"\n    Run prediction for a batch of spectra and optionally compare to ground truth SMILES.\n    Args:\n        INT (list of lists): Intensities for one or more MS2 spectra.\n        MZ (list of lists): m/z values for one or more MS2 spectra.\n        smiles (list, optional): List of ground truth SMILES strings. Defaults to None.\n        full_path_to_mdl (str, optional): Path to model checkpoint. Defaults to \"/home\".\n        device: Torch device. Defaults to None.\n        output_dir (str, optional): Directory to save results. Defaults to \"\".\n        ids_ms2 (list, optional): List of MS2 identifiers. Defaults to None.\n        num_pred (int, optional): Number of predictions per input. Defaults to 10.\n        predict_approach (str, optional): Decoding approach (\"greedy\" or \"beam_search\"). Defaults to \"greedy\".\n        predict_approach_args (dict, optional): Arguments for decoding approach. Defaults to None.\n    Returns:\n        None\n    \"\"\"\n    my_signal_math = SignalMath()\n    if device is None:\n        device = self.device\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    if ids_ms2 is not None:\n        filenames = [f'{output_dir}/pred_{ids_ms2[i]}.json' for i in range(len(MZ))]\n    else:\n        filenames = [f'{output_dir}/pred_{i}.json' for i in range(len(MZ))]\n    model, tokenizer = self.load_model_and_tokenizer(full_path_to_mdl, device)\n    cnt_correct_smiles = 0\n    for sample_i in range(len(MZ)):\n        intensities, Spec_list_Mz = my_signal_math.find_peak(np.array(INT[sample_i]), np.array(MZ[sample_i]))\n\n        if intensities is not None:\n            enc_input2, intensities, dec_input2, real_smiles = self.prepare_inputs(tokenizer, Spec_list_Mz, smiles, sample_i, intensities=intensities)\n        else:\n            enc_input2, dec_input2, real_smiles = self.prepare_inputs(tokenizer, Spec_list_Mz, smiles, sample_i)\n\n        if len(enc_input2) == 0:\n            continue\n\n        enc_inputs = torch.LongTensor(enc_input2).to(device)\n\n        if intensities is not None:\n            intensities = torch.LongTensor(intensities).to(torch.float32).to(device)\n\n        ans, correct_count = self.predict_single_sample(\n            model, tokenizer, enc_inputs, intensities, num_pred, predict_approach, predict_approach_args, real_smiles=real_smiles if smiles is not None else None\n        )\n        cnt_correct_smiles += correct_count\n        self.write_predictions_to_file(ans, smiles, filenames, sample_i)\n    print(f\"Accuracy: {cnt_correct_smiles/len(MZ)}\")\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.predict_single_sample","title":"<code>predict_single_sample(model, tokenizer, enc_inputs, intensities, num_pred, predict_approach, predict_approach_args, real_smiles=None)</code>","text":"<p>Runs prediction/decoding for a single sample. Args:     model: Transformer model.     tokenizer: Tokenizer object.     enc_inputs: Encoder input tensor(s).     intensities: Intensities for the sample.     num_pred: Number of predictions per input.     predict_approach: Decoding approach (\"greedy\" or \"beam_search\").     predict_approach_args: Arguments for decoding approach.     real_smiles: Real SMILES string(s) for the sample (optional). Returns:     ans: List of prediction results for this sample.     SIM: List of similarity scores (if real_smiles is provided).     cnt_correct_smiles: Count of correct predictions (if real_smiles is provided).</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def predict_single_sample(self, model, tokenizer, enc_inputs, intensities, num_pred, predict_approach, predict_approach_args, real_smiles=None):\n    \"\"\"\n    Runs prediction/decoding for a single sample.\n    Args:\n        model: Transformer model.\n        tokenizer: Tokenizer object.\n        enc_inputs: Encoder input tensor(s).\n        intensities: Intensities for the sample.\n        num_pred: Number of predictions per input.\n        predict_approach: Decoding approach (\"greedy\" or \"beam_search\").\n        predict_approach_args: Arguments for decoding approach.\n        real_smiles: Real SMILES string(s) for the sample (optional).\n    Returns:\n        ans: List of prediction results for this sample.\n        SIM: List of similarity scores (if real_smiles is provided).\n        cnt_correct_smiles: Count of correct predictions (if real_smiles is provided).\n    \"\"\"\n    jihe = []\n    SIM = []\n    ans = []\n    sm = ''\n    cnt_correct_smiles = 0\n    for i in range(len(enc_inputs)):\n        for j in range(num_pred):\n            if predict_approach == \"greedy\":\n                enc_smiles_predicted = self.greedy_decoder(model, enc_inputs[i].view(1, -1).to(enc_inputs.device), sm, start_symbol=tokenizer.vocab_smi['&lt;SOS&gt;'], device=enc_inputs.device, tokenizer=tokenizer, intensities=intensities)\n            elif predict_approach == \"beam_search\":\n                enc_smiles_predicted = self.beam_search_decoder(model, enc_inputs[i].view(1, -1), tokenizer.vocab_smi['&lt;SOS&gt;'], beam_size=predict_approach_args['beam_size'], intensities=intensities)\n            predictsmiles = [tokenizer.vocab_smi_rev[n.item()] for n in enc_smiles_predicted.squeeze()]\n            psm = ''.join(predictsmiles)\n            psm = psm.replace('&lt;PAD&gt;', '')\n            c = psm.find('&lt;EOS&gt;')\n            if real_smiles is not None:\n                realsmiles = real_smiles[i]\n                try:\n                    max_sim = similarity_score(psm[0:c], str(realsmiles), which_similarity=\"all\")\n                    tomitosim = similarity_score(psm[0:c], str(realsmiles), which_similarity=\"tomitosim\")\n                except Exception as e:\n                    max_sim = 0.0\n                    tomitosim = 0.0\n                SIM.append(max_sim)\n                DITT = dict(num=i, predict=psm[0:c], real=str(realsmiles), similar=max_sim, tomitosim=tomitosim)\n                print('predictsmiles:', psm[0:c])\n                print('realsmiles:', realsmiles)\n                if psm[0:c] == realsmiles:\n                    cnt_correct_smiles += 1\n            else:\n                DITT = dict(num=i, predict=psm[0:c])\n                print('predictsmiles:', psm[0:c])\n            jihe.append(DITT)\n        if real_smiles is not None:\n            if SIM:\n                max_sim = max(SIM)\n                pos = SIM.index(max_sim)\n                print(\"--------------------------------------\")\n                print('From all predicted smiles, max sim:', max_sim)\n                print('Smiles for max sim:', jihe[pos]['predict'])\n        ans.append(jihe)\n        SIM = []\n        jihe = []\n    return ans, cnt_correct_smiles\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.prepare_inputs","title":"<code>prepare_inputs(tokenizer, Spec_list_Mz, smiles=None, sample_i=None, intensities=None)</code>","text":"<p>Prepares encoder and decoder inputs for a sample. Args:     tokenizer: Tokenizer object.     Spec_list_Mz: List of m/z values for the sample.     smiles: List of ground truth SMILES strings (optional).     sample_i: Index of the sample (optional, required if smiles is provided).     intensities: List of intensity values for the sample (optional). Returns:     enc_input2: Encoder input.     dec_input2: Decoder input (if smiles is provided, else None).     real_smiles: Real SMILES (if smiles is provided, else None).</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def prepare_inputs(self, tokenizer, Spec_list_Mz, smiles=None, sample_i=None, intensities=None):\n    \"\"\"\n    Prepares encoder and decoder inputs for a sample.\n    Args:\n        tokenizer: Tokenizer object.\n        Spec_list_Mz: List of m/z values for the sample.\n        smiles: List of ground truth SMILES strings (optional).\n        sample_i: Index of the sample (optional, required if smiles is provided).\n        intensities: List of intensity values for the sample (optional).\n    Returns:\n        enc_input2: Encoder input.\n        dec_input2: Decoder input (if smiles is provided, else None).\n        real_smiles: Real SMILES (if smiles is provided, else None).\n    \"\"\"\n    if smiles is None:\n        if intensities is not None: # for arch_3\n            enc_input2, ity = self.make_encoder_inputs(tokenizer, Spec_list_Mz, intensities=intensities)\n            if len(enc_input2) == 0:\n                return [], None, None, None\n            else:\n                return enc_input2, ity, None, None\n        else:\n            enc_input2 = self.make_encoder_inputs(tokenizer, Spec_list_Mz)\n            if len(enc_input2) == 0:\n                return [], None, None\n            else:\n                return enc_input2, None, None\n    else:\n        dec_input2, real_smiles = self.make_decoder_inputs(tokenizer, Spec_list_Mz, smiles[sample_i])\n        if intensities is not None:\n            enc_input2, ity = self.make_encoder_inputs(tokenizer, Spec_list_Mz, intensities=intensities)\n            if len(enc_input2) == 0:\n                return [], None, None, None\n            else:\n                return enc_input2, ity, dec_input2, real_smiles\n        else:\n            enc_input2 = self.make_encoder_inputs(tokenizer, Spec_list_Mz)\n            if len(enc_input2) == 0:\n                return [], None, None\n            else:\n                return enc_input2, dec_input2, real_smiles\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.ranking_smiles","title":"<code>ranking_smiles(predictions)</code>","text":"<p>Rank SMILES predictions when no ground truth is available. For now, returns predictions as-is since we don't have similarity metrics.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list</code> <p>List of prediction dictionaries</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>Sorted list of predictions</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def ranking_smiles(self, predictions):\n    \"\"\"\n    Rank SMILES predictions when no ground truth is available.\n    For now, returns predictions as-is since we don't have similarity metrics.\n\n    Args:\n        predictions (list): List of prediction dictionaries\n\n    Returns:\n        list: Sorted list of predictions\n    \"\"\"\n    # Simple ranking - could be enhanced with other metrics\n    return predictions\n</code></pre>"},{"location":"prediction/#ptfifrijolpujc.PredictWorkflow.PredictWorkflowModule.write_predictions_to_file","title":"<code>write_predictions_to_file(ans, smiles, filenames, sample_i)</code>","text":"<p>Writes the predictions for a sample to a file, sorting as needed. Args:     ans: List of prediction results for this sample.     smiles: List of ground truth SMILES strings (optional).     filenames: List of output filenames.     sample_i: Index of the sample.</p> Source code in <code>ptfifrijolpujc/PredictWorkflow.py</code> <pre><code>def write_predictions_to_file(self, ans, smiles, filenames, sample_i):\n    \"\"\"\n    Writes the predictions for a sample to a file, sorting as needed.\n    Args:\n        ans: List of prediction results for this sample.\n        smiles: List of ground truth SMILES strings (optional).\n        filenames: List of output filenames.\n        sample_i: Index of the sample.\n    \"\"\"\n    if smiles is not None:\n        sorted_data = sorted(ans[0], key=lambda x: x['similar'], reverse=True)\n    else:\n        sorted_data = self.ranking_smiles(ans[0])\n    final = json.dumps(sorted_data)\n    with open(filenames[sample_i], 'w') as fileOb:\n        fileOb.write(final)\n    print('predict!')\n</code></pre>"},{"location":"prepredict_tutorial_notebook/","title":"Prepredict tutorial notebook","text":""},{"location":"preprediction/","title":"Pre-prediction","text":""},{"location":"preprediction/#ptfifrijolpujc.PrePredictWorkflow.PrePredictWorkflow","title":"<code>PrePredictWorkflow</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>ptfifrijolpujc/PrePredictWorkflow.py</code> <pre><code>class PrePredictWorkflow(object):\n    def __init__(self, filename):\n        \"\"\"\n        Initializes the pyOpenMS experiment with the .mzML file provided.\n\n        Args:\n            filename (str): filename of the .mzML file. Should include the .mzML extension\n        \"\"\"\n        self.exp_filename = filename\n        self.handle_to_exp = oms.MSExperiment()\n        oms.MzMLFile().load(filename, self.handle_to_exp)\n    def get_spectrum(self, rt_idx):\n        \"\"\"\n        Wrapper to the pyOpenMS method. Obtains the mass spectrum corresponding to the given retention time index `rt_idx`.\n\n        **The mass spectrum can be either the MS1, MS2, or MSn**. \n\n        The level of the mass spectrum can be checked with `self.handle_to_exp.getSpectra()[rt_idx]`.\n\n        Args:\n            rt_idx (int): an index identifying retention times. Ranges from [0, `len(self.handle_to_exp.getSpectra())`].\n\n        Returns:\n            mz (list): list of m/z.\n            inty (list): list of intensities. \n        \"\"\"\n        mz, inty = self.handle_to_exp.getSpectrum(rt_idx).get_peaks() \n        return mz, inty\n\n    def filter_spectra(self, dict_params, filter_type=\"range\", spectra_idx=0, ms_lvl=None):\n        \"\"\"_summary_\n\n        Args:\n            spectra_idx (_type_): _description_\n            dict_params (_type_): _description_\n            filter_type (str, optional): _description_. Defaults to \"range\".\n        \"\"\"\n        observed_spectrum = self.handle_to_exp.getSpectra()[spectra_idx]\n\n        if filter_type == \"range\":\n            mz_start = dict_params[\"mz_start\"]\n            mz_end = dict_params[\"mz_end\"]\n            ms_lvl = dict_params[\"ms_lvl\"]\n            filtered = oms.MSExperiment()\n            for s in self.handle_to_exp:\n                if s.getMSLevel() &gt; ms_lvl:\n                    filtered_mz = []\n                    filtered_int = []\n                    for mz, i in zip(*s.get_peaks()):\n                        if mz &gt; mz_start and mz &lt; mz_end:\n                            filtered_mz.append(mz)\n                            filtered_int.append(i)\n                    if len(filtered_mz) == 0:\n                        continue\n                    filtered_spec = oms.MSSpectrum()\n                    filtered_spec.setRT(s.getRT())\n                    filtered_spec.set_peaks((filtered_mz, filtered_int))\n                    filtered.addSpectrum(filtered_spec)\n            filtered.getSpectrum(2).get_peaks()\n        elif filter_type == \"win_mover\":\n            spectra_idx = dict_params[\"spectra_idx\"]\n            win_size = dict_params[\"win_size\"]\n            peak_cnt = dict[\"peak_cnt\"]\n            move_type = dict[\"move_type\"]\n\n            observed_spectrum = self.handle_to_exp.getSpectra()[spectra_idx]\n            window_mower_filter = oms.WindowMower()\n\n            # Copy the original spectrum\n            mowed_spectrum = copy.deepcopy(observed_spectrum)\n\n            # Set parameters\n            params = oms.Param()\n            # Defines the m/z range of the sliding window\n            params.setValue(\"windowsize\", win_size, \"\") #100\n            # Defines the number of highest peaks to keep in the sliding window\n            params.setValue(\"peakcount\", peak_cnt, \"\") # 1\n            # Defines the type of window movement: jump (window size steps) or slide (one peak steps)\n            params.setValue(\"movetype\", move_type, \"\") # params.setValue(\"movetype\", \"jump\", \"\"), \n                                                       # params.setValue(\"movetype\", \"slide\", \"\")\n            # Apply window mowing\n            window_mower_filter.setParameters(params)\n            window_mower_filter.filterPeakSpectrum(mowed_spectrum)\n        elif filter_type == \"thr_mover\":\n            threshold = dict_params[\"threshold\"]\n\n            threshold_mower_spectrum = copy.deepcopy(observed_spectrum)\n            threshold_mower_filter = oms.ThresholdMower()\n\n            # Set parameters\n            params = oms.Param()\n            params.setValue(\"threshold\", threshold, \"\") # 20.0\n\n            # Apply threshold mowing\n            threshold_mower_filter.setParameters(params)\n            threshold_mower_filter.filterPeakSpectrum(threshold_mower_spectrum)\n        elif filter_type == \"n_long_peaks\":\n            nlargest_filter = oms.NLargest()\n\n            # Set parameters\n            params = oms.Param()\n            params.setValue(\"n\", 4, \"\")\n\n            # Apply N-Largest filter\n            nlargest_filter.setParameters(params)\n            nlargest_filter.filterPeakSpectrum(nlargest_spectrum)\n\n    def ms_noise_reduction(self, type_spectrum=1, which_spectrum=\"first\", frame_length=11, poly_order=4, out_return=True):\n        \"\"\"\n        Applies a smoothing to the chosen mass spectrum type (1 or 2) and to the number\n\n        **To do**:\n        - Give the option to change `SavitskyGolay` by an adpatatvive filter.\n\n        Args:\n            type_spectrum (int): which type of spectrum. Only options are: 1 or 2.\n            which_spectrum (str): - `first` for first spectrum available \n                                  - `all` for all the available spectrums of type `type_spectrum`\n            frame_length (int): sliding window parameter of the Savitsky Golay filter\n            poly_order (int): order of polynomial. Parameter of the Savitsky Golay filter.\n            out_return (bool): `True` if the user wants to return an `MSSpectrum()` or `False` if the noise reduction wants to be performed on the `handle_to_exp` property of the class, having all the spectra. In the latter case, nothing will be returned and the modified spectrums will be saved on the spectrums of the experiment.\n        Returns:\n            None: returns `None` if `out_return`=`False`\n            spectrum_denoised (oms.MSpectrum or list of oms.MSSpectrum): one spectrum of type `type_spectrum` with noise reduction, or all the spectrums of type `type_spectrum` \n        \"\"\"\n\n        if which_spectrum == \"first\":\n            if out_return:\n                spectrum_denoised = oms.MSSpectrum()\n            cnt_ms = 1\n            for spec_i in self.handle_to_exp.getSpectra():\n                if spec_i.getMSLevel() == type_spectrum:\n                    if which_spectrum == \"first\" and cnt_ms == 1:\n                        if out_return:\n                            spectrum_denoised.setRT(spec_i.getRT())\n                            spectrum_denoised.set_peaks(spec_i.get_peaks())\n\n                        mySavGolFilter = oms.SavitzkyGolayFilter()\n                        param = mySavGolFilter.getParameters()\n                        param.setValue(\"frame_length\", frame_length)\n                        param.setValue(\"polynomial_order\", poly_order)\n                        mySavGolFilter.setParameters(param)\n\n                        if out_return:\n                            mySavGolFilter.filter(spectrum_denoised)\n                        else:\n                            mySavGolFilter.filter(spec_i)\n                        break\n                    cnt_ms = cnt_ms + 1\n        elif which_spectrum == \"all\":\n            if out_return:\n                spectrum_denoised = []\n            for spec_i in self.handle_to_exp.getSpectra():\n                if spec_i.getMSLevel() == type_spectrum:\n                    if out_return:\n                        this_spectrum_denoised = oms.MSSpectrum()\n                        this_spectrum_denoised.setRT(spec_i.getRT())\n                        this_spectrum_denoised.set_peaks(spec_i.get_peaks())\n\n                    mySavGolFilter = oms.SavitzkyGolayFilter()\n                    param = mySavGolFilter.getParameters()\n                    param.setValue(\"frame_length\", frame_length)\n                    param.setValue(\"polynomial_order\", poly_order)\n                    mySavGolFilter.setParameters(param)\n\n                    if out_return:\n                        mySavGolFilter.filter(this_spectrum_denoised)\n                        spectrum_denoised.append(this_spectrum_denoised)\n                    else:\n                        mySavGolFilter.filter(spec_i)\n\n        if out_return:\n            return spectrum_denoised\n        else:\n            return None\n\n    def QC_Normalizer(self, path_to_standard_sample, tol_mz=0.01):\n        \"\"\"\n        Applies quality control (QC) normalization  \n\n        In the folder `path_to_standard_sample` there has to be **ONLY** the following files:\n        - .mzML standard sample files\n        - .csv with 2 columns: m/z, retention index\n            mz, ri -&gt; header of .csv file\n\n        Args:\n            path_to_standard_sample (str): path to the folder containing all the .mzML standard sample files.\n        Returns:\n            experiment_normalized (MSExperiment): normalized and standardized experiment\n        \"\"\"\n        # Define tolerance values for m/z and RT matching\n        mz_tolerance = 0.01  # Adjust based on instrument's resolution\n        rt_tolerance = 5.0  # Adjust based on experiment conditions\n\n        rt = []\n        standard_samples_exps = []\n        for file_i in os.listdir():\n            if file_i.split(\".\")[1] == \"mzML\":\n                this_std_sample_exp = oms.MSExperiment()\n                oms.MzMLFile().load(file_i, this_std_sample_exp)\n                standard_samples_exps.append(this_std_sample_exp)\n\n        for file_i in os.listdir():\n            if file_i.split(\".\")[1] == \"csv\":\n                df_ri_mz = pd.read_csv(file_i)\n                mzs = df_ri_mz['mz'].tolist()\n\n                for this_mz in mzs:\n                    for std_smple in standard_samples_exps:\n                        # Assumption: there will be only one m/z in the mzML equal to each m/z in the .csv file\n                        for spec_i in std_smple.getSpectra():\n                            if spec_i.getMSLevel() == 2:\n                                precursors = spec_i.getPrecursors()\n                                if not precursors:\n                                    continue\n\n                                precursor_mz = precursors[0].getMZ()\n\n                                # Check if the feature and the MS/MS spectrum match within the tolerances\n                                if abs(this_mz - precursor_mz) &lt;= mz_tolerance:\n                                    rt.append(spec_i.getRT())\n                                    break\n            else:\n                continue\n\n\n    def make_features(self, mass_trace_mass_error_ppm=10.0, \n                    mass_trace_noise_threshold_int=0.12e04, \n                    mass_trace_chrom_peak_snr=3.0, \n                    mass_trace_min_sample_rate=0.5,\n                    mass_trace_min_length=5.0,\n                    mass_trace_max_length=-1.0,\n                    mass_trace_quant_method=\"area\",\n                    elution_peak_width_filtering=\"auto\",\n                    elution_peak_chrom_fwhm=2.0,\n                    elution_peak_chrom_peak_snr=3.0,\n                    elution_peak_min_fwhm=1.0,\n                    elution_peak_max_fwhm=60.0,\n                    elution_peak_masstrace_snr_filtering=\"false\",\n                    feature_detection_remove_single_traces=\"false\",\n                    feature_detection_local_rt_range=2.0,\n                    feature_detection_local_mz_range=10.0,\n                    feature_detection_charge_lower_bound=1,\n                    feature_detection_charge_upper_bound=3,\n                    feature_detection_chrom_fwhm=2.0,\n                    feature_detection_report_summed_ints=\"false\",\n                    feature_detection_enable_RT_filtering=\"true\",\n                    feature_detection_isotope_filtering_model=\"metabolites (5% RMS)\",\n                    feature_detection_mz_scoring_13C=\"false\",\n                    feature_detection_use_smoothed_intensities=\"true\",\n                    feature_detection_report_convex_hulls=\"false\",\n                    feature_detection_report_chromatograms=\"false\",\n                    feature_detection_mz_scoring_by_elements=\"false\"):\n        feature_finder_dict = {'metabo': oms.FeatureFindingMetabo(),\n                            'metabo_ident': oms.FeatureFinderAlgorithmMetaboIdent()}\n\n        mzML_files = [self.exp_filename, self.exp_filename]\n\n        # Processing the chromatograph to to get the features in the experiment, retention times and parental mass\n        feature_maps = []\n        for file in mzML_files:\n            # load mzML file into MSExperiment\n            exp = oms.MSExperiment()\n            oms.MzMLFile().load(file, exp)  # load each mzML file to an OpenMS file format (MSExperiment)\n\n            ########################################### MASS TRACE DETECTION ################################################\n            mass_traces = ([])  # introduce an empty list where the mass traces will be loaded\n            mtd = oms.MassTraceDetection()\n            mtd_par = (mtd.getDefaults()) # get the default parameters in order to edit them\n            mtd_par.setValue(\"mass_error_ppm\", mass_trace_mass_error_ppm)  # Allowed mass deviation (in ppm). High-res instrument, orbitraps\n            mtd_par.setValue(\"noise_threshold_int\", mass_trace_noise_threshold_int)  # Intensity threshold below which peaks are removed as noise. Data-dependent (usually works for orbitraps)\n            mtd_par.setValue(\"chrom_peak_snr\", mass_trace_chrom_peak_snr) # minimum intensity above noise_threshold_int (signal-to-noise) a peak should have to be considered an apex\n            mtd_par.setValue(\"min_sample_rate\", mass_trace_min_sample_rate) # Minimum fraction of scans along the mass trace that must contain a peak.\n            mtd_par.setValue(\"min_trace_length\", mass_trace_min_length) # Minimum expected length of a mass trace (in seconds)\n            mtd_par.setValue(\"max_trace_length\", mass_trace_max_length) # Maximum expected length of a mass trace (in seconds). Set to a negative value to disable maximal length check during mass trace detection.   \n\n            # Method of quantification for mass traces. For LC data 'area' is recommended, 'median' for direct injection data. 'max_height' simply uses the most intense peak in the trace.\n            mtd_par.setValue(\"quant_method\", mass_trace_quant_method) # options: {area, median, max_height}. \n            mtd.setParameters(mtd_par)  # set the new parameters\n            mtd.run(exp, mass_traces, 0)  # run mass trace detection\n\n            ##################################### ELUTION PEAK DETECTION #################################################### \n            # The SavitzkyGolay smoothing with a second order polynomial and a frame length of the fixed peak width is used here for peak deconvolution\n            mass_traces_deconvol = []\n            epd = oms.ElutionPeakDetection()\n            epd_par = epd.getDefaults()\n            epd_par.setValue(\"width_filtering\", elution_peak_width_filtering)  # Enable filtering of unlikely peak widths. The fixed setting filters out mass traces outside the [min_fwhm, max_fwhm] interval. The auto setting filters with the 5 and 95% quantiles of the peak width distribution.\n            epd_par.setValue(\"chrom_fwhm\", elution_peak_chrom_fwhm)  # Expected full-width-at-half-maximum of chromatographic peaks (in seconds)\n            epd_par.setValue(\"chrom_peak_snr\", elution_peak_chrom_peak_snr)  # Minimum signal-to-noise a mass trace should have.\n            epd_par.setValue(\"min_fwhm\", elution_peak_min_fwhm)  # Minimum full-width-at-half-maximum of chromatographic peaks (in seconds). Ignored if parameter width_filtering is off or auto.\n            epd_par.setValue(\"max_fwhm\", elution_peak_max_fwhm)  # Maximum full-width-at-half-maximum of chromatographic peaks (in seconds). Ignored if parameter width_filtering is off or auto.\n            epd_par.setValue(\"masstrace_snr_filtering\", elution_peak_masstrace_snr_filtering)  # Apply post-filtering by signal-to-noise ratio after smoothing.\n            epd.setParameters(epd_par)\n            epd.detectPeaks(mass_traces, mass_traces_deconvol)\n\n            ################################################## FEATURE DETECTION  #############################################3\n            feature_map = oms.FeatureMap()  # output features\n            chrom_out = []  # output chromatograms\n            ffm = feature_finder_dict['metabo']\n            ffm_par = ffm.getDefaults()\n            ffm_par.setValue(\"remove_single_traces\", feature_detection_remove_single_traces)  # remove mass traces without satellite isotopic traces\n            ffm_par.setValue(\"local_rt_range\", feature_detection_local_rt_range)  # RT range where to look for coeluting mass traces\n            ffm_par.setValue(\"local_mz_range\", feature_detection_local_mz_range)  # MZ range where to look for coeluting mass traces\n            ffm_par.setValue(\"charge_lower_bound\", feature_detection_charge_lower_bound)  # Lowest charge state to consider\n            ffm_par.setValue(\"charge_upper_bound\", feature_detection_charge_upper_bound)  # Highest charge state to consider\n            ffm_par.setValue(\"chrom_fwhm\", feature_detection_chrom_fwhm)  # Expected chromatographic peak width (in seconds)\n            ffm_par.setValue(\"report_summed_ints\", feature_detection_report_summed_ints)  # Set to true for a feature intensity summed up over all traces rather than using monoisotopic trace intensity alone.\n            ffm_par.setValue(\"enable_RT_filtering\", feature_detection_enable_RT_filtering)  # Require sufficient overlap in RT while assembling mass traces. Disable for direct injection data.\n            ffm_par.setValue(\"isotope_filtering_model\", feature_detection_isotope_filtering_model)  # Remove/score candidate assemblies based on isotope intensities. SVM isotope models for metabolites were trained with either 2% or 5% RMS error. For peptides, an averagine cosine scoring is used. Select the appropriate noise model according to the quality of measurement or MS device\n            ffm_par.setValue(\"mz_scoring_13C\", feature_detection_mz_scoring_13C)  # Use the 13C isotope peak position (~1.003355 Da) as the expected shift in m/z for isotope mass traces (highly recommended for lipidomics!). Disable for general metabolites (as described in Kenar et al. 2014, MCP.)\n            ffm_par.setValue(\"use_smoothed_intensities\", feature_detection_use_smoothed_intensities)  # Use LOWESS intensities instead of raw intensities\n            ffm_par.setValue(\"report_convex_hulls\", feature_detection_report_convex_hulls)  # Augment each reported feature with the convex hull of the underlying mass traces (increases featureXML file size considerably)\n            ffm_par.setValue(\"report_chromatograms\", feature_detection_report_chromatograms)  # Adds Chromatogram for each reported feature (Output in mzml)\n            ffm_par.setValue(\"remove_single_traces\", \"false\")  # Remove unassembled traces (single traces)\n            ffm_par.setValue(\"mz_scoring_by_elements\", feature_detection_mz_scoring_by_elements)  # Use the m/z range of the assumed elements to detect isotope peaks. A expected m/z range is computed from the isotopes of the assumed elements. If enabled, this ignores 'mz_scoring_13C'\n            ffm_par.setValue(\"elements\", \"CHNOPSFClBrI\")  # Elements assumes to be present in the sample (this influences isotope detection).\n            ffm.setParameters(ffm_par)\n            ffm.run(mass_traces_deconvol, feature_map, chrom_out)\n            feature_map.setUniqueIds()  # Assigns a new, valid unique id per feature\n            feature_map.setPrimaryMSRunPath([file.encode()])  # Sets the file path to the primary MS run (usually the mzML file)\n            feature_maps.append(feature_map)\n\n        return feature_maps\n\n    def make_ms2_from_features(self, \n                                feature_maps, \n                                mz_tolerance=2, \n                                rt_tolerance=15,\n                                max_peak_filter_pptg=0.2):\n\n        exp_4_transfomer = oms.MSExperiment()\n\n        for feature_map in feature_maps:\n            for feature in feature_map:\n                #The MS2 spectrum, corresponding RT and MS1 for a feture are stored in the next expirement class\n\n                # Get the m/z and RT (retention time) of the feature\n                feature_mz = feature.getMZ()\n                feature_rt = feature.getRT()\n\n                # Iterate through the spectra in the MS data\n                for spectrum in self.handle_to_exp:\n\n                    spctrm_4_exptrnsfrmr = oms.MSSpectrum()\n\n                    # Check if the spectrum is an MS/MS spectrum\n                    if spectrum.getMSLevel() == 2:\n                        # Get the precursor information for this MS/MS spectrum\n                        precursors = spectrum.getPrecursors()\n                        if not precursors:\n                            continue\n\n                        precursor_mz = precursors[0].getMZ()\n                        precursor_rt = spectrum.getRT()\n\n                        p_4_exptrnsfrmr = oms.Precursor()\n                        p_4_exptrnsfrmr.setMZ(precursor_mz)\n                        spctrm_4_exptrnsfrmr.setRT(precursor_rt)\n                        spctrm_4_exptrnsfrmr.setPrecursors([p_4_exptrnsfrmr])\n                        spctrm_4_exptrnsfrmr.setMSLevel(2)\n\n                        # Check if the feature and the MS/MS spectrum match within the tolerances\n                        if abs(feature_mz - precursor_mz) &lt;= mz_tolerance and abs(feature_rt - precursor_rt) &lt;= rt_tolerance:\n                            #print(f\"Found MS/MS spectrum for feature m/z: {feature_mz:.6f}, RT: {feature_rt:.3f}\")\n\n                            mzs = []\n                            intsts = []\n                            for peak in spectrum:\n                                mzs.append(peak.getMZ())\n                                intsts.append(peak.getIntensity())\n\n                            spctrm_4_exptrnsfrmr.set_peaks([mzs,intsts])\n                            exp_4_transfomer.addSpectrum(spctrm_4_exptrnsfrmr)\n\n        merger = oms.SpectraMerger()\n        #param = merger.getParameters()\n        #param.setValue(\"average_gaussian:ms_level\",2)\n        #merger.setParameters(param)\n        merger.mergeSpectraPrecursors(exp_4_transfomer)\n\n        # Filtering MS2 spectra\n        filtered_exp_4_transformer = oms.MSExperiment()\n        for spectrum in exp_4_transfomer:\n            if spectrum.getMSLevel() == 2:  # Only process MS2 spectra\n                filter = oms.ThresholdMower() #Function to remove peaks from spectra given a threshold\n                param = filter.getParameters()\n                max_peak = spectrum.getMaxIntensity()\n\n                filter_peak = max_peak * max_peak_filter_pptg #Defining the porcentage of peak filtering based on the maximum of invidual MS2 merged\n\n                param.setValue(\"threshold\", filter_peak) #I should understand how to define this threshold.\n                filter.setParameters(param)\n\n                filter.filterPeakSpectrum(spectrum)\n                filtered_exp_4_transformer.addSpectrum(spectrum)\n\n        mzs = []\n        itys = []\n        for s in filtered_exp_4_transformer:\n            mz, ity = s.get_peaks()\n            mzs.append(mz)\n            itys.append(ity)\n\n        mzs = [mz for mz in mzs if len(mz) &gt; 1] \n        itys = [ity for ity in itys if len(ity) &gt; 1]\n\n        return mzs, itys \n</code></pre>"},{"location":"preprediction/#ptfifrijolpujc.PrePredictWorkflow.PrePredictWorkflow.QC_Normalizer","title":"<code>QC_Normalizer(path_to_standard_sample, tol_mz=0.01)</code>","text":"<p>Applies quality control (QC) normalization  </p> <p>In the folder <code>path_to_standard_sample</code> there has to be ONLY the following files: - .mzML standard sample files - .csv with 2 columns: m/z, retention index     mz, ri -&gt; header of .csv file</p> <p>Parameters:</p> Name Type Description Default <code>path_to_standard_sample</code> <code>str</code> <p>path to the folder containing all the .mzML standard sample files.</p> required <p>Returns:     experiment_normalized (MSExperiment): normalized and standardized experiment</p> Source code in <code>ptfifrijolpujc/PrePredictWorkflow.py</code> <pre><code>def QC_Normalizer(self, path_to_standard_sample, tol_mz=0.01):\n    \"\"\"\n    Applies quality control (QC) normalization  \n\n    In the folder `path_to_standard_sample` there has to be **ONLY** the following files:\n    - .mzML standard sample files\n    - .csv with 2 columns: m/z, retention index\n        mz, ri -&gt; header of .csv file\n\n    Args:\n        path_to_standard_sample (str): path to the folder containing all the .mzML standard sample files.\n    Returns:\n        experiment_normalized (MSExperiment): normalized and standardized experiment\n    \"\"\"\n    # Define tolerance values for m/z and RT matching\n    mz_tolerance = 0.01  # Adjust based on instrument's resolution\n    rt_tolerance = 5.0  # Adjust based on experiment conditions\n\n    rt = []\n    standard_samples_exps = []\n    for file_i in os.listdir():\n        if file_i.split(\".\")[1] == \"mzML\":\n            this_std_sample_exp = oms.MSExperiment()\n            oms.MzMLFile().load(file_i, this_std_sample_exp)\n            standard_samples_exps.append(this_std_sample_exp)\n\n    for file_i in os.listdir():\n        if file_i.split(\".\")[1] == \"csv\":\n            df_ri_mz = pd.read_csv(file_i)\n            mzs = df_ri_mz['mz'].tolist()\n\n            for this_mz in mzs:\n                for std_smple in standard_samples_exps:\n                    # Assumption: there will be only one m/z in the mzML equal to each m/z in the .csv file\n                    for spec_i in std_smple.getSpectra():\n                        if spec_i.getMSLevel() == 2:\n                            precursors = spec_i.getPrecursors()\n                            if not precursors:\n                                continue\n\n                            precursor_mz = precursors[0].getMZ()\n\n                            # Check if the feature and the MS/MS spectrum match within the tolerances\n                            if abs(this_mz - precursor_mz) &lt;= mz_tolerance:\n                                rt.append(spec_i.getRT())\n                                break\n        else:\n            continue\n</code></pre>"},{"location":"preprediction/#ptfifrijolpujc.PrePredictWorkflow.PrePredictWorkflow.__init__","title":"<code>__init__(filename)</code>","text":"<p>Initializes the pyOpenMS experiment with the .mzML file provided.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>filename of the .mzML file. Should include the .mzML extension</p> required Source code in <code>ptfifrijolpujc/PrePredictWorkflow.py</code> <pre><code>def __init__(self, filename):\n    \"\"\"\n    Initializes the pyOpenMS experiment with the .mzML file provided.\n\n    Args:\n        filename (str): filename of the .mzML file. Should include the .mzML extension\n    \"\"\"\n    self.exp_filename = filename\n    self.handle_to_exp = oms.MSExperiment()\n    oms.MzMLFile().load(filename, self.handle_to_exp)\n</code></pre>"},{"location":"preprediction/#ptfifrijolpujc.PrePredictWorkflow.PrePredictWorkflow.filter_spectra","title":"<code>filter_spectra(dict_params, filter_type='range', spectra_idx=0, ms_lvl=None)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>spectra_idx</code> <code>_type_</code> <p>description</p> <code>0</code> <code>dict_params</code> <code>_type_</code> <p>description</p> required <code>filter_type</code> <code>str</code> <p>description. Defaults to \"range\".</p> <code>'range'</code> Source code in <code>ptfifrijolpujc/PrePredictWorkflow.py</code> <pre><code>def filter_spectra(self, dict_params, filter_type=\"range\", spectra_idx=0, ms_lvl=None):\n    \"\"\"_summary_\n\n    Args:\n        spectra_idx (_type_): _description_\n        dict_params (_type_): _description_\n        filter_type (str, optional): _description_. Defaults to \"range\".\n    \"\"\"\n    observed_spectrum = self.handle_to_exp.getSpectra()[spectra_idx]\n\n    if filter_type == \"range\":\n        mz_start = dict_params[\"mz_start\"]\n        mz_end = dict_params[\"mz_end\"]\n        ms_lvl = dict_params[\"ms_lvl\"]\n        filtered = oms.MSExperiment()\n        for s in self.handle_to_exp:\n            if s.getMSLevel() &gt; ms_lvl:\n                filtered_mz = []\n                filtered_int = []\n                for mz, i in zip(*s.get_peaks()):\n                    if mz &gt; mz_start and mz &lt; mz_end:\n                        filtered_mz.append(mz)\n                        filtered_int.append(i)\n                if len(filtered_mz) == 0:\n                    continue\n                filtered_spec = oms.MSSpectrum()\n                filtered_spec.setRT(s.getRT())\n                filtered_spec.set_peaks((filtered_mz, filtered_int))\n                filtered.addSpectrum(filtered_spec)\n        filtered.getSpectrum(2).get_peaks()\n    elif filter_type == \"win_mover\":\n        spectra_idx = dict_params[\"spectra_idx\"]\n        win_size = dict_params[\"win_size\"]\n        peak_cnt = dict[\"peak_cnt\"]\n        move_type = dict[\"move_type\"]\n\n        observed_spectrum = self.handle_to_exp.getSpectra()[spectra_idx]\n        window_mower_filter = oms.WindowMower()\n\n        # Copy the original spectrum\n        mowed_spectrum = copy.deepcopy(observed_spectrum)\n\n        # Set parameters\n        params = oms.Param()\n        # Defines the m/z range of the sliding window\n        params.setValue(\"windowsize\", win_size, \"\") #100\n        # Defines the number of highest peaks to keep in the sliding window\n        params.setValue(\"peakcount\", peak_cnt, \"\") # 1\n        # Defines the type of window movement: jump (window size steps) or slide (one peak steps)\n        params.setValue(\"movetype\", move_type, \"\") # params.setValue(\"movetype\", \"jump\", \"\"), \n                                                   # params.setValue(\"movetype\", \"slide\", \"\")\n        # Apply window mowing\n        window_mower_filter.setParameters(params)\n        window_mower_filter.filterPeakSpectrum(mowed_spectrum)\n    elif filter_type == \"thr_mover\":\n        threshold = dict_params[\"threshold\"]\n\n        threshold_mower_spectrum = copy.deepcopy(observed_spectrum)\n        threshold_mower_filter = oms.ThresholdMower()\n\n        # Set parameters\n        params = oms.Param()\n        params.setValue(\"threshold\", threshold, \"\") # 20.0\n\n        # Apply threshold mowing\n        threshold_mower_filter.setParameters(params)\n        threshold_mower_filter.filterPeakSpectrum(threshold_mower_spectrum)\n    elif filter_type == \"n_long_peaks\":\n        nlargest_filter = oms.NLargest()\n\n        # Set parameters\n        params = oms.Param()\n        params.setValue(\"n\", 4, \"\")\n\n        # Apply N-Largest filter\n        nlargest_filter.setParameters(params)\n        nlargest_filter.filterPeakSpectrum(nlargest_spectrum)\n</code></pre>"},{"location":"preprediction/#ptfifrijolpujc.PrePredictWorkflow.PrePredictWorkflow.get_spectrum","title":"<code>get_spectrum(rt_idx)</code>","text":"<p>Wrapper to the pyOpenMS method. Obtains the mass spectrum corresponding to the given retention time index <code>rt_idx</code>.</p> <p>The mass spectrum can be either the MS1, MS2, or MSn. </p> <p>The level of the mass spectrum can be checked with <code>self.handle_to_exp.getSpectra()[rt_idx]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>rt_idx</code> <code>int</code> <p>an index identifying retention times. Ranges from [0, <code>len(self.handle_to_exp.getSpectra())</code>].</p> required <p>Returns:</p> Name Type Description <code>mz</code> <code>list</code> <p>list of m/z.</p> <code>inty</code> <code>list</code> <p>list of intensities.</p> Source code in <code>ptfifrijolpujc/PrePredictWorkflow.py</code> <pre><code>def get_spectrum(self, rt_idx):\n    \"\"\"\n    Wrapper to the pyOpenMS method. Obtains the mass spectrum corresponding to the given retention time index `rt_idx`.\n\n    **The mass spectrum can be either the MS1, MS2, or MSn**. \n\n    The level of the mass spectrum can be checked with `self.handle_to_exp.getSpectra()[rt_idx]`.\n\n    Args:\n        rt_idx (int): an index identifying retention times. Ranges from [0, `len(self.handle_to_exp.getSpectra())`].\n\n    Returns:\n        mz (list): list of m/z.\n        inty (list): list of intensities. \n    \"\"\"\n    mz, inty = self.handle_to_exp.getSpectrum(rt_idx).get_peaks() \n    return mz, inty\n</code></pre>"},{"location":"preprediction/#ptfifrijolpujc.PrePredictWorkflow.PrePredictWorkflow.ms_noise_reduction","title":"<code>ms_noise_reduction(type_spectrum=1, which_spectrum='first', frame_length=11, poly_order=4, out_return=True)</code>","text":"<p>Applies a smoothing to the chosen mass spectrum type (1 or 2) and to the number</p> <p>To do: - Give the option to change <code>SavitskyGolay</code> by an adpatatvive filter.</p> <p>Parameters:</p> Name Type Description Default <code>type_spectrum</code> <code>int</code> <p>which type of spectrum. Only options are: 1 or 2.</p> <code>1</code> <code>which_spectrum</code> <code>str</code> <ul> <li><code>first</code> for first spectrum available                    - <code>all</code> for all the available spectrums of type <code>type_spectrum</code></li> </ul> <code>'first'</code> <code>frame_length</code> <code>int</code> <p>sliding window parameter of the Savitsky Golay filter</p> <code>11</code> <code>poly_order</code> <code>int</code> <p>order of polynomial. Parameter of the Savitsky Golay filter.</p> <code>4</code> <code>out_return</code> <code>bool</code> <p><code>True</code> if the user wants to return an <code>MSSpectrum()</code> or <code>False</code> if the noise reduction wants to be performed on the <code>handle_to_exp</code> property of the class, having all the spectra. In the latter case, nothing will be returned and the modified spectrums will be saved on the spectrums of the experiment.</p> <code>True</code> <p>Returns:     None: returns <code>None</code> if <code>out_return</code>=<code>False</code>     spectrum_denoised (oms.MSpectrum or list of oms.MSSpectrum): one spectrum of type <code>type_spectrum</code> with noise reduction, or all the spectrums of type <code>type_spectrum</code></p> Source code in <code>ptfifrijolpujc/PrePredictWorkflow.py</code> <pre><code>def ms_noise_reduction(self, type_spectrum=1, which_spectrum=\"first\", frame_length=11, poly_order=4, out_return=True):\n    \"\"\"\n    Applies a smoothing to the chosen mass spectrum type (1 or 2) and to the number\n\n    **To do**:\n    - Give the option to change `SavitskyGolay` by an adpatatvive filter.\n\n    Args:\n        type_spectrum (int): which type of spectrum. Only options are: 1 or 2.\n        which_spectrum (str): - `first` for first spectrum available \n                              - `all` for all the available spectrums of type `type_spectrum`\n        frame_length (int): sliding window parameter of the Savitsky Golay filter\n        poly_order (int): order of polynomial. Parameter of the Savitsky Golay filter.\n        out_return (bool): `True` if the user wants to return an `MSSpectrum()` or `False` if the noise reduction wants to be performed on the `handle_to_exp` property of the class, having all the spectra. In the latter case, nothing will be returned and the modified spectrums will be saved on the spectrums of the experiment.\n    Returns:\n        None: returns `None` if `out_return`=`False`\n        spectrum_denoised (oms.MSpectrum or list of oms.MSSpectrum): one spectrum of type `type_spectrum` with noise reduction, or all the spectrums of type `type_spectrum` \n    \"\"\"\n\n    if which_spectrum == \"first\":\n        if out_return:\n            spectrum_denoised = oms.MSSpectrum()\n        cnt_ms = 1\n        for spec_i in self.handle_to_exp.getSpectra():\n            if spec_i.getMSLevel() == type_spectrum:\n                if which_spectrum == \"first\" and cnt_ms == 1:\n                    if out_return:\n                        spectrum_denoised.setRT(spec_i.getRT())\n                        spectrum_denoised.set_peaks(spec_i.get_peaks())\n\n                    mySavGolFilter = oms.SavitzkyGolayFilter()\n                    param = mySavGolFilter.getParameters()\n                    param.setValue(\"frame_length\", frame_length)\n                    param.setValue(\"polynomial_order\", poly_order)\n                    mySavGolFilter.setParameters(param)\n\n                    if out_return:\n                        mySavGolFilter.filter(spectrum_denoised)\n                    else:\n                        mySavGolFilter.filter(spec_i)\n                    break\n                cnt_ms = cnt_ms + 1\n    elif which_spectrum == \"all\":\n        if out_return:\n            spectrum_denoised = []\n        for spec_i in self.handle_to_exp.getSpectra():\n            if spec_i.getMSLevel() == type_spectrum:\n                if out_return:\n                    this_spectrum_denoised = oms.MSSpectrum()\n                    this_spectrum_denoised.setRT(spec_i.getRT())\n                    this_spectrum_denoised.set_peaks(spec_i.get_peaks())\n\n                mySavGolFilter = oms.SavitzkyGolayFilter()\n                param = mySavGolFilter.getParameters()\n                param.setValue(\"frame_length\", frame_length)\n                param.setValue(\"polynomial_order\", poly_order)\n                mySavGolFilter.setParameters(param)\n\n                if out_return:\n                    mySavGolFilter.filter(this_spectrum_denoised)\n                    spectrum_denoised.append(this_spectrum_denoised)\n                else:\n                    mySavGolFilter.filter(spec_i)\n\n    if out_return:\n        return spectrum_denoised\n    else:\n        return None\n</code></pre>"},{"location":"quickstart/","title":"How get access to project's documentation","text":"<p>Be sure that you have python installed. Open a terminal and:</p> <ol> <li>Install the required repositories</li> </ol> <pre><code>pip install mkdocs-material\n</code></pre> <pre><code>pip install 'mkdocstrings[python]'\n</code></pre> <pre><code>pip install mkdocs-glightbox\n</code></pre> <ol> <li> <p>Set the current directory to the directory containing the file <code>mkdocs.yml</code></p> </li> <li> <p>Execute</p> </li> </ol> <pre><code>mkdocs serve\n</code></pre> <ol> <li>Open the hyperlink (localhost:80000 or 127.0.0.1:8000) by clicking it or by copying and pasting it in the web browser.</li> </ol>"},{"location":"signalmath/","title":"Signal Math","text":""},{"location":"signalmath/#ptfifrijolpujc.SignalMath.SignalMath","title":"<code>SignalMath</code>","text":"<p>               Bases: <code>object</code></p> <p>Wrappers to TeFT methods, other Library methods related with signal feature extraction (i.e. get peaks, ...)</p> Source code in <code>ptfifrijolpujc/SignalMath.py</code> <pre><code>class SignalMath(object):\n    \"\"\"\n    Wrappers to TeFT methods, other Library methods related with signal feature extraction (i.e. get peaks, ...)\n\n    \"\"\"\n\n    def __init__(self):\n        1\n\n    def add_subtract(self, numbers):\n        result = [(num - 0.1666, num - 0.0833, num, num + 0.0833, num + 0.1666) for num in numbers]\n        return [item for sublist in result for item in sublist]\n\n    def find_appltitude(self, ans2, amp):\n        Idiff = np.convolve(ans2, np.array([1, -1]), 'full')\n        idxPeak = []\n        for i in range(ans2.size):\n            if Idiff[i] &gt; 0 &gt;= Idiff[i + 1] and ans2[i] &gt; amp:\n                idxPeak.append(i)\n        idxPeak = self.remove_close_numbers(idxPeak)\n        return idxPeak\n\n    def remove_close_numbers(self, numbers):\n        for pair in itertools.combinations(numbers, 2):\n            if abs(pair[1] - pair[0]) &lt; 5:\n                numbers.remove(pair[1])\n                return self.remove_close_numbers(numbers)\n        return numbers\n\n    def find_peak(self, INT, MZ):\n        z = self.find_appltitude(INT, 0.1)\n        Spec_list_mz = []\n        Spec_list_amp = []\n        for i in z:\n            Spec_list_mz.append(MZ[i])\n            Spec_list_amp.append(INT[i])\n        Spec_list_amp, Spec_list_mz = (list(t) for t in zip(*sorted(zip(Spec_list_amp, Spec_list_mz), reverse=True)))\n        Spec_list_mz = Spec_list_mz[:5]\n        Spec_list_mz = sorted(Spec_list_mz)\n        Spec_list_Mz = self.add_subtract(Spec_list_mz)\n\n        return Spec_list_mz, Spec_list_Mz\n\n    '''   \n    def make_fragment_tree(self, Spec_list_mz):\n        mode = find_child(Spec_list_mz, 0.04, 0.5)\n        array = edge_weight(Spec_list_mz, mode, 0.0005)\n        Score, Tree = Generate_Tree(array, Spec_list_mz)\n\n        return Score, Tree\n    '''\n</code></pre>"},{"location":"structural_annotation_molecules/","title":"Structural annotation of unknown molecules in a miniaturized mass spectrometer based on a transformer enabled fragment tree method","text":""},{"location":"structural_annotation_molecules/#purpose","title":"Purpose","text":"<p>Generate specific structure of molecules from mass spectrometry spectra.</p> <p>To discover new homologous derivatives, natural product research, non-targeted metabolomics, drug research, food safety, pharmaceutical ingredient analysis, drug detection.</p>"},{"location":"structural_annotation_molecules/#interpretaion-of-mass-spectra","title":"Interpretaion of mass spectra","text":"<p>One common approach to automatically interpret MS\\(^{n}\\) is to search in a mass spectrometry database.</p> <p>Compares (through similarity measure) mass spectra of compounds under specific conditions with a database containing a large number of reference mass spectra.</p>"},{"location":"structural_annotation_molecules/#x-rank","title":"X-Rank","text":"<p>X-Rank algorithm to rank peak intensities of mass spectra, establish correlations between different mass spectra, determine the probability of matching with mass spectra from a reference library, enable crss-mass spectrometry platform recognition.</p>"},{"location":"structural_annotation_molecules/#database-search-algorithms","title":"Database search algorithms","text":""},{"location":"structural_annotation_molecules/#metfrag","title":"MetFrag","text":"<p>Combine database search algorithms and fragment prediction algorithms for identifying the structure of small molecules from tandem mass spectrometry data.</p> <p>Identifies compounds not yet included in mass spectrometry databases.</p>"},{"location":"structural_annotation_molecules/#sirius-series-methods","title":"SIRIUS series methods","text":"<p>More effective mass spectrometry database search algorithm. </p> <p>It uses:</p> <ul> <li>High-resolution isotope pattern analysis</li> <li>Fragment tree (FT)</li> <li>CSI: FingerID (search molecular databases)</li> </ul>"},{"location":"structural_annotation_molecules/#disadvantages","title":"Disadvantages","text":"<ul> <li>Inability to identify unknown natural products and drug metabolites.</li> <li>Miniaturized mass spectrometer used for on-site testing, it is difficult to provide corresponding databases for searching.</li> </ul>"},{"location":"structural_annotation_molecules/#context-of-commonly-used-deep-learning-models","title":"Context of commonly used Deep learning models","text":""},{"location":"structural_annotation_molecules/#advantage","title":"Advantage","text":"<ul> <li>Can generate molecular structures from mass spectrometry spectra without being given explicit rules. Uses molecular-input line-entry system strings and molecular graph constructuon (SMILES).</li> </ul>"},{"location":"structural_annotation_molecules/#canopus","title":"CANOPUS","text":"<p>SVM and DNN</p>"},{"location":"structural_annotation_molecules/#massgenie-transformed-based-dnn","title":"MassGenie: Transformed-based DNN","text":"<p>Transforms the molecular recognition probelm into a language translation problem, where the source language is a list of high-resolution mass spectra peaks, and the translation language is the SMILES strings of the molecule.</p>"},{"location":"structural_annotation_molecules/#darknps","title":"DarkNPS","text":"<p>Based on LSTM for automatic structural analysis of new psychoactive substances.</p>"},{"location":"structural_annotation_molecules/#msnovelist","title":"MSNovelist","text":"<p>Encode-Decoder network to achieve de nove prediction of the structure of unkown compunds from tandem masss spectrometry.</p>"},{"location":"structural_annotation_molecules/#transformer-enabled-fragment-tree-teft","title":"Transformer enabled Fragment Tree (TeFT)","text":"<p>Miniaturized mass spectrometer with low resolution spectra for 16 flavonoid alcohols.</p> <ul> <li> <p>Simulated semantic fragment tree model (SMILES tree) generated through the deep learning Transformer module and the FT directly generated through the original MS\\(^{n}\\) spectral data.</p> </li> <li> <p>Compares similarity of the 2 trees to predict the molecular structure of the tested chemical substance.</p> </li> </ul>"},{"location":"structural_annotation_molecules/#obtaining-data","title":"Obtaining data","text":"<p>MS\\(^{n}\\) spectra were obtained using high-resolution isolation and collision-induce dissociation (CID) sequences by carefully controlling the frequency and amplitude of the AC signal applied to the ion trap.</p> <p>The training set utilized was created by compiling open-source spectrometry databases GNPS, HMDB and MoNA.</p> <ul> <li>Molecular weight &lt; 500 Da</li> <li>Presence of C,H,O,N,P,S,Cl,Br,I,F</li> </ul>"},{"location":"structural_annotation_molecules/#workflow","title":"Workflow","text":"Workflow used by this reference <ol> <li> <p>MS\\(^{n}\\) spectral data -&gt; Transformer -&gt; List of SMILES strings for the molecule</p> </li> <li> <p>Candidate list -&gt; Simulated fragmentation -&gt; Series of SMILES trees</p> </li> </ol> <p>MS\\(^{n}\\) spectra data are sorted according to peak intensities and several fragments with the highest intensity are selected as inputs for the deep learning Transforme module.</p>"},{"location":"structural_annotation_molecules/#preprocessing","title":"Preprocessing","text":""},{"location":"structural_annotation_molecules/#similarity-metrics","title":"Similarity Metrics","text":"<p>All similarities presented here compare the molecular fingerprints (i.e. Morgan) of the two molecules</p> <ul> <li>Dice similarity:</li> </ul> \\[ \\frac{2c}{a+b-c} \\] <ul> <li>Tanimoto:</li> </ul> \\[ \\frac{c}{a+b-c} \\] <ul> <li>Euclidean:</li> </ul> \\[ \\sqrt{a + b - 2c} \\] <p>Where:</p> <ul> <li>a is the number of on bits in molecule A</li> <li>b is number of on bits in molecule B</li> <li>c is the number of bits that are on in both molecules</li> <li>d is the number of common off bits</li> <li>n is the bit length (total number of bits) of the fingerprint: \\(n = a + b \u2212 c + d\\).</li> </ul>"},{"location":"summary_pipeline/","title":"Pre-prediction","text":""},{"location":"summary_pipeline/#prediction","title":"Prediction","text":"<p>The following is a diagram of how the pipeline proceeds after the Pre-prediction step.</p> Pipeline after preprediction <ol> <li> <p>There are available \\(N\\) experimental tandem mass spectrums (denoted by \\(MS\\)) \\(\\left{ MS_{i}\\right}_{i=1}^{N}\\).</p> </li> <li> <p>For each \\(MS_{i}\\), \\(M\\) SMILES (denoted by \\(S\\)) are predicted, giving groups: \\(MS_{1} \\rightarrow \\left{ S_{1i} \\right}_{i=1}^{M}\\), \\(MS_{2} \\rightarrow \\left{ S_{2i} \\right}_{i=1}^{M}\\), ..., \\(MS_{N} \\rightarrow \\left{ S_{Ni} \\right}_{i=1}^{M}\\)</p> </li> <li> <p>For each of the \\(M\\) SMILES (associated to an experimental mass spectrum) a computational mass spectrum (denoted by \\(P\\)) is predicted, thus giving \\(S_{ij} \\rightarrow P_{ij}, \\: \\: i=1,...,M \\and j=1,...,N\\)</p> </li> <li> <p>Each of these \\(P\\) are transformed into a new representation (new vector) by using an embedding transformation (using the methodologies described in DreaMS), so that for a given \\(j\\), the \\(P_{ij}_{i=1}^{M}\\) can be compared with its \\(MS_{j}\\) in these new represantations, using any available similarity metric. Cosine similarity is used as the default metric.</p> </li> <li> <p>The highest similar \\(P_{kj}\\) from the previous step (with \\(k\\) being the index of the highest similar spectrum) is chosen as the most likely computational spectrum to resemble the original spectrum \\(M_{j}\\), and therefore its associated SMILES \\(S_{ik}\\) is also chosen as the most likely SMILES associated to that spectrum \\(M_{j}\\)</p> </li> </ol>"},{"location":"training/","title":"Training","text":""},{"location":"training/#data","title":"Data","text":"<p>The data available for the training process can be consulted in placeholder for the actual place. </p> <p>The sources of data are two:</p> <ul> <li>MassSpecGym dataset (available at MassSpecGym dataset)</li> <li>The TeFT paper dataset (available at TeFT)</li> </ul> <p>A statistical description of the MassSpecGym dataset is available at the page of its paper MassSpecGym paper. </p> <p>Both datasets provide both the tandem mass spectrums and Simplified Molecular Input Line Entry System (SMILES). However, the TeFT dataset does not provide intensities for its spectrums, only the mass-to-charge ratios. Besides that, the TeFT dataset does not provide any additional metadata (i.e. ion mode, energy level, etc.).</p>"},{"location":"training/#architecture","title":"Architecture","text":"<p>The base architecture for the task of predicting SMILES from tandem mass spectrums is a vanilla transformer. Modifications of it are (for now) implemented in the <code>v2</code> and <code>experimental</code> branches of the repository. The transformer architecture makes use of an encoder-decoder structure. Modifications in those branches are mainly done in the encoder part and in the attention mechanism. </p> <p>There are experimental architectural changes as well (in the <code>v2</code> branch) for training multiple tasks, namely: the main prediction task (SMILES from tandem mass spectrums), and a secondary task in charge of the prediction of the mass of the precursor. Also have in mind that custom loss functions, beyond the CrossEntropy, are also defined (in the main branch under the <code>RegByMassCrossEntropy</code> class in the <code>TrainWorkFlowV2.py</code> file) to try to force the predictions to match the precursor mass of the training tandem mass spectrums precursors.</p>"},{"location":"tutorial_full_pipeline_script/","title":"Full pipeline script","text":"<p>The script <code>predict_CASMI_2022_modular.py</code> executes end-to-end workflow:</p> <ul> <li>Receives raw sample experimental .mzML files (accounting for MS1 - and/or MS2, depending on the protocol -)</li> <li>Outputs a folder with SMILES predictions for all the assumed metabolites and a pickle (<code>.pkl</code>) file containing the MS/MS predicted spectrums</li> </ul> <p>To get the list of the arguments the script uses, set your current directory to the <code>scripts</code> folder of the repo, and then execute:</p> <pre>python predict_CASMI_2022_modular.py --help\n</pre> <p>Some of the arguments are:</p> <ul> <li>-h, --help: show this help message and exit</li> <li>--chromatograms_dir: Directory containing .mzML files for chromatograms</li> <li>--ddas_dir: Directory containing .mzML files for DDA (optional)</li> <li>--model_dir: Path to directory containing models</li> <li>--model_name: Name of the model to use</li> <li>--output_dir: Output directory for prediction results</li> <li>--protocol: Protocol to use: \"CASMI\" or \"ptfi\" (default: CASMI)</li> <li>--params_file: JSON file containing prediction parameters (optional)</li> <li>--num_pred: Number of predictions to make (default: 3)</li> <li>--device: Device to run predictions on (default: cpu)</li> <li>--pickle_path: Path to save the MS data as a pickle file (default: output_dir/ms_data.pkl)</li> </ul> <p>An example of usage is:</p> <pre>python predict_CASMI_2022_modular.py --chromatograms_dir /shared/users/ptfi/data/CASMI/pos/A_M1_posPFP --model_dir /shared/users/ptfi/models --model_name dd_arch1_lf1_data_1.pth --output_dir /users/jdvillegas/ptfi-frijol-pujc/results/A_M1_posPFP --num_pred 3 --device cpu --protocol CASMI --params_file /home/julian/Documents/repos/ptfi-frijol-pujc/scripts/prediction_params.json --mass_trace_noise_threshold_int 15000\n</pre> <p>The full list of parameters has the parameters used in scripts <code>predict_known_metabolites_dec_2024.py</code> and <code>Features_Concensus_extraction.py</code></p>"},{"location":"tutorial_full_pipeline_script/#full-pipeline-script","title":"Full pipeline script\u00b6","text":""},{"location":"tutorial_github_setup/","title":"About Github","text":"<p>These are the active branches under development:</p> <ul> <li>main (all be merged to here in the future)</li> <li>gustavo_branch (preprediction branch)</li> <li>v2 (prediction branch)</li> </ul> <p></p> <p>For non developers:</p> <ol> <li>Since the repo is private, ask for a Personal Access Token (PAT) to github admins (Julian, Camila)</li> </ol> <p>For developers:</p> <ol> <li>Ask github admins for a github request to contribute. After that, create your own PAT.</li> </ol> <ol> <li>Set your working directory in a terminal to the place where to locate the repo, and execute</li> </ol> <pre>git clone https://&lt;USERNAME&gt;:&lt;TOKEN&gt;@github.com/iomicasjaverianacali/ptfi-frijol-pujc.git\n</pre> <p>For non developers:</p> <ul> <li>USERNAME: jvjav</li> <li>TOKEN: the PAT provided by the github admin</li> </ul> <p>For developers:</p> <ul> <li>USERNAME: your github username</li> <li>TOKEN: the PAT you created</li> </ul> <ol> <li>Install conda (anaconda)</li> </ol> <ol> <li>Create a new environment:</li> </ol> <pre>conda create --name thenameyouwant\n</pre> <ol> <li>Activate the environment:</li> </ol> <pre>conda activate thenameyouwant\n</pre> <p>Make sure you are in the directory having the <code>requirements.txt</code> file. From there execute:</p> <pre>pip install -r requirements.txt\n</pre>"},{"location":"tutorial_github_setup/#about-github","title":"About Github\u00b6","text":""},{"location":"tutorial_github_setup/#note","title":"NOTE\u00b6","text":"<ul> <li>Latest preprediction features are available in the <code>gustavo_branch</code></li> <li>Latest prediction features are available in the <code>v2</code> branch</li> <li>Stable features are available in the <code>main</code> branch</li> </ul>"},{"location":"tutorial_github_setup/#download-main-repo","title":"Download main repo\u00b6","text":""},{"location":"tutorial_github_setup/#set-a-virtual-environment","title":"Set a virtual environment\u00b6","text":""},{"location":"tutorial_github_setup/#install-packages-needed","title":"Install packages needed\u00b6","text":""},{"location":"tutorial_ms2_smiles/","title":"Training","text":"<p>Neural network architectures are implemented in the files <code>MolecularStructureTeFT.py</code>, <code>MolecularStructureTeFTWithBias.py</code>, <code>MolecularStructureTeFTWithIntensity.py</code> (under test), <code>MolecularStructureTeFTWithMLP.py</code> (multi-task learning under revision).</p> <p>The tokenizer is implemented in <code>MolecularStructureDictionary.py</code>.</p> <p>The Training integrator is implemented in <code>TrainWorkFlowV2.py</code> (branch <code>v2</code>).</p> <p>The loss functions are defined as classes in <code>LossFunctions.py</code> (and for the multi-task learning also under the <code>LossWithMassPrediction.py</code>).</p> <p>The script used to train the neural network is named <code>train_teft_original_data_datadriven_tokenizer_v2.py</code> (the one in the <code>v2</code> branch).</p> <p>To check the arguments accepted by the script you can execute :</p> <pre>python train_teft_original_data_datadriven_tokenizer_v2.py --help\n</pre> <p>and after doing so, the list of arguments will be shown:</p> <ul> <li>-h, --help            show this help message and exit</li> <li>--max_sentence_size : Maximum length of MS/MS to be considered</li> <li>--tokenizer: Tokenizer class name (Available options: DataDrivenTokenizer and SimpleTokenizer)</li> <li>--dir_vocab: path to the folder containing the tokenizer vocabulary (i.e. SPE_ChEMBL.txt)</li> <li>--class_balancing:     Enable class balancing</li> <li>--balance_by: Balance dataset by m/z values or molecular weights ('mz'or 'molecular_weight')</li> <li>--balance_bins: Number of bins to use for histogram when balancing the dataset</li> <li>--batch_size: Batch size for training</li> <li>--device: Device to use for training ('cpu' or 'cuda')</li> <li>--loss_function: Loss function to use for training ('CrossEntropy' or 'RegByMassCrossEntropy')</li> <li>--architecture: Available transformer-based architectures to use for training ('MolecularStructureTeFT' 'MolecularStructureTeFTWithBias', 'MolecularStructureTeFTWithIntensity')</li> <li>--model_name: Model name for saving</li> <li>--max_samples: Maximum number of samples to use from the dataset. If None, use all samples.</li> <li>--epochs: Number of training epochs</li> <li>--d_model: Transformer architecture parameter emmbedding hidden size</li> <li>--d_ff: Transformer architecture parameter feedForward dimension</li> <li>--d_k: Transformer architecture parameter dimension of K( and Q)</li> <li>--d_v: Transformer architecture parameter dimension of V</li> <li>--n_layers: Number of Encoder/Decoder Layers</li> <li>--n_heads: Number of heads in Multi-Head Attention</li> </ul> <p>In the command line execute:</p> <pre>python train_teft_original_data_datadriven_tokenizer_v2.py --max_sentence_size 100 --tokenizer DataDrivenTokenizer --dir_vocab /users/jdvillegas/ptfi-frijol-pujc-v2/data --batch_size 246 --device cuda --loss_function CrossEntropy --architecture MolecularStructureTeFTWithIntensity --model_name dd_su_arch3_lf1_code_pl\n</pre> <p>Since training is very computing demanding, it is rather better to run it on the GPUs available in the cluster.</p> <p>To do so here is an example:</p> <pre>#!/bin/bash\n#SBATCH --job-name=dd_su_arch3_lf1_code_pl  # Job name\n#SBATCH -o dd_su_arch3_lf1_code_pl.out  # File to which STDOUT will be written, %j inserts jobid\n#SBATCH -e dd_su_arch3_lf1_code_pl.err  # File to which STDERR will be written, %j inserts jobid\n#SBATCH --partition=GPU # FULL if running CPU, GPU if running GPU, 3 GPUs with 24 GB each\n#SBATCH --nodes=1\n##SBATCH --tasks-per-node=12 # 40 CPUs per node\n##SBATCH --ntasks-per-socket=6 # 19 CPUs per socket\n##SBATCH --cpus-per-task=1 # 1 CPU per task\n#SBATCH --mem=200G # each node has around 380 GB, do not reserve all memory if you don\u00b4t need all of it\n##SBATCH --nodelist=node22  # exactly which node to use, node 22 -&gt; test node\n##SBATCH --exclude=node21   # which node to exclude\n#SBATCH --gres=gpu:3\n\n#module load lang/python/3.12.3\n#module load cuda/11.8\n\n# Source the Conda script to enable the `conda activate` command\nsource ~/miniconda3/etc/profile.d/conda.sh  # Update this to your actual conda.sh path\n\n# Activate the specific Conda environment\nconda activate metabolomicscuda\n#conda activate wearmepylow\n\nscratch_dir=/scratch/jdvillegas/ptfi_$SLURM_JOB_ID\n\nmkdir -p $scratch_dir;\ncp -r /users/jdvillegas/ptfi-frijol-pujc-v2/classes/InputManagement.py $scratch_dir\ncp -r /users/jdvillegas/ptfi-frijol-pujc-v2/classes/MolecularStructureDictionary.py $scratch_dir\ncp -r /users/jdvillegas/ptfi-frijol-pujc-v2/classes/MolecularStructureTeFT.py $scratch_dir\ncp -r /users/jdvillegas/ptfi-frijol-pujc-v2/classes/MolecularStructureTeFTWithBias.py $scratch_dir\ncp -r /users/jdvillegas/ptfi-frijol-pujc-v2/classes/MolecularStructureTeFTWithIntensity.py $scratch_dir\ncp -r /users/jdvillegas/ptfi-frijol-pujc-v2/classes/TrainWorkFlowWithIntensity.py $scratch_dir\ncp -r /users/jdvillegas/ptfi-frijol-pujc-v2/classes/LossFunctions.py $scratch_dir\ncp -r /users/jdvillegas/ptfi-frijol-pujc-v2/classes/Plotter.py $scratch_dir\ncp -r /users/jdvillegas/ptfi-frijol-pujc-v2/classes/SignalMath.py $scratch_dir\ncp -r /users/jdvillegas/ptfi-frijol-pujc-v2/classes/TrainWorkFlowV2.py $scratch_dir\ncp -r /users/jdvillegas/ptfi-frijol-pujc-v2/scripts/train_teft_original_data_datadriven_tokenizer_v2.py $scratch_dir\ncd $scratch_dir\n\n# Run your Python script\n#time python train_teft_original_data_datadriven_tokenizer_v2.py\ntime python train_teft_original_data_datadriven_tokenizer_v2.py --max_sentence_size 100 --tokenizer DataDrivenTokenizer --dir_vocab /users/jdvillegas/ptfi-frijol-pujc-v2/data --batch_size 246 --device cuda --loss_function CrossEntropy --architecture MolecularStructureTeFTWithIntensity --model_name dd_su_arch3_lf1_code_pl\n\n#mkdir $SLURM_SUBMIT_DIR/TeFT_DD_wo_reg_v2_nopad_nobal_dir_attn_$SLURM_JOB_ID\n#cp -r * $SLURM_SUBMIT_DIR/TeFT_DD_wo_reg_v2_nopad_nobal_dir_attn_$SLURM_JOB_ID\n\ncp -r $scratch_dir /users/jdvillegas/slurm_results/ptfi/$SLURM_JOB_ID\n\necho \"TrainWorkflowV2 LF1 Arch3 (Intensity gating + Bias in Attention)\" &gt; /users/jdvillegas/slurm_results/ptfi/$SLURM_JOB_ID/DescriptionBatchJob.txt\n#rm -r $scratch_dir\n\n# Deactivate the Conda environment\nconda deactivate\n</pre> <p>The SMILES prediction is done by the <code>PredictWorkflow()</code> class of the <code>Workflow.py</code> file.</p> <p>Prediction can be done by executing the following snippet of code:</p> <pre>from ptfifrijolpujc import PredictWorkflow\n\npw = PredictWorkflow()\npw.predict( List[List], # List of list of intensities\n            List[List], # List of list of m/z\n            smiles=List, # List of smiles\n            full_path_to_mdl=str, # full path to trained model\n            device=str, # either 'cpu' or 'cuda'\n            output_dir=str, # path to directory where to save results\n            num_pred=int, # number of SMILES predictions per MS/MS\n            predict_approach=str, # either \"beam_search\" or \"greedy\",\n            predict_approach_args=dict|None) # Only used when `predict_approach` is \"greedy. A dict with the only key `beam_size` (i.e. {'beam_size': 10})\n</pre> <p>Working prediction examples <code>predict_known_metabolites_dec_2024.py</code> and <code>predict_CASMI_2022_modular.py</code> are avilable to use.</p> <ul> <li><code>predict_known_metabolites_dec_2024.py</code>: does predictions for a list of 314 known metabolites available in the file <code>path/to/the/repo/ptfi-frijol-pujc/data/Espectros_conocidos 1.xlsx</code>.</li> </ul> <p>To check the input parameters of <code>predict_known_metabolites_dec_2024.py</code>, set the current working directory to the <code>scripts</code> folder and execute:</p> <pre>python predict_known_metabolites_dec_2024.py --help\n</pre> <ul> <li>-h, --help: show this help message and exit</li> <li>--dir_path: Directory path containing the Excel file</li> <li>--filename: Excel filename</li> <li>path_to_mdl: Full path to the model directory</li> <li>--trained_model_name: Name of the trained model file</li> <li>--output_dir: Output directory for results</li> <li>--num_pred: Number of predictions</li> <li>--predict_approach: Prediction approach is either 'greedy' or 'beam_search'</li> <li>--device: 'cpu' or 'cuda'</li> <li>--beam_size: [OPTIONAL] Only if the <code>predict_approach</code> is 'beam_search', the length of the beam should be supplied as a dict (i.e. {<code>beam_size</code>: 5})</li> </ul> <p>Specific example of use:</p> <pre>python predict_known_metabolites_dec_2024.py --dir_path /home/julian/Documents/repos/ptfi-frijol-pujc/data --filename Espectros_conocidos 1.xlsx --path_to_mdl /home/julian/Documents/FTP/ptfi/models --trained_model_name dd_arch1_lf1_data_1.pth --output_dir /home/julian/Documents/repos/ptfi-frijol-pujc/results/kn-dd-su-arch1-lf1-test --num_pred 10 --device cpu --predict_approach beam_search --beam_size 5\n</pre>"},{"location":"tutorial_ms2_smiles/#training","title":"Training\u00b6","text":""},{"location":"tutorial_ms2_smiles/#predict","title":"Predict\u00b6","text":""},{"location":"tutorial_pipeline_preprocessing/","title":"Chromatographic Data Preprocessing for Analysis","text":"<p>In the command line execute:</p> <pre>python Features_Concensus_extraction.py --chromatograms_dir /shared/users/ptfi/data/CASMI/pos/A_M1_posPFP --model_dir /shared/users/ptfi/models --model_name dd_arch1_lf1_data_1.pth --output_dir /users/glara/scratch/ --num_pred 3 --device cpu --protocol CASMI\n</pre> <pre>RT window size calculated as 240 seconds.\nProgress of 'mass trace detection':\n-- done [took 2.17 s (CPU), 2.18 s (Wall)] -- \nProgress of 'elution peak detection':\n-- done [took 3.07 s (CPU), 0.11 s (Wall)] -- \nProgress of 'assembling mass traces to features':\nLoading metabolite isotope model with 5% RMS error\n-- done [took 2.10 s (CPU), 0.11 s (Wall)] -- \nProgress of 'mass trace detection':\n-- done [took 2.74 s (CPU), 2.76 s (Wall)] -- \nProgress of 'elution peak detection':\n-- done [took 2.60 s (CPU), 0.08 s (Wall)] -- \nProgress of 'assembling mass traces to features':\n-- done [took 2.09 s (CPU), 0.10 s (Wall)] -- \nProgress of 'computing RT transformations':\n-- done [took 0.41 s (CPU), 0.41 s (Wall)] -- \nProgress of 'linking features':\n-- done [took 0.47 s (CPU), 0.47 s (Wall)] -- \n/shared/users/ptfi/data/CASMI/pos/A_M1_posPFP/A_M1_posPFP_01.mzml\n/shared/users/ptfi/data/CASMI/pos/A_M1_posPFP/A_M1_posPFP_02.mzml\nWarning: SpectraDistance received the unknown parameter 'mass_tolerance'!\nNumber of M/z lists: 98\nNumber of Intensity lists: 98\nNumber of Retention times: 98\nNumber of MS2 Precursor masses: 98\nMS data saved to /users/glara/scratch/ms_data.pkl\nCluster sizes:\n&lt;Loading metabolite isotope model with 5% RMS error&gt; occurred 2 times\n  size 2: 6x\n  size 3: 5x\n  size 4: 10x\n  size 5: 2x\n  size 6: 2x\n  size 7: 1x\n  size 8: 6x\n  size 9: 2x\n  size 10: 3x\n  size 11: 4x\n  size 12: 2x\n  size 13: 1x\n  size 14: 2x\n  size 16: 3x\n  size 17: 1x\n  size 18: 1x\n  size 21: 2x\n  size 23: 2x\n  size 24: 1x\n  size 25: 1x\n  size 26: 1x\n  size 27: 1x\n  size 28: 1x\n  size 31: 1x\n  size 32: 5x\n  size 33: 2x\n  size 36: 2x\n  size 37: 1x\n  size 45: 1x\n  size 46: 1x\n  size 47: 1x\n  size 48: 1x\n  size 52: 1x\n  size 54: 1x\n  size 66: 1x\n  size 68: 1x\n  size 83: 1x\n  size 87: 1x\n  size 106: 1x\n  size 107: 1x\n  size 144: 1x\n  size 178: 1x\n  size 189: 1x\n  size 194: 1x\n  size 205: 1x\n  size 207: 1x\n  size 212: 1x\n  size 385: 1x\n  size 487: 1x\n  size 635: 1x\n  size 640: 1x\n  size 723: 1x\n  size 1100: 1x\n  size 1607: 1x\n  size 3577: 1x\nNumber of merged peaks: 439595/383942 (114.50 %) of blocked spectra\n</pre>"},{"location":"tutorial_pipeline_preprocessing/#chromatographic-data-preprocessing-for-analysis","title":"Chromatographic Data Preprocessing for Analysis\u00b6","text":"<p>To check the arguments accepted by the script you can execute :</p> <pre>python Features_Concensus_extraction.py --help\n</pre>"},{"location":"tutorial_pipeline_preprocessing/#parameters-for-preprocesing-chromatograph-data","title":"Parameters for Preprocesing Chromatograph Data\u00b6","text":""},{"location":"tutorial_pipeline_preprocessing/#mass-trace-detection-parameters","title":"Mass Trace Detection Parameters\u00b6","text":"<ul> <li>--mass_trace_mass_error_ppm: Mass error tolerance in parts per million (default: 10.0).</li> <li>--mass_trace_noise_threshold_int: Intensity threshold for noise filtering (default: 1200).</li> <li>--mass_trace_chrom_peak_snr: Signal-to-noise ratio for chromatographic peak detection (default: 3.0).</li> <li>--mass_trace_min_sample_rate: Minimum required sampling rate (default: 0.5).</li> <li>--mass_trace_min_length: Minimum length of mass traces (default: 5.0).</li> <li>--mass_trace_max_length: Maximum length of mass traces. -1.0 for no maximum (default: -1.0).</li> <li>--mass_trace_quant_method: Method for quantification (\"area\" or alternative methods).</li> </ul>"},{"location":"tutorial_pipeline_preprocessing/#elution-peak-detection-parameters","title":"Elution Peak Detection Parameters\u00b6","text":"<ul> <li>--elution_peak_width_filtering: Setting for peak width filtering (default: \"auto\").</li> <li>--elution_peak_chrom_fwhm: Full width at half maximum for chromatographic peaks (default: 2.0).</li> <li>--elution_peak_chrom_peak_snr: Signal-to-noise ratio for elution peak detection (default: 3.0).</li> <li>--elution_peak_min_fwhm: Minimum full width at half maximum (default: 1.0).</li> <li>--elution_peak_max_fwhm: Maximum full width at half maximum (default: 60.0).</li> <li>--elution_peak_masstrace_snr_filtering: Enable/disable SNR filtering for mass traces (default: \"false\").</li> </ul>"},{"location":"tutorial_pipeline_preprocessing/#feature-detection-parameters","title":"Feature Detection Parameters\u00b6","text":"<ul> <li>--feature_detection_remove_single_traces: Whether to remove features with single traces (default: \"false\").</li> <li>--feature_detection_local_rt_range: Local retention time range for feature detection (default: 2.0).</li> <li>--feature_detection_local_mz_range: Local m/z range for feature detection (default: 10.0).</li> <li>--feature_detection_charge_lower_bound: Lower bound for charge state detection (default: 1).</li> <li>--feature_detection_charge_upper_bound: Upper bound for charge state detection (default: 3).</li> <li>--feature_detection_chrom_fwhm: Chromatographic FWHM for feature detection (default: 2.0).</li> <li>--feature_detection_report_summed_ints: Whether to report summed intensities (default: \"false\").</li> <li>--feature_detection_enable_RT_filtering: Enable/disable RT filtering (default: \"true\").</li> <li>--feature_detection_isotope_filtering_model: Model for isotope filtering (default: \"metabolites (5% RMS)\").</li> <li>--feature_detection_mz_scoring_13C: Enable/disable 13C m/z scoring (default: \"false\").</li> <li>--feature_detection_use_smoothed_intensities: Use smoothed intensities (default: \"true\").</li> <li>--feature_detection_report_convex_hulls: Report convex hulls (default: \"true\").</li> <li>--feature_detection_report_chromatograms: Report chromatograms (default: \"false\").</li> <li>--feature_detection_mz_scoring_by_elements: Enable/disable m/z scoring by elements (default: \"false\").</li> <li>--filename_feature_map: Name of the feautureXML files with the Feature map info. Do not add the extension. (default: feature_map).</li> <li>--filename_consensus_map: Name of the consensusXML files with the Consensus map info. Do not add the extension. (default: consensus_map).</li> </ul>"},{"location":"tutorial_pipeline_preprocessing/#ms2-feature-detection-parameters","title":"MS2-Feature Detection Parameters\u00b6","text":"<ul> <li>--make_ms2_mz_tolerance: m/z tolerance for MS2 features mapping (default: 0.01).</li> <li>--make_ms2_rt_tolerance: RT tolerance for MS2 features mapping (default: 5.0).</li> <li>--filename_ms2s_mzml: Name of the mzML files with the MS2 spectra. Do not add the extension. (default: ms2s).</li> <li>--max_peak_filter_pptg: Maximum peak filter for pptg (default: 0.2).</li> <li>--merger_spectra_mz_binning_width: m/z binning width for merging spectra (default: 5.0).</li> <li>--merger_spectra_mz_binning_width_unit: Unit for m/z binning width (default: \"ppm\").</li> <li>--merger_spectra_sort_blocks: Sorting method for blocks (default: \"RT_ascending\").</li> <li>--merger_spectra_mz_tolerance: m/z tolerance for merging spectra (default: 1.0e-04).</li> <li>--merger_spectra_mass_tolerance: Mass tolerance for merging spectra (default: 0.0).</li> <li>--merger_spectra_rt_tolerance: RT tolerance for merging spectra (default: 15.0).</li> <li>--filter_type: Type of filter to apply (default: \"window_mower\").</li> <li>--window_mower_windowsize: Window size for the window mower filter if --filter_type:\"window_mower\" (default: 50.0).</li> <li>--window_mower_peakcount: Number of peaks to keep in the window mower filter if --filter_type:\"window_mower\" (default: 2).</li> <li>--window_mower_movetype: Movement type for the window mower filter if --filter_type:\"window_mower\" (default: \"slide\").</li> <li>--threshold_mower_threshold: Threshold for the threshold mower filter if --filter_type:\"threshold_mower\" (default: 0.05).</li> <li>--nlargest_n: Number of largest peaks to keep if --filter_type:\"nlargest\" (default: 200).</li> </ul>"},{"location":"tutorial_pipeline_preprocessing/#ms2-spectra-merging","title":"MS2 Spectra Merging\u00b6","text":""},{"location":"tutorial_pipeline_preprocessing/#-merger_spectra_mz_tolerance-10e-04","title":"--merger_spectra_mz_tolerance 1.0e-04\u00b6","text":""},{"location":"tutorial_pipeline_preprocessing/#-merger_spectra_rt_tolerance-150","title":"--merger_spectra_RT_tolerance 15.0\u00b6","text":""},{"location":"tutorial_pipeline_preprocessing/#ms2-filter","title":"MS2 Filter\u00b6","text":""},{"location":"tutorial_pipeline_preprocessing/#-filter_type-window_mower","title":"--filter_type: window_mower\u00b6","text":""},{"location":"tutorial_pipeline_preprocessing/#-filter_type-threshold_mower","title":"--filter_type: threshold_mower\u00b6","text":""},{"location":"tutorial_pipeline_preprocessing/#-filter_type-nlargest","title":"--filter_type: nlargest\u00b6","text":""}]}